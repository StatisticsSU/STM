[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Contents\n\nThis is just a skeleton template for a course that will be given in the future.\n\n\n\nLiterature\n\nAuthors (2021). Book Name\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phoneâ€™s calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\n\n\nTeachers\n\n\n\n\nMattias VillaniCourse responsible and lecturerProfessor"
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html",
    "href": "tutorial/numericalML/numericalML.html",
    "title": "Maximum likelihood by numerical optimization",
    "section": "",
    "text": "In this tutorial you will learn how maximum likelihood estimates and standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is typically surprisingly accurate also for smaller sample sizes.\n\nIt will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner giving you the super-power ðŸ’ª to use the computer to estimate the parameters and their uncertainty in quite complex models.\nWe will start with simple models with a single parameter to cover all the concepts, and then move on to the practically more important multi-parameter case.\nLetâ€™s first load some useful libraries (install them using install.packages() if you havenâ€™t already).\n\nlibrary(latex2exp) # for plotting mathematical symbols (LaTeX)\nlibrary(remotes)   # for loading packages from GitHub\nlibrary(ggplot2)   # for fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html#footnotes",
    "href": "tutorial/numericalML/numericalML.html#footnotes",
    "title": "Maximum likelihood by numerical optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdfâ†©ï¸Ž\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.â†©ï¸Ž"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html",
    "href": "tutorial/bootstrap/bootstrap.html",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "",
    "text": "In this tutorial you will learn about the bootstrap method for approximating the sampling distribution of any estimator, for example the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\nLetâ€™s first load some libraries that we will use (install them using install.packages() if you havenâ€™t already).\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html#footnotes",
    "href": "tutorial/bootstrap/bootstrap.html#footnotes",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdfâ†©ï¸Ž"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nTutorials\n\nMaximum likelihood by numerical optimization html\nBootstrap html\nBonus: State-space models"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nLecture 1 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 2 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 3 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 4 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 5 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 6 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 7 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 8 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 9 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 10 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 11 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 12 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 13 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 14 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 15 - Introduction.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Programming for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future."
  },
  {
    "objectID": "computerlabs.html",
    "href": "computerlabs.html",
    "title": "Computer labs for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nComputer lab 1 \nUppgifter: html\nComputer lab 2 \nUppgifter: html\nComputer lab 3 \nUppgifter: html\nComputer lab 4 \nUppgifter: html"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nExercise 1 - Basic probability.\nProblems: Book 2.1, 2.2, â€¦\nExercise 2 - Whatever.\nProblems: Book 3.1, 3.2, â€¦"
  },
  {
    "objectID": "tutorial/statespace/statespace.html",
    "href": "tutorial/statespace/statespace.html",
    "title": "State-Space models and the Kalman Filter",
    "section": "",
    "text": "This tutorial gives a very brief introduction to state-space models in the dlm package, exemplified with the local level model fitted to the well-known Nile river data."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#footnotes",
    "href": "tutorial/statespace/statespace.html#footnotes",
    "title": "Maximum likelihood by numerical optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdfâ†©ï¸Ž\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.â†©ï¸Ž"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#state-space-model",
    "href": "tutorial/statespace/statespace.html#state-space-model",
    "title": "State-Space models and the Kalman Filter",
    "section": "State-space model",
    "text": "State-space model\nA state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) is in the dlm package written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{C} &= 1 \\\\\n\\boldsymbol{A} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nHence the state vector is a single scalar, \\(\\mu_t\\), the unobserved local level of time series. We learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) .\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models: filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data.\n\n\nFiltering\nLetâ€™s do some filtering in the dlm package.\nLoad the dlm package:\n\n#install.packages(\"dlm\") # uncomment the first time to install.\nlibrary(dlm)\n\nSet up the local level model as a state-space model in the dlm package with \\(\\sigma_\\varepsilon^2 = 10000\\) and \\(\\sigma_\\nu^2 = 10000\\) (see below on how to estimate these parameters).\n\nmodel = dlm(FF = 1, V = 10000, GG = 1, W = 10000, m0 = 0, C0 = 100^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\nu^2\\) were just set to some values above. Letâ€™s instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 0, C0 = 100^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(0,0), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1]  9120.485 15956.088\n\n\nSo we see that the values used initially are not too far of the maximum likelihood estimates:\n\\(\\hat \\sigma_\\varepsilon^2 \\approx 9120\\) and \\(\\hat\\sigma_\\nu^2 \\approx 15956\\). We can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 0, C0 = 100^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nSmoothing\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\"), lty = 1, \n    col = c(\"steelblue\", \"red\"))\n\n\n\n\n\n\nForecasting\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), \n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, \n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#filtering",
    "href": "tutorial/statespace/statespace.html#filtering",
    "title": "State-Space models and the Kalman Filter",
    "section": "Filtering",
    "text": "Filtering\nLoad the package\n\n#install.packages(\"kalmanfilter\")\nlibrary(kalmanfilter)\n\nSet up the state-space model"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#dlm-package",
    "href": "tutorial/statespace/statespace.html#dlm-package",
    "title": "State-Space models and the Kalman Filter",
    "section": "dlm package",
    "text": "dlm package\nIn the package dlm package with univariate observation vector \\(y_t\\) and state vector \\(\\boldsymbol{\\theta}_t\\) the model is written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is obtained with\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{C} &= 1 \\\\\n\\boldsymbol{A} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\n\nFiltering\nLoad the dlm package\n\n#install.packages(\"dlm\")\nlibrary(dlm)\n\nSet up the local level model as a state-space model with \\(\\sigma_\\varepsilon^2 = 10000\\) and \\(\\sigma_\\nu^2 = 10000\\) (see below on how I decided these values).\n\nmodel = dlm(FF = 1, V = 10000, GG = 1, W = 10000, m0 = 0, C0 = 100^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\nu^2\\) were just set to some values above. Letâ€™s instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 0, C0 = 100^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(0,0), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1]  9120.485 15956.088\n\n\nSo we see that the values used initially are not too far of the maximum likelihood estimates:\n\\(\\hat \\sigma_\\varepsilon^2 \\approx 9120\\) and \\(\\hat\\sigma_\\nu^2 \\approx 15956\\). We can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 0, C0 = 100^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nSmoothing\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\"), lty = 1, \n    col = c(\"steelblue\", \"red\"))\n\n\n\n\n\n\nForecasting\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), \n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, \n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  }
]