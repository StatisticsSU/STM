[
  {
    "objectID": "tutorial/numericalML/MLnumerical.html",
    "href": "tutorial/numericalML/MLnumerical.html",
    "title": "Maximum likelihood by numerical optimization",
    "section": "",
    "text": "In this tutorial you will learn how maximum likelihood estimates and standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is typically surprisingly accurate also for smaller sample sizes.\n\nIt will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner giving you the super-power üí™ to use the computer to estimate the parameters and their uncertainty in quite complex models.\nWe will start with simple models with a single parameter to cover all the concepts, and then move on to the practically more important multi-parameter case.\nLet‚Äôs first load some useful libraries (install them using install.packages() if you haven‚Äôt already).\n\nlibrary(latex2exp) # for plotting mathematical symbols (LaTeX)\nlibrary(remotes)   # for loading packages from GitHub\nlibrary(ggplot2)   # for fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html",
    "href": "tutorial/bootstrap/bootstrap.html",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "",
    "text": "In this tutorial you will learn about the bootstrap method for approximating the sampling distribution of any estimator, for example the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\n\nLet‚Äôs first load some libraries that we will use (install them using install.packages() if you haven‚Äôt already).\n\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution\n\n\nData\nWe will use the dataset ebaycoins in the R package SUdatasets for illustration. The dataset contains data from 1000 eBay auctions of collector‚Äôs coins1. Let‚Äôs load the dataset and have a look:\n\n#install_github(\"StatisticsSU/SUdatasets\") # uncomment if this is not installed\nlibrary(\"SUdatasets\")\nhead(ebaycoins)\n\n  BookVal MinorBlem MajorBlem PowerSeller IDSeller Sealed NegFeedback\n1   18.95         0         0           0        0      0           0\n2   43.50         0         0           1        0      0           0\n3   24.50         0         0           1        0      0           0\n4   34.50         1         0           0        0      0           0\n5   99.50         0         0           0        0      0           1\n6    9.50         0         0           0        0      0           0\n  ReservePriceFrac NBidders FinalPrice\n1        0.3688654        2      15.50\n2        0.2298851        6      41.00\n3        1.0200000        1      24.99\n4        0.7217391        1      24.90\n5        0.1672362        4      72.65\n6        1.2094737        2      17.52\n\n\nEach auction (rows in the dataset) will be taken as an observation, and the dataset has the following variables:\n\nthe number of bidders in each auction (NBidders)\nthe final price (FinalPrice)\nthe book value of the coin according a coin collectors catalogue (BookVal).\nthe seller‚Äôs reservations price (lowest price that the seller is willing to sell for) as a fraction the book value (ReservePriceFrac).\nbinary variables on whether or not the seller is a verified ebay seller (IDSeller), sells large quantites (PowerSeller) and if the seller has many reviews with negative feedback (NegFeedback)\ninformation about the condition of the object: if it has a minor blemish (MinorBlem), a major one (MajorBlem), or sold in its original unbroken packaging (Sealed).\n\n\n\nMaximum likelihood for the Poisson model\nWe will first analyze only the variable NBidders and later move over to a regression modeling situation. Since NBidders is a count variable, a natural first model to consider is the Poisson model:\n\\[\nY_1,\\ldots,Y_n \\vert \\lambda \\overset{\\mathrm{iid}}{\\sim}\\mathrm{Poisson}(\\lambda)\n\\]\nwhere we use the symbol \\(Y\\) for the random variable NBidders and \\(y\\) as the observed value.\nWe can estimate \\(\\lambda\\) by the maximum likelihood (ML) metod. The ML method finds the value for \\(\\lambda\\) in the Poisson distribution that maximizes the probability of the observed dataset \\(y_1,\\ldots,y_n\\). As we have seen in tutorial on numerical ML, the ML estimate for this model is just the sample mean \\(\\hat \\lambda = \\bar y\\) and the standard error is \\(\\mathrm{SE}(\\hat\\lambda)=\\sqrt{\\bar y/n}\\). For NBidders in the ebayscoins data we have\n\nn = length(ebaycoins$NBidders)\nmessage(paste(\"ML estimate:\", mean(ebaycoins$NBidders)))\n\nML estimate: 3.635\n\nmessage(paste(\"SE of ML estimate:\", sqrt(mean(ebaycoins$NBidders)/n)))\n\nSE of ML estimate: 0.0602909611799314\n\n\n\n\nSampling distribution by the bootstrap\nThe bootstrap is an simulation-based technique for obtaining an approximation to the sampling distribution. Recall first that the sampling distribution of an estimator answers the question ‚Äúwhat is the distribution of the estimator when we repetedly draw samples of size \\(n\\) from the population?‚Äù. The underlying assumption of the bootstrap method is that the sample is good representation of the underlying population distribution. We can therefore approximate the sampling distribution by sampling a lot of new datasets from the sample with replacement. This means the each new so called bootstrap sample contains only observations from the sample, but since the sampling is with replacement each such bootstrap sample will typically contain multiple copies of some observations while some observations in the original sample will not appear at all.\nTo fix ideas, assume that we have av sample of only \\(n=5\\) observations: \\(y=(3,5,1,7,2)\\). Here are five bootstrap samples:\n\ny = c(3,5,1,7,2)\nsample(y, replace = TRUE)\n\n[1] 1 7 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 5 5 3 2 5\n\nsample(y, replace = TRUE)\n\n[1] 3 2 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 7 7 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 7 1 5 1 1\n\n\nNow, for each bootstrap sample we compute the estimator, in the Poisson case the mean of the bootstrap sample. The bootstrap approximation of the sampling distribution is then approximated by a histogram of the estimates from the bootstrap samples.\n\n\nBootstrap for the univariate Poisson model\nLet‚Äôs try this out for the Poisson model with a single parameter \\(\\lambda\\), where the ML estimate was earlier shown to be \\(\\hat\\lambda=\\bar y\\) and the estimator standard error \\(SE(\\hat\\lambda)=\\sqrt{\\bar y/n}\\). The code below computes the bootstrap approximation of the sampling distribution of \\(\\hat\\lambda = \\bar y\\) for the number of bidders in the ebaycoins data.\n\ny = ebaycoins$NBidders\nnboot = 10000\nmlboot = rep(NA, nboot)\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  mlboot[j] = mean(yboot)\n}\nhist(mlboot, 100, freq = FALSE, ylim = c(0,7), c = \"steelblue\", \n     main = \"sampling distribution via the bootstrap\",\n     xlab = TeX(r'($\\hat{\\lambda}$)'))\n\n\n\n\nHere we actually know the true standard error of \\(\\hat \\lambda\\), which is \\(SE(\\hat\\lambda)=\\sqrt{\\bar y/n}\\) so we can compare with the bootstrap estimate:\n\nmessage(paste(\"true SE for ML estimator\", sqrt(mean(y)/n)))\n\ntrue SE for ML estimator 0.0602909611799314\n\nmessage(paste(\"bootstrap SE for ML estimator\", sd(mlboot)))\n\nbootstrap SE for ML estimator 0.0806338511804169\n\n\nwhich is fairly close, but in this case it is less accurate than the asymptotic standard error we obtained in the tutorial tutorial on numerical ML.\nIn this simple case we had a formula for the ML estimator \\(\\bar y\\). But what if we didn‚Äôt have such a formula (Iike in the Poisson regression case), what can we do then? Well, we can just use optim on each bootstrap sample.\nTo use optim, we need to code up the Poisson log-likelihood. Let us define this as an R function that takes the data y and the parameter lambda as inputs and returns the log-likelihood value loglik as output.\n\nloglik_pois <- function(lambda, y){\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nWe will only draw 1000 bootstrap samples so that the computations run relatively fast:\n\ny = ebaycoins$NBidders\nn = length(y)\nmle = mean(y)\nmle_se = sqrt(mean(y)/n)\nnboot = 1000\nmlboot = rep(NA, nboot)\ninitVal = 2\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  optres <- optim(initVal, loglik_pois, gr=NULL, yboot, \n                  control=list(fnscale=-1), \n                  method=c(\"BFGS\"), hessian=TRUE)\n  mlboot[j] = optres$par\n}\nhist(mlboot, 50, freq = FALSE, ylim = c(0,7), c = \"steelblue\", xlab = TeX(r'($\\hat{\\lambda}$)'), \n     main = \"sampling distribution via the bootstrap\")\n\n\n\n\nIt is important to note that I use the generated bootstrap sample yboot as the argument to loglik_pois at every iteration of the loop, i.e.¬†optim gets a fresh bootstrap sample at every iteration of the loop.\n\n\nBootstrap for the Poisson regression model\nWe will now use the bootstrap for the ML estimates in the Poisson regression model with \\(p\\) explanatory variables:\n\\[\ny_i \\vert x_i \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\lambda_i) \\text{, where }\n\\lambda_i = \\exp(\\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_p x_{i,p})\n\\]\nWe have already coded the log-likelihood using matrix-vector notation in tutorial on numerical ML.\n\nloglik_poisreg <- function(betavect, y, X){\n  lambda = exp(X %*% betavect)\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nWe can now generate bootstrap samples with replacement like before, but we need to be careful in keeping the response variable observations \\(y_i\\) and the covariate observations \\(\\mathbf{x}_i = (x_{i,1},x_{i,2},\\ldots,x_{i,p})\\) aligned. We want to same observations, but not breaking the correspondence between \\(y\\) and the covariates. This is solved by sampling observation indicies \\(i \\in \\{1,2,\\ldots,n\\}\\) with replacement and then selecting the pairs \\(y_i\\) and \\(\\mathbf{x}_i = (x_{i,1},x_{i,2},\\ldots,x_{i,p})\\) for the sampled indicies. One a bootstrap sample has been generated we compute the ML estimate \\(\\hat{\\boldsymbol{\\beta}}\\) by optim. Like this:\n\nnboot = 1000\ny = ebaycoins$NBidders\nX = cbind(1, ebaycoins$ReservePriceFrac, ebaycoins$PowerSeller)\ninitVal = c(0,0,0)\np = dim(X)[2]\nmlboot = matrix(rep(NA, nboot*p), nboot, p)\nfor (j in 1:nboot){\n  #if (j%%100 == 0){print(paste(\"iteration:\", j))}\n  bootIdx = sample(1:n, replace = TRUE) # sample observation numbers with replacement\n  optres <- optim(initVal, loglik_poisreg, gr=NULL, y[bootIdx], X[bootIdx,], \n                control=list(fnscale=-1), method=c(\"BFGS\"))\n  mlboot[j,] = optres$par\n}\npar(mfrow = c(1,3))\nhist(mlboot[,1], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_0$)'), \n     main = TeX(r'($\\hat{\\beta}_0$)'))\nhist(mlboot[,2], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_1$)'), \n     main = TeX(r'($\\hat{\\beta}_1$)'))\nhist(mlboot[,3], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_2$)'), \n     main = TeX(r'($\\hat{\\beta}_2$)'))\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Contents\n\nThis is just a skeleton template for a course that will be given in the future.\n\n\n\nLiterature\n\nAuthors (2021). Book Name\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phone‚Äôs calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\n\n\nTeachers\n\n\n\n\nMattias VillaniCourse responsible and lecturerProfessor"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nLecture 1 - Introduction.\nRead: Ch 1 | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 2 - Probability recap. \nL√§s: Slides"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Programming for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nExercise 1 - Basic probability.\nProblems: Book 2.1, 2.2, ‚Ä¶\nExercise 2 - Whatever.\nProblems: Book 3.1, 3.2, ‚Ä¶"
  },
  {
    "objectID": "computerlabs.html",
    "href": "computerlabs.html",
    "title": "Computer labs for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nComputer lab 1 \nUppgifter: html\nComputer lab 2 \nUppgifter: html\nComputer lab 3 \nUppgifter: html\nComputer lab 4 \nUppgifter: html"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nTutorials\n\nMaximum likelihood by numerical optimization html\nBootstrap html"
  },
  {
    "objectID": "tutorial/numericalML/MLnumerical.html#footnotes",
    "href": "tutorial/numericalML/MLnumerical.html#footnotes",
    "title": "Maximum likelihood by numerical optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf‚Ü©Ô∏é\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.‚Ü©Ô∏é"
  }
]