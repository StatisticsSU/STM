[
  {
    "objectID": "tutorial/numericalML/MLnumerical.html",
    "href": "tutorial/numericalML/MLnumerical.html",
    "title": "Maximum likelihood by numerical optimization",
    "section": "",
    "text": "Introduction\n\nIn this tutorial you will learn how maximum likelihood estimates and their standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is also surprisingly accurate also for smaller sample sizes.\n\nIt will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner that give you the super-power 💪 to use the computer to estimate the parameters and their uncertainty in quite complex models.\nWe will start with simple models with a single parameter to cover all the concepts, and then move on to the practially more important multi-parameter case.\nLet’s first load some libraries that we will use (install them using install.packages() if you haven’t already).\n\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution\n\n\n\nData\nWe will use the dataset ebaycoins in the R package SUdatasets for illustration. The dataset contains data from 1000 eBay auctions of collector’s coins1. Let’s load the dataset and have a look:\n\n#install_github(\"StatisticsSU/SUdatasets\") # uncomment if this is not installed\nlibrary(\"SUdatasets\")\nhead(ebaycoins)\n\n  BookVal MinorBlem MajorBlem PowerSeller IDSeller Sealed NegFeedback\n1   18.95         0         0           0        0      0           0\n2   43.50         0         0           1        0      0           0\n3   24.50         0         0           1        0      0           0\n4   34.50         1         0           0        0      0           0\n5   99.50         0         0           0        0      0           1\n6    9.50         0         0           0        0      0           0\n  ReservePriceFrac NBidders FinalPrice\n1        0.3688654        2      15.50\n2        0.2298851        6      41.00\n3        1.0200000        1      24.99\n4        0.7217391        1      24.90\n5        0.1672362        4      72.65\n6        1.2094737        2      17.52\n\n\nEach auction (rows in the dataset) will be taken as an observation, and the dataset has the following variables:\n\nthe number of bidders in each auction (NBidders)\nthe final price (FinalPrice)\nthe book value of the coin according a coin collectors catalogue (BookVal).\nthe seller’s reservations price (lowest price that the seller is willing to sell for) as a fraction the book value (ReservePriceFrac).\nbinary variables on whether or not the seller is a verified ebay seller (IDSeller), sells large quantites (PowerSeller) and if the seller has many reviews with negative feedback (NegFeedback)\ninformation about the condition of the object: if it has a minor blemish (MinorBlem), a major one (MajorBlem), or sold in its original unbroken packaging (Sealed).\n\nWe will intially analyze only the variable NBidders , the number of bidders in each auction. A histogram of the data are plotted below.\n\n\nShow the code\ndf = data.frame(prop.table(table(ebaycoins$NBidders)))\nnames(df) <- c(\"NBidders\", \"Proportions\")\n\nggplot(df, aes(x = NBidders, y = Proportions)) + \n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"NBidders\",\n    y = \"Proportion\"\n  ) +\n   theme_minimal() + # Use a minimal theme\n  theme(\n    panel.grid.major = element_blank(), # Remove major gridlines\n    panel.grid.minor = element_blank()  # Remove minor gridlines\n  )\n\n\n\n\n\n\n\nMaximum likelihood for the Poisson model\nSince NBidders is a count variable, a natural first model to consider is the Poisson model:\n\\[\nY_1,\\ldots,Y_n \\vert \\lambda \\overset{\\mathrm{iid}}{\\sim}\\mathrm{Poisson}(\\lambda)\n\\]\nwhere we use the symbol \\(Y\\) for the random variable NBidders and \\(y\\) as the observed value. The Poisson has one particularly noteworthy property: the parameter \\(\\lambda\\) is both the mean and the variance, i.e. if \\(X\\sim\\mathrm{Poisson}(\\lambda)\\), then \\(E(X)=V(X)=\\lambda\\). This makes the Poisson model suitable for count data where the mean and variance are approximately equal, which is not true for all datasets.\n\n\n\n\n\n\nInteractive exploration\n\n\n\nYou can get to know the Poisson distribution \\(P(X=x)\\) by changing its parameter \\(\\lambda\\) with the slider below. Hover over the bars in the plot if you want to see the exact probabilities. The darker shaded bars represent the distribution function, i.e. probabilities of the form \\(P(X\\leq x)\\).\n\n\n\nWe can estimate \\(\\lambda\\) by the maximum likelihood (ML) metod. The ML method finds the value for \\(\\lambda\\) in the Poisson distribution that maximizes the probability of the observed dataset \\(y_1,\\ldots,y_n\\).\n\nProbability of the observed data - only the first auction\nTo break this down, let us start with the probability of observing the number of bidders in the first auction \\(y_1\\) (which was Nbidders = 2, see the dataset above). Assuming a Poisson distribution, this probability is given by the formula (where \\(y_1\\) is the observed number of bidders in the first auction): \\[\nP(y_1 \\vert \\lambda) = \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!}\n\\]\nwhere \\(P(y_1 \\vert \\lambda)=\\mathrm{Pr}(Y_1=y_1 \\vert \\lambda)\\). Now, assume that \\(\\lambda = 3\\). The probability of observing the number of bidders in the first auction (Nbidders = 2) is then \\[\nP(y_1 = 2 \\vert \\lambda = 3) = \\frac{3^{2}e^{-3}}{2!} = 0.2240418\n\\] We can let R do this calculation for us using the dpois function\n\ndpois(x = 2, lambda = 3)\n\n[1] 0.2240418\n\n\nIf instead we had \\(\\lambda = 2\\), this probability changes to\n\ndpois(x = 2, lambda = 2)\n\n[1] 0.2706706\n\n\nThis is higher, so \\(\\lambda = 2\\) agrees better with observing Nbidders = 2 in the first auction.\n\n\nProbability of the observed data - first two auctions\nWhat if we look at the two first auctions where we have observed \\(y_1 = 2\\) and \\(y_2 = 6\\). What is the joint probability of observing \\(y_1 = 2\\) and \\(y_2 = 6\\) when \\(\\lambda = 2\\)? If we assume that the number of bidders in the different auctions are independent, then we known that this probability is the product of the individual probabilities: \\[\nP(y_1 = 2,y_2 = 6 \\vert \\lambda = 2)= P(y_1 = 2 \\vert \\lambda = 2)P(y_2 = 6 \\vert \\lambda = 2) = \\frac{2^{2}e^{-2}}{2!} \\frac{2^{6}e^{-2}}{6!}\n\\] Rather than pulling out our calculators, let R do the work for us\n\ndpois(x = 2, lambda = 2)*dpois(x = 6, lambda = 2)\n\n[1] 0.003256114\n\n\nThis probability is of course lower than the probability of only observing the first auction, but it is the relative probabilities for different \\(\\lambda\\) that interests us2. For example, the probability of observing \\(y_1 = 2\\) and \\(y_2 = 6\\) when \\(\\lambda = 3\\) is\n\ndpois(x = 2, lambda = 3)*dpois(x = 6, lambda = 3)\n\n[1] 0.01129381\n\n\nwhich is a lot higher than when \\(\\lambda=2\\). So when we added another observation \\(y_2 = 6\\), the data started to ‘prefer’ a higher \\(\\lambda\\). The reason is that \\(y_2 = 6\\) is a rather high count that is more likely to be observed if \\(\\lambda = 3\\) than if \\(\\lambda = 2\\) (remember \\(\\lambda\\) is the mean of the Poisson distribution).\nBut what is the optimal \\(\\lambda\\) for these two observations, i.e. which value of \\(\\lambda\\) gives the highest joint probability for the observed \\(y_1 = 2\\) and \\(y_2 = 6\\)? We can explore this by plotting the joint probability \\(P(y_1 = 2,y_2 = 6 \\vert \\lambda)\\) as a function of \\(\\lambda\\). We need to compute this probability for a range of \\(\\lambda\\) values in a for loop:\n\nnlambdas = 2000\nlambdas = seq(0.01, 10, length = nlambdas)\njointprob = rep(NA, length(nlambdas))\nfor (i in 1:nlambdas){\n  jointprob[i] = dpois(x = 2, lambda=lambdas[i])*dpois(x = 6,lambda=lambdas[i])\n}\nplot(lambdas, jointprob, type = \"l\", col = \"orange\", lwd = 2, \n     xlab = TeX(r'($\\lambda$)'), \n     main = \"Probability of two bidders in auction 1 and six bidders in auction 2\")\n\n\n\n\nVery nice! We can see that \\(\\lambda = 4\\) gives the highest probability to the observed number of bidders in the first two auctions. The function plotted above is called the likelihood function (for the first two observations).\n\n\nProbability of the observed data - all auctions\nIt is now easy to see that the likelihood function for all \\(n=1000\\) auctions is obtained by multiplying the Poisson probabilities for all \\(n=1000\\) auctions:\n\\[\nP(y_1,y_2,\\ldots,y_n \\vert \\lambda)=\\prod_{i=1}^n P(y_i \\vert \\lambda)\n\\]\nwhere \\(P(y_i \\vert \\lambda)\\) is the Poisson probability for observing \\(y_i\\) bidders in auction \\(i\\) in the dataset, if the parameter has the value \\(\\lambda\\). You should note that the likelihood function is the probability of the observed data viewed as a function of the parameter (like we did with the orange line above). The data observations are observed and fixed (hence the lower case letters \\(y_1,y_2,\\ldots,y_n\\)) and the likelihood function explores how the joint probability of the observed data varies as we change \\(\\lambda\\).\nThe maximum likelihood (ML) method for parameter estimation sets the parameter \\(\\lambda\\) to the value that maximizes the probability of observed data, i.e. the \\(\\lambda\\) that maximizes the likelihood function. This method can be used in in any probability model, not just the Poisson model. Knowing the ML method gives you superpowers! 💪\n\n- This model is too complex, it is impossible to estimate its parameters!\n- ML: hold my beer.\n\nWhen we start to multiply the probabilities for all data points, the joint probability becomes tiny, and we run into numerical problems. We there often choose to plot and maximize the logarithm of the likelihood, the so called log-likelihood function\n\\[\nl(\\lambda)=\\log P(y_1,y_2,\\ldots,y_n \\vert \\lambda)=\\sum_{i=1}^n \\log P(y_i \\vert \\lambda)\n\\]\n\n\n\n\n\n\nFigure 1: The logarithm is a monotonically increasing function.\n\n\n\nI have given the log-likelihood a symbol \\(l(\\lambda)\\) to really make it clear that we want to view this as a function of the parameter \\(\\lambda\\). This function does also depend on the observed data \\(y_1,y_2,\\ldots,y_n\\), but this is not written out explicitly in the symbol \\(l(\\lambda)\\), to reduce the clutter. Since the logarithm is a monotonically increasing function (see plot in margin), it does not matter if we maximize the likelihood or the log-likelihood. We get exactly the same estimate of \\(\\lambda\\). And logarithms are also often more easy to deal with mathematically. Let’s verify that we find the same maximum if we plot the log-likelihood function for the first two auctions (note how I use the log=TRUE argument in dpois to get the log of the Poisson probabilities, which I now add):\n\nnlambdas = 2000\nlambdas = seq(0.01, 10, length = nlambdas)\nloglik = rep(NA, length(nlambdas))\nfor (i in 1:nlambdas){\n  loglik[i] = dpois(x = 2, lambda = lambdas[i], log = TRUE) + \n    dpois(x = 6, lambda = lambdas[i], log = TRUE)\n}\nplot(lambdas, loglik, type = \"l\", col = \"orange\", lwd = 2, ylab = \"log-likelihood\",\n     xlab = TeX(r'($\\lambda$)'), \n     main = \"log-likelihood two bidders in auction 1 and six bidders in auction 2\")\n\n\n\n\nWe are now ready to plot the log-likelihood function based on all \\(n=1000\\) observations (auctions). It seems like a real mess to add terms like dpois(x = 6, lambda = lambdas[i], log = TRUE) for each of the 1000 observations. One solution is to use a for loop, but we can do even better (loops are slow in R). The dpois function is vectorized, meaning that it can compute the probabilties for all observations in one go. Let’s try it for the first two auctions and \\(\\lambda = 2\\):\n\n# first one observation at the time. first obs:\ndpois(x = c(2), lambda = 2)\n\n[1] 0.2706706\n\n# seconds obs\ndpois(x = c(6), lambda = 2)\n\n[1] 0.0120298\n\n# and now using that dpois accepts a vector c(2,6) as first argument\ndpois(x = c(2,6), lambda = 2)\n\n[1] 0.2706706 0.0120298\n\n\nNote how the call dpois(x = c(2,6), lambda = 2) returns a vector where each element is a probability for the corresponding observation. This also works with the log = TRUE argument\n\ndpois(x = c(2,6), lambda = 2, log = TRUE)\n\n[1] -1.306853 -4.420368\n\n\nso we can obtain the log-likelihood for all \\(n=1000\\) observations by just summing this vector:\n\nloglik_vect = dpois(x = ebaycoins$NBidders, lambda = 2, log = TRUE)\nsum(loglik_vect)\n\n[1] -2974.806\n\n\nSweet! So now we can plot the log-likelihood for all the data:\n\nnlambdas = 2000\nlambdas = seq(0.01, 10, length = nlambdas)\nloglik = rep(NA, length(nlambdas))\nfor (i in 1:nlambdas){\n  loglik[i] = sum(dpois(x = ebaycoins$NBidders, lambda = lambdas[i], log = TRUE))\n}\nplot(lambdas, loglik, type = \"l\", col = \"orange\", lwd = 2, ylab = \"log-likelihood\",\n     xlab = TeX(r'($\\lambda$)'), main = \"log-likelihood for all 1000 auctions\")\n\n\n\n\nIt is a little tricky to read of which \\(\\lambda\\) value give the maximum log-likelihood, but we can look for the maximal log-likelihood among the 2000 \\(\\lambda\\) values that we used to compute the function values in the graph:\n\nlambdas[which.max(loglik)] # which.max finds the position of loglik with the max\n\n[1] 3.633187\n\n\nwhich should be really close to the true maximum likelihood estimate.\n\n\n\n\n\n\nInteractive exploration\n\n\n\nThe interactive graph below lets you explore how different $\\lambda $ values give a different fit to the data. The graph to the left shows the log-likelihood function. The orange dot marks out the log-likelihood value for the \\(\\lambda\\) value chosen with the slider. By clicking the checkbox you will see the fit based on the ML estimate. Note that the Poisson model is a bad fit to the this data, even with the ML estimate. We will improve on this later on when we turn to Poisson regression models.\n\n\n\nOk, but do we really need to compute the log-likelihood function for a bunch of \\(\\lambda\\) values to determine where the maximum is? Fortunately no, and there are two smarter ways of getting the ML estimate:\n\nUse methods from mathematical analysis to obtain the ML estimate as a nice little formula.\nUse numerical maximization in R to let the computer find the ML estimate for us.\n\nOption 1 is nice (once the maths is done 🥵) but is usually only possible in simpler models.\nOption 2 unlocks your superpowers to easily find the ML estimate in essentially any model you like. 😍\n\n\n\nDerivation of the ML estimator and its standard error\nBut let’s torture ourselves a bit and derive the ML estimate for the Poisson model mathematically. Here we go.\nWe need to start with writing out the log-likelihood function\n\\[\\ell(\\lambda) = \\log P(y_1,y_2,\\ldots,y_n \\vert \\lambda) = \\sum_{i=1}^n \\log P(y_i \\vert \\lambda)\\]\nfor the Poisson model. In the case where the data comes from a Poisson distribution, the probability function for one observation is given by \\[P(y) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\] Therefore,\n\\[\\log P(y) = -\\lambda + y\\log\\lambda - \\log y!,\\] so the log-likelihood becomes: \n\\[\\ell(\\lambda) = \\sum_{i=1}^n\\left(-\\lambda + y_i\\log\\lambda - \\log(y_i !)\\right) = -n\\lambda + \\log\\lambda \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\log(y_i !)\\]\nWe know from calculus that the maximum of a function \\(f(x)\\) is found by setting its first derivative to zero and solving for \\(x\\). The first derivative is simply:\n\\[f'(\\lambda) = \\frac{d}{d\\lambda}\\ell(\\lambda) = -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\\]\nwhich gives the solution \\[\\lambda = \\frac{\\sum_{i=1}^n y_i}{n} = \\bar y.\\]\nTo verify that this is indeed a maximum, we can check that the second derivative is negative at \\(\\lambda = \\bar y\\). The second derivative is\n\\[f''(\\lambda)=\\frac{d^2}{d\\lambda^2}\\ell(\\lambda) = - \\frac{\\sum_{i=1}^n y_i}{\\lambda^2},\\]\nwhich is negative for all \\(\\lambda\\) because both the data and \\(\\lambda\\) must be positive.\nThus, the maximum likelihood estimate for the parameter \\(\\lambda\\) in the Poisson model is simply the mean, given by \\(\\hat\\lambda = \\bar y\\). 🥳\nLet’s check if the formula gives the same result as we obtained by plotting:\n\nmean(ebaycoins$NBidders)\n\n[1] 3.635\n\n\nYup, checks out.\nA nice thing with having a formula for the estimator is that we can then try to compute how much the estimate can vary from sample to sample, i.e. computing the standard deviation of the sampling distribution of \\(\\hat\\lambda = \\bar y\\). Let us first derive the mean of the ML estimator\n\\[\n\\mathrm{E}(\\hat\\lambda)=\\mathrm{E}(\\bar y)=\\mathrm{E}\\Bigg(\\frac{\\sum_{i=1}^n Y_i}{n}\\Bigg)=\\frac{1}{n}\\Big(\\sum_{i=1}^n \\mathrm{E} Y_i\\Big)=\\frac{1}{n}\\Big(\\sum_{i=1}^n \\lambda\\Big)=\\frac{1}{n}n\\lambda=\\lambda\n\\]\nwhere we have used the \\(\\mathrm{Y}=\\lambda\\) for a Poisson variable. This shows that the ML estimator is unbiased (which is not always true for ML estimators, except in large samples). The variance of the sampling distribution is: \\[\n\\mathrm{Var}(\\hat\\lambda) = \\mathrm{Var}(\\bar x) = \\mathrm{Var}\\Bigg(\\frac{\\sum_{i=1}^n Y_i}{n}\\Bigg) = \\frac{1}{n^2}\\mathrm{Var}\\big(\\sum_{i=1}^nY_i\\big) = \\frac{1}{n^2}n\\mathrm{Var}\\big(Y_i\\big) =  \\frac{\\lambda}{n}\n\\] where the last equality uses that the variance of a \\(\\mathrm{Pois}(\\lambda)\\) variable is \\(\\lambda\\) (remember for Poisson both the mean and variance are \\(\\lambda\\)).\n\n\nRemember that for a random variable \\(X\\) we have\n\\[\n\\mathrm{Var}(aX)=a^2\\mathrm{Var}(X)\\\\\n\\] and for independent random variables \\(X_1,X_2,\\ldots,X_n\\) we have \\[\n\\mathrm{Var}(\\sum_{i=1}^nX_i) = \\sum_{i=1}^n\\mathrm{Var}(X_i)\n\\]\nThe standard deviation is of course then \\[\nS(\\hat \\lambda) = \\sqrt{\\frac{\\lambda}{n}}\n\\]\nOk, that is a nice little formula, but how can we use it? It depends on \\(\\lambda\\), which we don’t know. The easy fix is to replace \\(\\lambda\\) with the best guess we have: the ML estimate \\(\\hat\\lambda = \\bar y\\). This gives us the standard error of the ML estimator \\[\nSE(\\hat \\lambda) = \\sqrt{\\frac{\\bar y}{n}}\n\\]\nWe can now compute the standard error for the ebaycoins data\n\nn = length(ebaycoins$NBidders)\nlambda_hat = mean(ebaycoins$NBidders)\nsqrt(lambda_hat/n)\n\n[1] 0.06029096\n\n\nwhich represents an estimate of the ‘typical’ variation of the ML estimator from sample to sample.\nSo an approximate 95% confidence interval is (assuming an approximate normal sampling distribution; since \\(n=1000>30\\) we can use the central limit theorem to motivate this):\n\nc(lambda_hat - 1.96*sqrt(lambda_hat/n), lambda_hat + 1.96*sqrt(lambda_hat/n))\n\n[1] 3.51683 3.75317\n\n\n\n\nNumerical optimization to find the ML estimator and its standard error\nWe will now learn how R (or any other programming language for data analysis) can be find the ML estimate by numerically maximizing the log-likelihood function. For this we don’t have to do the mathematical derivation that we did before, which is very important for more complex models where the maths is hard or even impossible. All R needs to know about is the log-likelihood function itself, so we need to code up such a function. This is quite easy for most models.\nLet us define an R function that takes the data y and the parameter lambda as inputs and returns the log-likelihood value loglik as output.\n\nloglik_pois <- function(lambda, y){\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nWe can try it out with some values for \\(\\lambda\\)\n\nloglik_pois(lambda = 2, y = ebaycoins$NBidders)\n\n[1] -2974.806\n\nloglik_pois(lambda = 3, y = ebaycoins$NBidders)\n\n[1] -2500.94\n\nloglik_pois(lambda = mean(ebaycoins$NBidders), y = ebaycoins$NBidders)\n\n[1] -2438.032\n\n\nwhere the last value is for the ML estimate \\(\\hat\\lambda = \\bar y\\).\n\n\n\n\n\n\nFirst argument must be the parameter that you optimize\n\n\n\nIt is crucial that the very first argument in your log-likelihood function is the parameter that you want to find ML estimate for. So you have to write function(lambda, y) and not function(y, lambda) in the function definition. As usual in programming, the exact name lambda is not required, but whatever you call the model parameter, it has to go first in the function definition.\n\n\nNow that we have told R about the function that we want to maximize, we can use the optim function to do the maximization (optimization is the term used for both finding the minimum or the maximum of a function). Here is the code and output, which I will now explain.\n\ninitVal = 2\noptres <- optim(initVal, loglik_pois, gr=NULL, ebaycoins$NBidders, control=list(fnscale=-1), \n                method=c(\"BFGS\"), hessian=TRUE)\noptres$par\n\n[1] 3.635\n\n\nFirst, note that the result from the optimization is return to a list, which I have named optres. The slot optres$par contains the value for lambda that maximizes the log-likelihood function. It agrees with our earlier estimates. Good, but what on earth are all those arguments to the optim function? Let me tell you:\n\nthe first argument, which I have named initVal, is the initial value that R starts out with. The optim function then finds the maximum by iteratively moving this initial value to a better and better value (higher log-likelihood). For simple problems, this choice is not so important (try to change it in the Quarto notebook), but for models with a large number of parameters it can matter. The better the initial guess, the quicker the algorithm will reach the maximum.\nthe second argument, loglik_pois, is the name of the function that you want to maximize.\nthe third argument just says that we don’t know the derivative of the log-likelihood function. If we would supply this, the algorithm would be even quicker, but most of the time we just say it is NULL.\nthe arguments following the third argument, in this case only ebaycoins$NBidders, are all the things that R needs to evaluate the log-likelihood function loglik_pois. We only have one such argument, the data variable y which in our case here is the vector with the number of bidders in each auction, ebaycoins$NBidders. It is common to have more than one such variable, and then you just keep adding them with a comma (,) between each argument. We will see an example of this later when we look at Poission regression where we also need to supply the covariate data.\nthe very cryptic argument control=list(fnscale=-1) tells R that we want to maximize rather than minimize. The reason is that the optim function does minimization by default, and control=list(fnscale=-1) tells R to multiply the log-likelihood function by \\(-1\\) which flips the function upside down, turning the minimization into a maximization. It is a little hacky, so just get used to always have this argument when you want to maximize, rather than minimize.\nthe final two arguments method=c(\"BFGS\") and hessian=TRUE tell R to use a specific optimization method that also allows us to obtain things we need to approximate the standard error of the ML estimate. More on this below.\n\n\n\nApproximate sampling distribution of the ML estimator in large samples\nWe now know how to find the ML estimate for any model for which we can code up the log-likelihood function. Pretty cool! But we can do more! In large samples we can approximate the sampling distribution of the ML estimator \\(\\hat\\lambda\\) in essentially any model with the following normal distribution \\[\n\\hat\\lambda \\overset{\\mathrm{approx}}{\\sim} N\\Bigg(\\lambda_0, \\frac{1}{-l''(\\lambda_0)}\\Bigg)\n\\] where \\(\\lambda_0\\) is the true parameter in the population model and \\[\nl''(\\lambda) = \\frac{d^2}{d\\lambda^2}l(\\lambda)\n\\] is the second derivate of the log-likelihood function \\(l(\\lambda)\\). So \\(l''(\\lambda_0)\\) is the second derivative of the log-likelihood function evaluated in the point \\(\\lambda = \\lambda_0\\). This says that the sampling distribution of the ML estimator will be approximately normal in large samples (\\(n\\) large) with a standard deviation of \\[\n\\frac{1}{\\sqrt{-l''(\\lambda_0)}}\n\\] Since we don’t know the \\(\\lambda_0\\) that generated the data we have to estimate it and the standard error is therefore \\[\n\\frac{1}{\\sqrt{-l''(\\hat \\lambda)}}\n\\] where \\(\\hat\\lambda\\) is the ML estimate, which in our Poisson case is \\(\\hat\\lambda = \\bar y\\). We were able to compute \\(\\sqrt{l''(\\hat \\lambda)}\\) mathematically for the Poisson model above when we verified that \\(\\hat\\lambda = \\bar y\\) was indeed the maximal point. By the way, don’t worry about the negative sign that makes it look like we are taking the square root of a negative number. The number under the square root sign is always positive, because \\(l''(\\hat \\lambda)\\) is always negative (remember that we do check if a maximum was found by checking if the second derivative is negative at the maximum).\nIn summary, the following approximate sampling distribution of the ML estimator is very accurate in large samples (\\(n>30\\) is a rule of thumb) \\[\n\\hat\\lambda \\overset{\\mathrm{approx}}{\\sim} N\\Bigg(\\lambda_0, \\frac{1}{-l''(\\hat\\lambda)}\\Bigg)\n\\]\nWhy does the second derivative appear here at all? Is there any intution for it? There is. Maybe you know that the first derivative \\(f'(x)\\) of a function \\(f(x)\\) measures the slope (tangent) of the function at the point \\(x\\). The second derivative measures how fast the first derivative changes. So when computing the second derivative at the maximum (mode) we get a measure of the curvature around the maximum of the log-likelihood, how peaky the log-likelihood is there. So for a log-likelihood which is very peaky there is clearly a lot of information in the data to pin down a good estimate. A peaky function has a large second derivative (the first derivative changes rapidly around the mode) and \\(\\frac{1}{-l''(\\hat\\lambda)}\\) become very small, i.e. a small standard error.\n\n\n\n\n\n\nInteractive exploration\n\n\n\nThe widget below shows how the second derivative (which measures the speed of change in the first derivative) measures how peaky a function is. The orange line represent the first derivative, which you can see changes rapidly in the peaky function when you change the point where the derivative is evaluated.\n\n\n\nThe beauty of it all is that optim can actually also compute the second derivative at the ML estimate, so optim gives all we need for computing the (approximate) standard error! This is where the arguments method=c(\"BFGS\") and hessian=TRUE come into play. By using these two arguments, optim will also return the second derivative evaluated at the ML estimate: \\(l''(\\hat \\lambda)\\) (hessian is basically another word for second derivative). Here is the complete code:\n\ninitVal = 2\noptres <- optim(initVal, loglik_pois, gr=NULL, ebaycoins$NBidders, \n                control=list(fnscale=-1), method=c(\"BFGS\"), hessian=TRUE)\nmle = optres$par\nmle_se = 1/sqrt(-optres$hessian[1])\nmessage(paste(\"The ML estimate of lambda is\", round(mle, 4), \n              \" with an approximate standard error of\", round(mle_se,4) ))\n\nThe ML estimate of lambda is 3.635  with an approximate standard error of 0.0603\n\n\nNote how the slot optres$hessian contains the second derivative at the ML estimate (the maximal point) and how well the approximate standard error agrees with the exact standard error derived above \\[\n\\sqrt{\\frac{\\bar y}{n}} = 0.0602909\n\\] We can plot the approximate normal sampling distribution\n\nlambdas = seq(mle - 4*mle_se, mle + 4*mle_se, by = 0.001)\nplot(lambdas, dnorm(lambdas, mle, mle_se), type =\"l\", main = \"sampling distribution\",\n     col = \"orange\", lwd = 2, xlab = TeX(r'($ \\hat{\\lambda}$)'))\n\n\n\n\n\n\nMulti-parameter models\n\nPoisson regression with one covariate\nWe will now consider the case with models that have more than one parameter, which is the usual case in applied statistical work. As an example, consider the Poisson regression model where the response count \\(y\\) is regressed on a single covariate \\(x\\): \\[\ny_i \\vert x_i \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\lambda_i) \\text{, where }\n\\lambda_i = \\exp(\\beta_0 + \\beta_1 x_i)\n\\] where each observation \\(y_i\\) has its own Poisson mean \\(\\lambda_i\\) which are in turn modelled as a function of the covariate \\(x_i\\). We use the exponential function to make sure that \\(\\lambda_i>0\\) for any \\(\\beta_0\\), \\(\\beta_1\\) and covariate values \\(x_i\\).\nIf you think that the above is a weird way of writing a regression, note that the usual linear regression with normal error terms \\[\ny_i = \\beta_0 + \\beta_1 + \\varepsilon_i, \\text{, where } \\varepsilon_i \\overset{\\mathrm{iid}}{\\sim} N(0,\\sigma^2)\n\\]\ncan equally well be written \\[\ny_i \\vert x_i  \\overset{\\mathrm{indep}}{\\sim} \\mathrm{N}(\\mu_i, \\sigma^2) \\text{, where }\n\\mu_i = \\beta_0 + \\beta_1 x_i.\n\\] Here we don’t need to mess around with exponential function since the mean in the normal distribution is allowed to also be negative. If you are wondering how one can write the Poisson regression with some sort of error term \\(\\varepsilon\\), then stop wondering now. You can’t!\nSuppose we want to explain the number of bidders (NBidders) in an auction with the covariate ReservePriceFrac (the seller’s lowest acceptable price divided by the coin’s book value). This seems like a sensible covariate since if you set a high reservation price you will most likely attract fewer bidders. We can still use the maximum likelihood idea: choose the parameters \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the probability of the observed number of bids. As is usual in regression, the covariate \\(x\\) is taken to be fixed, so we don’t worry about the probability for this variable. Here we run into trouble: while we can compute the (partial) derivative of the log-likelihood for each of the two parameters, we can’t solve the resulting two equations to find formulas for the ML estimates \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\).\nAny suggestions on how to proceed? Optim! Let’s code up the log likelihood for the Poisson regression\n\nloglik_poisreg <- function(betavect, y, x){\n  lambda = exp(betavect[1] + betavect[2]*x)\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nNote in particular that we used a vector as first argument, a vector containing both parameters, \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). As mentioned before, optim wants the first argument of the optimized function to be the parameter, and when the model contains more than one parameter, you need to pack them in a vector like I did here.\n\n\n\n\n\n\nname calling can hurt\n\n\n\nDon’t use the name beta as variable name, since R already uses the name beta for the Beta function.\n\n\nAlright, let’s try out our function for a couple of \\(\\beta_0\\) and \\(\\beta_1\\) values, just to see if it works:\n\nloglik_poisreg(betavect = c(1,1), y = ebaycoins$NBidders, \n               x = ebaycoins$ReservePriceFrac)\n\n[1] -3709.59\n\nloglik_poisreg(betavect = c(1,-1), y = ebaycoins$NBidders, \n               x = ebaycoins$ReservePriceFrac)\n\n[1] -2857.133\n\n\nfrom which we see that the observed \\(y\\) data in the \\(1000\\) auctions is much more probable with the parameters \\(\\beta_0=1\\) and \\(\\beta_1=-1\\) than with the parameters \\(\\beta_0=1\\) and \\(\\beta_1=1\\); this makes sense since we expect the number of bidders to decrease with larger ReservePriceFrac.\nLet’s throw this at the optim function to get the ML estimate:\n\ny = ebaycoins$NBidders\nx = ebaycoins$ReservePriceFrac\ninitVal = c(0,0)\noptres <- optim(initVal, loglik_poisreg, gr=NULL, y, x, control=list(fnscale=-1), \n                method=c(\"BFGS\"), hessian=TRUE)\nmle = optres$par\nmessage(paste(\"The ML estimate of beta0 is\", round(mle[1], 4) ))\n\nThe ML estimate of beta0 is 2.1077\n\nmessage(paste(\"The ML estimate of beta1 is\", round(mle[2], 4) ))\n\nThe ML estimate of beta1 is -1.7434\n\n\nHoly smoke, that is cool! Note that after the argument gr=NULL I have both y and x as the additional arguments needed to evaluate the loglik_poisreg function. The return value optres$par is a vector containing the two \\(\\beta\\) parameters.\n\n\n\n\n\n\nInteractive exploration\n\n\n\nExplore how changing values for \\(\\beta_0\\) and \\(\\beta_1\\) changes the fit of the Poisson regression by dragging the sliders below. The graph at the bottom shows the two-dimensional log-likelihood function in the form of a contour plot of \\(l(\\beta_0,\\beta_1)\\). You should think of the log-likelihood function like a mountain and each of the contours (the ellipses) as levels around the mountain of a specific fixed altitude (height). The smaller contours are higher up the mountain. The orange dot shows the log-likelihood of the Poisson regression fit chosen with the sliders.\nBy clicking on the checkbox “show ML fit” you get to see the optimal maximum likelihood fit (note that the log-likelihood for the ML fit is at the top of the mountain, as it should since this is literally the definition of ML).\n\n\n\nWhat about the standard errors of \\(\\beta_0\\) and \\(\\beta_1\\)? Can we approximate those similarly to the case with one model parameter? Yes! But we need the following more complicated result for the large-sample approximate sampling distribution, which will take a little of explaining. We need to know what a bivariate normal distribution is.\nA bivariate normal distribution for two random variables \\(X\\) and \\(Y\\) is essentially a univariate normal distribution for each of the two random variables \\(X\\sim N(\\mu_x,\\sigma_x^2)\\) and \\(Y\\sim N(\\mu_y,\\sigma_y^2)\\) and a correlation coefficient \\(\\rho\\)\n\\[\n\\rho = \\frac{\\mathrm{Cov}(X,Y)}{SD(X) \\cdot SD(Y)}\n\\]that determines the degree of dependence between the two variables. We write\n\\[\n(X,Y) \\sim N(\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\rho)\n\\]\nwhich shows that the distribution has five parameters. The joint density for the bivariate normal distribution is given by the fairly complicated expression (that you shouldn’t learn to memorize)\n\\[\n\\small\np(x,y)=\\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\\exp\\Bigg(\n-\\frac{1}{2(1-\\rho^2)} \\bigg[\n\\Big( \\frac{x-\\mu_x}{\\sigma_x} \\Big)^2 + \\Big( \\frac{y-\\mu_y}{\\sigma_y} \\Big)^2 -2\\rho\\Big( \\frac{x-\\mu_x}{\\sigma_x} \\Big) \\Big( \\frac{y-\\mu_y}{\\sigma_y} \\Big)\n\\bigg]\n\\Bigg)\n\\]\n\n\n\n\n\n\nInteractive exploration\n\n\n\nYou can experiment with the five parameters in the bivariate normal distribution with the sliders below. Note in particular what happens when you change the correlation coefficient parameter \\(\\rho\\) to positive and negative values.\n\n\n\nAn alternative way to express that two variables follow a bivariate normal distribution is\n\\[\n(X,Y) \\sim N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n\\]\nwhere \\(\\boldsymbol{\\mu}\\) is the mean vector that contains the two means\n\\[\n\\boldsymbol{\\mu} = \\pmatrix{\\mu_y \\\\ \\mu_y}\n\\]\nand \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix\n\\[\n\\boldsymbol{\\Sigma} = \\pmatrix{\\sigma_x^2 \\hspace{1cm} \\rho\\sigma_x\\sigma_y \\\\\n\\rho\\sigma_x\\sigma_y \\hspace{1cm}\\sigma_y^2}\n\\]\nwhere the elements on the diagonal (\\(\\sigma_x^2\\) and \\(\\sigma_y^2\\)) are the variances and the off-diagonal elements (which are the same) are the covariances \\(\\mathrm{Cov}(X,Y) = \\rho\\sigma_x\\sigma_y\\).\nWe can now express the approximate sampling distribution of the ML estimates of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) :\n\nThe sampling distribution of the ML estimate of the vector with Poisson regression coefficients can be approximated in large samples (n large) by a bivariate normal distribution:\n\\[\n\\hat{\\boldsymbol{\\beta}} = \\pmatrix{\\hat\\beta_0 \\\\ \\hat \\beta_1} \\overset{\\mathrm{approx}}{\\sim} N(\\boldsymbol{\\beta}, \\Sigma)\n\\]\n\nwhere the standard deviation of \\(\\hat\\beta_0\\) is the square root of the element in first row and first column of \\(\\boldsymbol{\\Sigma}\\) and the standard deviation of \\(\\hat\\beta_1\\) is the square root of the element in second row and second column of \\(\\boldsymbol{\\Sigma}\\).\nWe already know how to obtain the ML estimate of \\(\\hat{\\boldsymbol{\\beta}}\\) with optim, but how can we use optim to compute an estimate of the covariance matrix \\(\\boldsymbol{\\Sigma}\\)? It turns out that we can use almost the same code, the arguments method=c(\"BFGS\") and hessian=TRUE will cause optim to return a matrix of second derivatives, as Hessian matrix. If you remember how we used the second derivative to obtain the standard error when we only had one parameter: mle_se = 1/sqrt(-optres$hessian[1]) we can do nearly the same to obtain the standard errors in the bivariate case:\nmle_cov = solve(-optres$hessian)\nmle_se = sqrt(diag(mle_cov))\nwhere the first line of code computes an estimate of \\(\\boldsymbol{\\Sigma}\\) and the second line of code extracts the diagonal elements (this is diag(mle_cov)) and then directly computes the square root of each element to obtain the standard error. The only weird part is that the division is now done with the solve function. This function computes the matrix inverse, which is a special type of division for matrices, like covariance matrices. You don’t need to worry to much about it now, just know that the following code gives us the ML estimates and their respective standard errors.\n\ny = ebaycoins$NBidders\nx = ebaycoins$ReservePriceFrac\ninitVal = c(0,0)\noptres <- optim(initVal, loglik_poisreg, gr=NULL, y, x, control=list(fnscale=-1), \n                method=c(\"BFGS\"), hessian=TRUE)\nmle = optres$par\nmle_cov = solve(-optres$hessian)\nmle_se = sqrt(diag(mle_cov))\nmessage(paste(\"The ML estimate of beta0 is\", round(mle[1], 4), \n              \"with approx standard error\", mle_se[1] ))\n\nThe ML estimate of beta0 is 2.1077 with approx standard error 0.0268950137116756\n\nmessage(paste(\"The ML estimate of beta1 is\", round(mle[2], 4), \n              \"with approx standard error\", mle_se[2] ))\n\nThe ML estimate of beta1 is -1.7434 with approx standard error 0.0562816780005428\n\n\n\n\nPoisson regression with two covariates\nAll of this is great, but what if I want to add another covariate to the model, perhaps the binary covariate PowerSeller? Now I have a Poisson regression model with three parameters\n\\[\ny_i \\vert x_i \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\lambda_i) \\text{, where }\n\\lambda_i = \\exp(\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2})\n\\]\nwhere \\(x_1\\) is the old ReservePriceFrac covariate and \\(x_2\\) is the new PowerSeller covariate. So \\(x_{5,2}\\) is for example the \\(5\\)th observation on the second covariate, PowerSeller .\nThe log-likelihood \\(l(\\beta_0 ,\\beta_1 , \\beta_2)\\) is now a function of three variables, which makes it hard to visualize in a plot. But we can obtain the ML estimates and standard errors by optim, in exact the same way. We would have to define the log-likelihood function:\n\nloglik_poisreg <- function(betavect, y, x1, x2){\n  lambda = exp(betavect[1] + betavect[2]*x1 + betavect[3]*x2)\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nand then run optim on it.\n\n\nPoisson regression with arbitrary number of covariates\nIt is however a little annoying to have to rewrite the log-likelihood function every time we decide to add a covariate to the regression. There is a beautiful way around this, but it requires a final step to explain. Still have the energy to learn something new? Let’s go!\nFirst thing is that we add all the covariates as columns to a matrix. For the case with two covariates, it looks like this:\n\\[\n\\mathbf{X} = \\pmatrix{\n1 \\; x_{ 11} \\; x_{12} \\\\\n1 \\; x_{ 21} \\; x_{22} \\\\\n\\vdots \\;\\;\\;\\; \\vdots \\;\\;\\;\\; \\vdots \\\\\n1 \\; x_{ n1} \\; x_{n2}\n}\n\\]\nThe dots \\(\\vdots\\) are just saying that there a bunch of rows that I am not writing out here. There is nothing really special here, it is just the mathematical way of saying a table with data. Well, ok, something is a little special: what is the deal with the first column with only 1’s? Those are there to represent the intercept \\(\\beta_0\\). If you think of it, the intercept in the regression effect \\(\\exp(\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2})\\) is sort of like having a covariate \\(x_{i,0}\\) that always takes the value one, since trivially \\(\\beta_0 \\cdot 1 = \\beta_0\\). Finally, \\(\\mathbf{X}\\) is in bold UPPERCASE font, to show that it is a matrix. Let’s set up this matrix in R code using the cbind function in R (column bind):\n\nX = cbind(1,ebaycoins$ReservePriceFrac, ebaycoins$PowerSeller)\nhead(X)\n\n     [,1]      [,2] [,3]\n[1,]    1 0.3688654    0\n[2,]    1 0.2298851    1\n[3,]    1 1.0200000    1\n[4,]    1 0.7217391    0\n[5,]    1 0.1672362    0\n[6,]    1 1.2094737    0\n\n\nHere used the cbind function to construct the matrix. There is also the matrix command in R to construct any type of matrix from its elements. Here is an example with a \\(3\\times2\\) matrix (three rows and two columns):\n\nA = matrix(c(1,2,5,0.4,3,10), 3, 2)\nA\n\n     [,1] [,2]\n[1,]    1  0.4\n[2,]    2  3.0\n[3,]    5 10.0\n\n\nWe can similarly put all the observations on the response variable in a long vector:\n\\(\\mathbf{y}=\\pmatrix{y_1 \\\\y_2 \\\\ \\vdots \\\\ y_n}\\) which is in bold lowercase font to show that it is a vector. We have already set up this before, but lets do it again:\n\ny = ebaycoins$NBidders\n\nThe often used c function in R constructs general vectors:\n\nb = c(1,4)\nb\n\n[1] 1 4\n\n\nFinally, we can also (as we already did above) pack all the regression coefficient in a vector \\[\\boldsymbol{\\beta} = \\pmatrix{\\beta_0  \\\\ \\beta_1 \\\\ \\beta_2\n}\n\\] Here comes the beautiful, but little tricky part, we can multiply the matrix \\(\\mathbf{X}\\) with the vector \\(\\boldsymbol{\\beta}\\), in a very specific way that gives the end result \\[\n\\mathbf{X}*\\boldsymbol{\\beta} = \\pmatrix{\\beta_0 + \\beta_1 x_{1,1} + \\beta_2 x_{1,2} \\\\ \\beta_0 + \\beta_1 x_{2,1} + \\beta_2 x_{2,2} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} }\n\\]\nWhat I have denote by the *-symbol is the matrix multiplication operator, which we applied to a matrix and and vector gives the end result above. We will not define the matrix multiplication here, but you can read more about it if you want on wikipedia. This matrix multiplication is convenient, since we can then compute all the \\(\\lambda\\)s in the Poisson regression at once:\n\\[\n\\pmatrix{\\lambda_1 \\\\ \\lambda_2 \\\\ \\vdots \\\\ \\lambda_n} = \\exp(\\mathbf{X}*\\boldsymbol{\\beta}) = \\pmatrix{\\exp(\\beta_0 + \\beta_1 x_{1,1} + \\beta_2 x_{1,2}) \\\\ \\exp(\\beta_0 + \\beta_1 x_{2,1} + \\beta_2 x_{2,2}) \\\\ \\vdots \\\\ \\exp(\\beta_0 + \\beta_1 x_{n,1} + \\beta_2 x_{n,2} )}\n\\]\nbecause in R the exp function is vectorized, so exp() on a vector will return a vector where the exponential function is applied to each element separetely. R uses the symbol %*% for matrix multiplication. Let’s try to use it:\n\nA = matrix(c(1,2,5,0.4,3,10), 3, 2)\nb = c(1,4)\nA%*%b\n\n     [,1]\n[1,]  2.6\n[2,] 14.0\n[3,] 45.0\n\n\nOk, that’s it, with this new matrix terminology and the matrix product, we can define the log-likelihood function generally for any number of covariates placed in the columns of \\(\\mathbf{X}\\):\n\nloglik_poisreg <- function(betavect, y, X){\n  lambda = exp(X %*% betavect)\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nNote that this works for any \\(\\mathbf{X}\\), by just adding more and more covariates as columns. But let’s try to use it now for the regression with two covariates:\n\ny = ebaycoins$NBidders\nX = cbind(1, ebaycoins$ReservePriceFrac, ebaycoins$PowerSeller)\nbetavect = c(0,-1, 0.2)\nloglik_poisreg(betavect, y, X)\n\n[1] -5208.825\n\n\nWe can now finally find the ML estimates and standard errors from optim:\n\ny = ebaycoins$NBidders\nX = cbind(1, ebaycoins$ReservePriceFrac, ebaycoins$PowerSeller)\ninitVal = c(0,0,0)\noptres <- optim(initVal, loglik_poisreg, gr=NULL, y, X, control=list(fnscale=-1), \n                method=c(\"BFGS\"), hessian=TRUE)\nmle = optres$par\nmle_cov = solve(-optres$hessian)\nmle_se = sqrt(diag(mle_cov))\nmessage(paste(\"The ML estimate of beta0 is\", round(mle[1], 4), \n              \"with approx standard error\", round(mle_se[1], 4) ))\n\nThe ML estimate of beta0 is 2.1019 with approx standard error 0.028\n\nmessage(paste(\"The ML estimate of beta1 is\", round(mle[2], 4), \n              \"with approx standard error\", round(mle_se[2], 4) ))\n\nThe ML estimate of beta1 is -1.7579 with approx standard error 0.0597\n\nmessage(paste(\"The ML estimate of beta2 is\", round(mle[3], 4), \n              \"with approx standard error\", round(mle_se[3], 4) ))\n\nThe ML estimate of beta2 is 0.0262 with approx standard error 0.0355\n\n\n😍😍😍\nSo now we know how to find the ML estimates and standard errors using numerical optimization. Before we also said that the sampling distribution was approximately normal (Poisson model with one parameter) or bivariate normal (Poisson regression with one covariate and two parameters \\(\\beta_0\\) and \\(\\beta_1\\)). But what is the sampling distribution in the case with two or more covariates? 🤔 It is what we call a multivariate normal distribution, and we write it exactly like we did for the bivariate case, but this time with \\(p\\) variables\n\\[\n(X_1,X_2,\\ldots,X_p) \\sim N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n\\]\nwhere \\(\\boldsymbol{\\mu}\\) is the mean vector that now contains \\(p\\) means\n\\[\n\\boldsymbol{\\mu} = \\pmatrix{\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p}\n\\]\nand \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix which now has \\(p\\) rows and \\(p\\) columns:\n\\[\n\\boldsymbol{\\Sigma} = \\pmatrix{\n\\sigma_1^2 & \\rho_{12}\\sigma_1\\sigma_2 & \\cdots & \\rho_{1p}\\sigma_1\\sigma_p \\\\\n\\rho_{12}\\sigma_1\\sigma_2 & \\sigma_2^2 & \\cdots &\\rho_{2p}\\sigma_2\\sigma_p \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n\\rho_{1p}\\sigma_1\\sigma_p & \\rho_{2p}\\sigma_1\\sigma_p & \\cdots & \\sigma_p^2 }\n\\]\nwhere the diagonal elements are the variance for each variable, and the \\(\\rho_{ij}\\) are the correlation coefficients between all pairs of variables.\n\n\n\n\n\n\nFootnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf↩︎\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.↩︎"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html",
    "href": "tutorial/bootstrap/bootstrap.html",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "",
    "text": "In this tutorial you will learn about the bootstrap method for approximating the sampling distribution of any estimator, for example the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\n\nLet’s first load some libraries that we will use (install them using install.packages() if you haven’t already).\n\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution\n\n\nData\nWe will use the dataset ebaycoins in the R package SUdatasets for illustration. The dataset contains data from 1000 eBay auctions of collector’s coins1. Let’s load the dataset and have a look:\n\n#install_github(\"StatisticsSU/SUdatasets\") # uncomment if this is not installed\nlibrary(\"SUdatasets\")\nhead(ebaycoins)\n\n  BookVal MinorBlem MajorBlem PowerSeller IDSeller Sealed NegFeedback\n1   18.95         0         0           0        0      0           0\n2   43.50         0         0           1        0      0           0\n3   24.50         0         0           1        0      0           0\n4   34.50         1         0           0        0      0           0\n5   99.50         0         0           0        0      0           1\n6    9.50         0         0           0        0      0           0\n  ReservePriceFrac NBidders FinalPrice\n1        0.3688654        2      15.50\n2        0.2298851        6      41.00\n3        1.0200000        1      24.99\n4        0.7217391        1      24.90\n5        0.1672362        4      72.65\n6        1.2094737        2      17.52\n\n\nEach auction (rows in the dataset) will be taken as an observation, and the dataset has the following variables:\n\nthe number of bidders in each auction (NBidders)\nthe final price (FinalPrice)\nthe book value of the coin according a coin collectors catalogue (BookVal).\nthe seller’s reservations price (lowest price that the seller is willing to sell for) as a fraction the book value (ReservePriceFrac).\nbinary variables on whether or not the seller is a verified ebay seller (IDSeller), sells large quantites (PowerSeller) and if the seller has many reviews with negative feedback (NegFeedback)\ninformation about the condition of the object: if it has a minor blemish (MinorBlem), a major one (MajorBlem), or sold in its original unbroken packaging (Sealed).\n\n\n\nMaximum likelihood for the Poisson model\nWe will first analyze only the variable NBidders and later move over to a regression modeling situation. Since NBidders is a count variable, a natural first model to consider is the Poisson model:\n\\[\nY_1,\\ldots,Y_n \\vert \\lambda \\overset{\\mathrm{iid}}{\\sim}\\mathrm{Poisson}(\\lambda)\n\\]\nwhere we use the symbol \\(Y\\) for the random variable NBidders and \\(y\\) as the observed value.\nWe can estimate \\(\\lambda\\) by the maximum likelihood (ML) metod. The ML method finds the value for \\(\\lambda\\) in the Poisson distribution that maximizes the probability of the observed dataset \\(y_1,\\ldots,y_n\\). As we have seen in tutorial on numerical ML, the ML estimate for this model is just the sample mean \\(\\hat \\lambda = \\bar y\\) and the standard error is \\(\\mathrm{SE}(\\hat\\lambda)=\\sqrt{\\bar y/n}\\). For NBidders in the ebayscoins data we have\n\nn = length(ebaycoins$NBidders)\nmessage(paste(\"ML estimate:\", mean(ebaycoins$NBidders)))\n\nML estimate: 3.635\n\nmessage(paste(\"SE of ML estimate:\", sqrt(mean(ebaycoins$NBidders)/n)))\n\nSE of ML estimate: 0.0602909611799314\n\n\n\n\nSampling distribution by the bootstrap\nThe bootstrap is an simulation-based technique for obtaining an approximation to the sampling distribution. Recall first that the sampling distribution of an estimator answers the question “what is the distribution of the estimator when we repetedly draw samples of size \\(n\\) from the population?”. The underlying assumption of the bootstrap method is that the sample is good representation of the underlying population distribution. We can therefore approximate the sampling distribution by sampling a lot of new datasets from the sample with replacement. This means the each new so called bootstrap sample contains only observations from the sample, but since the sampling is with replacement each such bootstrap sample will typically contain multiple copies of some observations while some observations in the original sample will not appear at all.\nTo fix ideas, assume that we have av sample of only \\(n=5\\) observations: \\(y=(3,5,1,7,2)\\). Here are five bootstrap samples:\n\ny = c(3,5,1,7,2)\nsample(y, replace = TRUE)\n\n[1] 1 7 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 5 5 3 2 5\n\nsample(y, replace = TRUE)\n\n[1] 3 2 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 7 7 5 3 3\n\nsample(y, replace = TRUE)\n\n[1] 7 1 5 1 1\n\n\nNow, for each bootstrap sample we compute the estimator, in the Poisson case the mean of the bootstrap sample. The bootstrap approximation of the sampling distribution is then approximated by a histogram of the estimates from the bootstrap samples.\n\n\nBootstrap for the univariate Poisson model\nLet’s try this out for the Poisson model with a single parameter \\(\\lambda\\), where the ML estimate was earlier shown to be \\(\\hat\\lambda=\\bar y\\) and the estimator standard error \\(SE(\\hat\\lambda)=\\sqrt{\\bar y/n}\\). The code below computes the bootstrap approximation of the sampling distribution of \\(\\hat\\lambda = \\bar y\\) for the number of bidders in the ebaycoins data.\n\ny = ebaycoins$NBidders\nnboot = 10000\nmlboot = rep(NA, nboot)\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  mlboot[j] = mean(yboot)\n}\nhist(mlboot, 100, freq = FALSE, ylim = c(0,7), c = \"steelblue\", \n     main = \"sampling distribution via the bootstrap\",\n     xlab = TeX(r'($\\hat{\\lambda}$)'))\n\n\n\n\nHere we actually know the true standard error of \\(\\hat \\lambda\\), which is \\(SE(\\hat\\lambda)=\\sqrt{\\bar y/n}\\) so we can compare with the bootstrap estimate:\n\nmessage(paste(\"true SE for ML estimator\", sqrt(mean(y)/n)))\n\ntrue SE for ML estimator 0.0602909611799314\n\nmessage(paste(\"bootstrap SE for ML estimator\", sd(mlboot)))\n\nbootstrap SE for ML estimator 0.0806338511804169\n\n\nwhich is fairly close, but in this case it is less accurate than the asymptotic standard error we obtained in the tutorial tutorial on numerical ML.\nIn this simple case we had a formula for the ML estimator \\(\\bar y\\). But what if we didn’t have such a formula (Iike in the Poisson regression case), what can we do then? Well, we can just use optim on each bootstrap sample.\nTo use optim, we need to code up the Poisson log-likelihood. Let us define this as an R function that takes the data y and the parameter lambda as inputs and returns the log-likelihood value loglik as output.\n\nloglik_pois <- function(lambda, y){\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nWe will only draw 1000 bootstrap samples so that the computations run relatively fast:\n\ny = ebaycoins$NBidders\nn = length(y)\nmle = mean(y)\nmle_se = sqrt(mean(y)/n)\nnboot = 1000\nmlboot = rep(NA, nboot)\ninitVal = 2\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  optres <- optim(initVal, loglik_pois, gr=NULL, yboot, \n                  control=list(fnscale=-1), \n                  method=c(\"BFGS\"), hessian=TRUE)\n  mlboot[j] = optres$par\n}\nhist(mlboot, 50, freq = FALSE, ylim = c(0,7), c = \"steelblue\", xlab = TeX(r'($\\hat{\\lambda}$)'), \n     main = \"sampling distribution via the bootstrap\")\n\n\n\n\nIt is important to note that I use the generated bootstrap sample yboot as the argument to loglik_pois at every iteration of the loop, i.e. optim gets a fresh bootstrap sample at every iteration of the loop.\n\n\nBootstrap for the Poisson regression model\nWe will now use the bootstrap for the ML estimates in the Poisson regression model with \\(p\\) explanatory variables:\n\\[\ny_i \\vert x_i \\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\lambda_i) \\text{, where }\n\\lambda_i = \\exp(\\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_p x_{i,p})\n\\]\nWe have already coded the log-likelihood using matrix-vector notation in tutorial on numerical ML.\n\nloglik_poisreg <- function(betavect, y, X){\n  lambda = exp(X %*% betavect)\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n\nWe can now generate bootstrap samples with replacement like before, but we need to be careful in keeping the response variable observations \\(y_i\\) and the covariate observations \\(\\mathbf{x}_i = (x_{i,1},x_{i,2},\\ldots,x_{i,p})\\) aligned. We want to same observations, but not breaking the correspondence between \\(y\\) and the covariates. This is solved by sampling observation indicies \\(i \\in \\{1,2,\\ldots,n\\}\\) with replacement and then selecting the pairs \\(y_i\\) and \\(\\mathbf{x}_i = (x_{i,1},x_{i,2},\\ldots,x_{i,p})\\) for the sampled indicies. One a bootstrap sample has been generated we compute the ML estimate \\(\\hat{\\boldsymbol{\\beta}}\\) by optim. Like this:\n\nnboot = 1000\ny = ebaycoins$NBidders\nX = cbind(1, ebaycoins$ReservePriceFrac, ebaycoins$PowerSeller)\ninitVal = c(0,0,0)\np = dim(X)[2]\nmlboot = matrix(rep(NA, nboot*p), nboot, p)\nfor (j in 1:nboot){\n  #if (j%%100 == 0){print(paste(\"iteration:\", j))}\n  bootIdx = sample(1:n, replace = TRUE) # sample observation numbers with replacement\n  optres <- optim(initVal, loglik_poisreg, gr=NULL, y[bootIdx], X[bootIdx,], \n                control=list(fnscale=-1), method=c(\"BFGS\"))\n  mlboot[j,] = optres$par\n}\npar(mfrow = c(1,3))\nhist(mlboot[,1], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_0$)'), \n     main = TeX(r'($\\hat{\\beta}_0$)'))\nhist(mlboot[,2], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_1$)'), \n     main = TeX(r'($\\hat{\\beta}_1$)'))\nhist(mlboot[,3], 50, freq = FALSE, c = \"steelblue\", xlab = TeX(r'($\\hat{\\beta}_2$)'), \n     main = TeX(r'($\\hat{\\beta}_2$)'))\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Contents\n\nThis is just a skeleton template for a course that will be given in the future.\n\n\n\nLiterature\n\nAuthors (2021). Book Name\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phone’s calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\n\n\nTeachers\n\n\n\n\nMattias VillaniCourse responsible and lecturerProfessor"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nLecture 1 - Introduction.\nRead: Ch 1 | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 2 - Probability recap. \nLäs: Slides"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Programming for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nExercise 1 - Basic probability.\nProblems: Book 2.1, 2.2, …\nExercise 2 - Whatever.\nProblems: Book 3.1, 3.2, …"
  },
  {
    "objectID": "computerlabs.html",
    "href": "computerlabs.html",
    "title": "Computer labs for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nComputer lab 1 \nUppgifter: html\nComputer lab 2 \nUppgifter: html\nComputer lab 3 \nUppgifter: html\nComputer lab 4 \nUppgifter: html"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nTutorials\n\nMaximum likelihood by numerical optimization html\nBootstrap html"
  }
]