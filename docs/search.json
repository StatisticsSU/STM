[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "Aim\n\nThis is a course on the Masterâ€™s Program in Data Science, Statistics and Decision Analysis at Stockholm University.\nThe course is specifically designed to bridge between the basic course Statistics and Data Analysis for Computer and Systems Sciences, 15 hp and the masterâ€™s level course Bayesian Learning, 7.5 hp. The objective is therefore to provide a focused course in the probability, statistical theory and modeling needed to follow the Bayesian Learning course.\n\n\n\nContents\n\nMathematical methods: derivatives, integrals, optimization, numerical optimization, vectors and matrices.\nProbability theory: discrete and continuous stochastic variables, density and probability functions, distribution functions, multivariate distributions, multivariate normal distribution, marginal distributions, conditional distributions, independence, expected value, variance, and covariance, functions of stochastic variables, sampling distributions, law of large numbers, central limit theorem.\nModelling and prediction: linear and non-linear regression, dummy variables and interactions, model selection, cross-validation, overfitting, regularization, classification, logistic regression, multinomial logistic regression, Poisson regression.\nInference: point estimation, bias-variance trade-off, maximum likelihood (ML), likelihood theory, numerical optimization for ML estimation, bootstrap.\nTime series: trend and seasonality, autocorrelation, autoregressive models.\n\n\n\nLiterature\n\nWackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage.\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nExamination\nThe course is examined by a\n\nwritten exam (grades A-F)\nhome assignment (grade pass/fail).\n\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phoneâ€™s calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\nThe course makes heavy use of interactive Observable notebooks in javascript that runs in your browser. The widgets will be linked below each relevant lecture. All widgets used in the course are available here.\n\n\nTeachers\n\n\n\n\n\n\n\nMattias VillaniCourse responsible  lecturerProfessor\n\n\n\nFasna KottakkunnanExercisesComputer labs\n\n\n\n\n\nRalf XhaferiExercisesComputer labs"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "The course builds on mathematical and statistical concepts that you need to train on by solving many exercises.\n\n\nSchedule\nThe course schedule can be found on TimeEdit.\n\n\nLiterature - problems\nThe exercise numbers below uses the numbering in the course book (denoted by MSA below):  Wackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage. Note: Problems in the book marked with Applet exercises should be solved with the Observable widgets for the course.\n\n\nExercise problems\nExercise 1 - Differentiation, optimization and integration.\nProblems: MSA X.X, Y.Y, â€¦\nExercise 2 - Discrete random variables.\nProblems: MSA 3.1, 3.9, 3.12, 3.33, 3.39, 3.40, 3.92, 3.93, 3.94, 3.122, 3.124, 3.125, 3.167\nExercise 3 - Continuous random variables.\nProblems: MSA 4.9, 4.11, 4.12, 4.14, 4.20, 4.30, 4.71, 4.84, 4.85, 4.88, 4.105a. 4.109, 4.110, 4.114a-d, 4.115, 4.121, 4.128\nExercise 4 - Joint and conditional distributions.\nProblems: MSA 5.4, 5.16, 5.5, 5.7, 5.25, 5.36, 5.48, 5.60, 5.61, 5.76, 5.89, 5.91, 5.103, 5.114 (hint: these are well-known distributions), 5.136, 5.141.\nExercise 5 - Transformation of variables. Law of large numbers. Central limit theorem.\nProblems: MSA 6.4a, 6.12, 6.24, 6.26.\nInteractive problems: W5.1\nExercise 6 - Maximum likelihood estimation.\nProblems: MSA 9.80, 9.81, 9.83,\nExercise 7 - Linear algebra. Linear regression in matrix form.\nProblems: MSA X.X, Y.Y, â€¦\nExercise 8 - Time series\nProblems: MSA X.X, Y.Y, â€¦"
  },
  {
    "objectID": "interactive_exercises.html",
    "href": "interactive_exercises.html",
    "title": "Interactive exercises for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "A good way to learn Statistics is to experiment with different distributions etc by changing parameters interactively. These interactive exercises makes use of the Observable widgets for the course.\n\n\nInteractive exercises\n\nWx.y is the interactive widget exercise for Chapter x Problem y.\n\nExercise 1 - Differentiation, optimization and integration.\nProblems:\nExercise 2 - Discrete random variables.\nProblems:\nExercise 3 - Continuous random variables.\nProblems:\nExercise 4 - Joint and conditional distributions.\nProblems:\nExercise 5 - Transformation of variables\n\nExercise 6 - Maximum likelihood estimation.\nProblems:\nExercise 7 - Linear algebra. Linear regression in matrix form.\nProblems:\nExercise 8 - Time series\nProblems:"
  },
  {
    "objectID": "interactive_exercises/Chapter5.html",
    "href": "interactive_exercises/Chapter5.html",
    "title": "Interactive Exercises - Chapter 5",
    "section": "",
    "text": "Problem W5.1\nUse the widget for the law of large numbers for this exercise with the population parameters \\(\\mu=3\\) and \\(\\sigma=0.2\\).\n\nWhat is the smallest sample size \\(n\\) that gives a probability of at most \\(0.01\\) for the event that the sample mean deviates from its mean \\(\\mu = 3\\) by at least \\(\\epsilon = 0.1\\) units? That is, use the widget to determine the smallest \\(n\\) for which \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\leq 0.01.\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=26\\) gives \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.01079\\] so this sample size is not large enough. However, for \\(n=27\\) we get \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.009375,\\] which is smaller than the required probability of \\(0.01\\). So \\(n=27\\) is the smallest possible sample size. Check for yourself: \n\n\n\n\nLetâ€™s be even more demanding now and require that the sample mean can deviate by at most \\(\\epsilon = 0.01\\) units from the mean \\(\\mu\\). What is now the smallest sample size \\(n\\) that achieves this?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=2654\\) is the smallest \\(n\\) and \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.01) \\approx 0.009999\\]\n\n\n\n\n\n\nProblem W5.2\nUse the widget for the central limit theorem for this exercise.\n\nChoose the Beta distribution with parameters \\(\\alpha=0.5\\) and \\(\\beta=0.5\\) as the data distribution. Set sample size \\(n=2\\) and look at the orange histogram that shows the sampling distribution of the sample mean for a sample of size \\(n=2\\). Does it look normally distributed? Continue to increase sample size \\(n\\) to 3, 4, 5 and so on. How large \\(n\\) do you need for the sampling distribution to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(n=2\\) the distribution is no longer bathtub shaped, but it is clearly not normal (yet). It is hard to say exactly of course, but already for \\(n=10\\) is the sampling distribution roughly bell shaped like the normal distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the chi-squared distribution with \\(\\nu=3\\) degrees of freedom.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt takes at least until \\(n=20\\) for the sampling distribution to no longer have the long right hand tail of the chi-squared distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the Cauchy distribution with location \\(m=0\\) and scale \\(\\gamma=1\\). How large must \\(n\\) be before the sampling distribution of \\(\\bar{X}\\) seems to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Cauchy distribution is one of the cases where the central limit theorem does not hold. No matter how large you make \\(n\\), the distribution of \\(\\bar{X}\\) will never be normal. The mean and variance of the Cauchy do not exist, which violates the assumptions of the theorem; it has so extremely heavy tails that the mean does not exist, even though the Cauchy distribution is symmetric around the location \\(m\\). ðŸ¤¯"
  }
]