[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "Aim\n\nThis is a course on the Master‚Äôs Program in Data Science, Statistics and Decision Analysis at Stockholm University.\nThe course is specifically designed to bridge between the basic course Statistics and Data Analysis for Computer and Systems Sciences, 15 hp and the master‚Äôs level course Bayesian Learning, 7.5 hp. The objective is therefore to provide a focused course in the probability, statistical theory and modeling needed to follow the Bayesian Learning course.\n\n\n\nContents\n\nMathematical methods: derivatives, integrals, optimization, numerical optimization, vectors and matrices.\nProbability theory: discrete and continuous stochastic variables, density and probability functions, distribution functions, multivariate distributions, multivariate normal distribution, marginal distributions, conditional distributions, independence, expected value, variance, and covariance, functions of stochastic variables, sampling distributions, law of large numbers, central limit theorem.\nModelling and prediction: linear and non-linear regression, dummy variables and interactions, model selection, cross-validation, overfitting, regularization, classification, logistic regression, multinomial logistic regression, Poisson regression.\nInference: point estimation, bias-variance trade-off, maximum likelihood (ML), likelihood theory, numerical optimization for ML estimation, bootstrap.\nTime series: trend and seasonality, autocorrelation, autoregressive models.\n\n\n\nLiterature\n\nWackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage.\nVillani, M. (2025). Bayesian Learning - the prequel. Notes on basic mathematics, probability and statistical inference. Work in progress.\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nExamination\nThe course is examined by a\n\nwritten exam (grades A-F)\nhome assignment (grade pass/fail).\n\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phone‚Äôs calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\nThe course makes heavy use of interactive Observable notebooks in javascript that runs in your browser. The widgets will be linked below each relevant lecture. All widgets used in the course are available here.\n\n\nTeachers\n\n\n\n\n\n\n\nMattias VillaniCourse responsible  lecturerProfessor\n\n\n\nFasna KottakkunnanExercisesComputer labs\n\n\n\n\n\nRalf XhaferiExercisesComputer labs"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "The course builds on mathematical and statistical concepts that you need to train on by solving many exercises.\n\n\nSchedule\nThe course schedule can be found on TimeEdit.\n\n\nLiterature - problems\nThe exercise numbers below uses the numbering in the course book (denoted by MSA below):  Wackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage. Note: Problems in the book marked with Applet exercises should be solved with the Observable widgets for the course.\nThe BLprequel listed below are section numbers from a book Bayesian Learning - the prequel that I have started writing for this course.\n\n\nExercise problems\nExercise 1 - Differentiation, optimization and integration.\nProblems: Exercises from the BLprequel book, Sections 1.14 (Differentiation), 1.15 (Integration) and 1.16 (Function optimization).\nExercise 2 - Discrete random variables.\nProblems: MSA 3.1, 3.9, 3.12, 3.33, 3.39, 3.40, 3.92, 3.93, 3.94, 3.122, 3.124, 3.125, 3.167\nExercise 3 - Continuous random variables.\nProblems: MSA 4.9, 4.11, 4.12, 4.14, 4.20, 4.30, 4.71, 4.88, 4.105a. 4.109, 4.110, 4.114a-d, 4.128\nInteractive problems: W4.1, W4.2\nExercise 4 - Joint and conditional distributions.\nProblems: MSA 5.4, 5.16, 5.5, 5.7, 5.25, 5.36, 5.48, 5.60, 5.61, 5.76, 5.89, 5.91, 5.103, 5.114 (hint: these are well-known distributions), 5.136, 5.141.\n\nExercise 5 - Transformation of variables. Law of large numbers. Central limit theorem.\nProblems: MSA 6.4a, 6.24, 6.23a-b, 6.12 (use the transformation method), 6.28.\nInteractive problems: W5.1, W5.2\nExercise 6 - Maximum likelihood estimation.\nProblems: MSA 9.80, 9.81, 9.85a, 9.97b, 9.98, 9.103.\nExercise 7 - Linear algebra. Linear regression in matrix form.\nProblems: MSA 5.131a, 11.66, 11.68 (use R to compute the matrix inverse)\nExercise 8 - Time series\nProblems: MSA X.X, Y.Y, ‚Ä¶"
  },
  {
    "objectID": "interactive_exercises.html",
    "href": "interactive_exercises.html",
    "title": "Interactive exercises",
    "section": "",
    "text": "A good way to learn Statistics is to experiment with different distributions etc by changing parameters interactively. These interactive exercises makes use of the Observable widgets for the course.\n\n\nInteractive exercises\n\nWx.y is the interactive widget exercise for Chapter x Problem y.\n\nExercise 1 - Differentiation, optimization and integration.\nProblems:\nExercise 2 - Discrete random variables.\nProblems:\nExercise 3 - Continuous random variables\nProblems:\nExercise 4 - Joint and conditional distributions.\nProblems:\nExercise 5 - Transformation of variables\n\nExercise 6 - Maximum likelihood estimation.\nProblems:\nExercise 7 - Linear algebra. Linear regression in matrix form.\nProblems:\nExercise 8 - Time series\nProblems:"
  },
  {
    "objectID": "interactive_exercises/Chapter5.html",
    "href": "interactive_exercises/Chapter5.html",
    "title": "Interactive Exercises - Chapter 5",
    "section": "",
    "text": "Problem W5.1\nUse the widget for the law of large numbers for this exercise with the population parameters \\(\\mu=3\\) and \\(\\sigma=0.2\\).\n\nWhat is the smallest sample size \\(n\\) that gives a probability of at most \\(0.01\\) for the event that the sample mean deviates from its mean \\(\\mu = 3\\) by at least \\(\\epsilon = 0.1\\) units? That is, use the widget to determine the smallest \\(n\\) for which \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\leq 0.01.\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=26\\) gives \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.01079\\] so this sample size is not large enough. However, for \\(n=27\\) we get \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.009375,\\] which is smaller than the required probability of \\(0.01\\). So \\(n=27\\) is the smallest possible sample size. Check for yourself: \n\n\n\n\nLet‚Äôs be even more demanding now and require that the sample mean can deviate by at most \\(\\epsilon = 0.01\\) units from the mean \\(\\mu\\). What is now the smallest sample size \\(n\\) that achieves this?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=2654\\) is the smallest \\(n\\) and \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.01) \\approx 0.009999\\]\n\n\n\n\n\n\nProblem W5.2\nUse the widget for the central limit theorem for this exercise.\n\nChoose the Beta distribution with parameters \\(\\alpha=0.5\\) and \\(\\beta=0.5\\) as the data distribution. Set sample size \\(n=2\\) and look at the orange histogram that shows the sampling distribution of the sample mean for a sample of size \\(n=2\\). Does it look normally distributed? Continue to increase sample size \\(n\\) to 3, 4, 5 and so on. How large \\(n\\) do you need for the sampling distribution to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(n=2\\) the distribution is no longer bathtub shaped, but it is clearly not normal (yet). It is hard to say exactly of course, but already for \\(n=10\\) is the sampling distribution roughly bell shaped like the normal distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the chi-squared distribution with \\(\\nu=3\\) degrees of freedom.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt takes at least until \\(n=20\\) for the sampling distribution to no longer have the long right hand tail of the chi-squared distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the Cauchy distribution with location \\(m=0\\) and scale \\(\\gamma=1\\). How large must \\(n\\) be before the sampling distribution of \\(\\bar{X}\\) seems to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Cauchy distribution is one of the cases where the central limit theorem does not hold. No matter how large you make \\(n\\), the distribution of \\(\\bar{X}\\) will never be normal. The mean and variance of the Cauchy do not exist, which violates the assumptions of the theorem; it has so extremely heavy tails that the mean does not exist, even though the Cauchy distribution is symmetric around the location \\(m\\). ü§Ø"
  },
  {
    "objectID": "interactive_exercises/Chapter4.html",
    "href": "interactive_exercises/Chapter4.html",
    "title": "Interactive Exercises - Chapter 4",
    "section": "",
    "text": "Problem W4.1\nUse the widget for the gamma distribution in the scale parameterization (the one used in the course book) for this exercise. Note that the names for the two parameters in the Gamma distribution is not \\(\\alpha\\) and \\(\\beta\\) as in the book, but instead \\(\\alpha\\) and \\(\\theta\\). C‚Äôest la vie. ü§∑‚Äç‚ôÇÔ∏è\n\nStart with the \\(\\mathrm{Gamma}(3,1)\\) distribution and gradually move the first parameter \\(\\alpha\\) toward 1. What happens with the shape of the distribution at \\(\\alpha = 1\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe distribution for \\(\\alpha = 1\\) becomes highest in the point \\(x=0\\) with a monotonically decreasing density in \\(x\\). The Gamma distribution with \\(\\alpha = 1\\) is actually the exponential distribution.\n\n\n\n\nLet us explore the effect of the second parameter, the scale parameter \\(\\theta\\).\n\n\nSet \\(\\alpha=2\\) and \\(\\theta=2\\). What is the mean and variance? What is \\(\\mathrm{Pr}(X\\leq 3)\\)?\nSet \\(\\alpha=4\\) and \\(\\theta=1\\), what is the mean and variance and \\(\\mathrm{Pr}(X\\leq 3)\\) now?\nWhat if \\(\\alpha=16\\) and \\(\\theta=0.25\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe mean \\(E(X)= \\alpha \\theta\\) is the same in all the three settings and the variance \\(V(X)=\\alpha \\theta^2\\) decreases as \\(\\theta\\) becomes smaller, and so does \\(\\mathrm{Pr}(X\\leq 3)\\)."
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html",
    "href": "tutorial/numericalML/numericalML.html",
    "title": "Maximum likelihood by numerical optimization",
    "section": "",
    "text": "In this tutorial you will learn about the maximum likelihood method for estimating parameters in statistical models. You will also learn how maximum likelihood estimates and standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is typically surprisingly accurate also for smaller sample sizes.\n\nIt will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner giving you the super-power üí™ to use the computer to estimate the parameters and their uncertainty in quite complex models.\nWe will start with simple models with a single parameter to cover all the concepts, and then move on to the practically more important multi-parameter case.\nLet‚Äôs first load some useful libraries (install them using install.packages() if you haven‚Äôt already).\n\nlibrary(latex2exp) # for plotting mathematical symbols (LaTeX)\nlibrary(remotes)   # for loading packages from GitHub\nlibrary(ggplot2)   # for fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html#footnotes",
    "href": "tutorial/numericalML/numericalML.html#footnotes",
    "title": "Maximum likelihood by numerical optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf‚Ü©Ô∏é\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.‚Ü©Ô∏é"
  },
  {
    "objectID": "tutorial/statespace/statespace.html",
    "href": "tutorial/statespace/statespace.html",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "",
    "text": "This tutorial gives a very brief introduction to state-space models, along with inference methods like Kalman filtering, smoothing and forecasting. The methods are illustrated using the R package dlm , exemplified with the local level model fitted to the well-known Nile river data. The tutorial is also sprinkled with some cool interactivity in Javascript."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#piecewise-constant-model",
    "href": "tutorial/statespace/statespace.html#piecewise-constant-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Piecewise constant model",
    "text": "Piecewise constant model\nAn extremely simple model for a time series is to treat the observations as independent normally distributed with the same mean \\(\\mu\\) and variance \\(\\sigma_\\varepsilon\\)\n\\[\ny_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\n\nShow the code\n#install.packages(\"latex2exp\")\nlibrary(latex2exp)\nn = 200\nmu = 2\nsigma_eps = 1\ny = rnorm(n, mean = mu, sd = sigma_eps)\nplot(seq(1,n), y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y_t\", lwd = 1.5,\n    main = \"Simulated data from the naive iid model\")\nlines(seq(1,n), rep(mu,n), type = \"l\", col = \"orange\")\nlegend(\"topright\", legend = c(TeX(\"$y_t$\"), TeX(\"$\\\\mu$\")), lty = 1, lwd = 1.5, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nThis model is of course not something to write home about, it basically ignores the time series nature of the data. Let us start to make it a little more interesting by allowing the mean to vary of time. This means that we will have a time-varying parameter model where the mean \\(\\mu_t\\) changes (abruptly) at certain time points \\(t_1, t_2, \\dots, t_K\\):\n\\[\ny_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\\[\n\\begin{align}   \n\\mu_t &=\n\\begin{cases}            \n  \\mu_1 & \\text{if $1 \\leq t \\leq t_1$} \\\\\n  \\mu_2 & \\text{if $t_1 &lt; t \\leq t_2$} \\\\            \n  \\vdots & \\vdots \\\\\n  \\mu_K & \\text{if $t_{K-1} &lt; t \\leq T$}. \\\\          \n\\end{cases}\n\\end{align}\n\\]\nHere is a widget that lets you simulate data from the piecewise constant model1."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#local-level-model",
    "href": "tutorial/statespace/statespace.html#local-level-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Local level model",
    "text": "Local level model\nThe piecewise constant model has a few abrupt changes in the mean, but what if the mean changes more gradually? The local level model has a constantly changing mean following a random walk model:\n\\[y_t = \\mu_t + \\varepsilon_t,\\qquad \\varepsilon_t \\sim N(0,\\sigma_\\varepsilon^2)\\]\n\\[\\mu_t = \\mu_{t-1} + \\eta_t,\\qquad \\eta_t \\sim N(0,\\sigma_\\eta^2)\\]\nwhich models the observed time series \\(y_t\\) as a mean \\(\\mu_t\\) plus a random measurement error or disturbance \\(\\varepsilon_t\\). The mean \\(\\mu_t\\) evolves over time as a random walk driven by innovations \\(\\eta_t\\).\nHere is a widget that simulates data from the model. Go ahead, experiment with the measurement/noise \\(\\sigma_\\varepsilon\\) and the standard deviation of the innovations to the mean process, \\(\\sigma_\\eta\\). For example, drive \\(\\sigma_\\eta\\) toward zero and note how the mean becomes close to constant over time."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#regression-with-time-varying-parameters",
    "href": "tutorial/statespace/statespace.html#regression-with-time-varying-parameters",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Regression with time-varying parameters",
    "text": "Regression with time-varying parameters\nThe usual simple linear time series regression model is\n\\[\ny_t = \\alpha + \\beta x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\qquad t=1,\\ldots,T\n\\]\nwhere \\(y_t\\) is a time series response variable (for example electricity price) that is being explained by the explanatory variable \\(x_t\\) (for example temperature). This model assumes that the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma_\\varepsilon\\) are constant in time, that the relationship between electricity price and temperature has remained the same throughout the whole observed time period.\nIt sometimes makes sense to let the parameters vary with time. Here is one such model, the time-varying regression model:\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nwhere the intercept \\(\\alpha\\) now has a time \\(t\\) subscript and evolves in time following a random walk process\n\\[\\alpha_{t} = \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)\\]\nso that in every time period, the intercept changes by adding on an innovation \\(\\eta_t\\) drawn from a normal distribution with standard deviation \\(\\sigma_\\alpha\\). This standard deviation therefore controls how much the intercept changes over time. The slope \\(\\beta\\) changes over time in a similar fashion, with the speed of change determined by \\(\\sigma_\\beta\\).\nHere is a widget that simulates data from the time-varying regression above. By moving the slider (show regline at time) you can plot the regression line \\(\\alpha_t + \\beta_t x_t\\) at any time period \\(t\\). The single data point at that time period is the larger dark red point. The plot also highlights (darker blue) data points that are closer in time to the time chosen by the slider. To the left you can see the whole time path of the simulated \\(\\alpha\\) and \\(\\beta\\) with the current parameters highlighted by dots."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#state-space-model---filtering-smoothing-and-forecasting",
    "href": "tutorial/statespace/statespace.html#state-space-model---filtering-smoothing-and-forecasting",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "State-space model - filtering, smoothing and forecasting",
    "text": "State-space model - filtering, smoothing and forecasting\n\nThe state space model\nAll of the models above, and many, many, many more can be written as a so called state-space model. A state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) can be written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{v})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is a state-space model with a single scalar state variable \\(\\boldsymbol{\\theta}_t = \\mu_t\\) and parameters\n\\[\n\\begin{align}\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nWe learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) . The first equation is often called the observation or measurement model since it gives the connection between the unobserved state and the observed measurements. The measurements can also be a vector, but we will use a single measurement in this tutorial. The second equation is called the state transition model since it determines how the state evolves over time.\nWe can even let the state-space parameters \\(\\boldsymbol{F}, \\boldsymbol{G}, \\boldsymbol{V}, \\boldsymbol{W}\\) be different i every time period. This is in fact needed if we want to write the time-varying regression model in state-space form. Recall the time varying regression model\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nWe can tuck the two time-varying parameters in a vector \\(\\boldsymbol{\\beta}_t=(\\alpha_t,\\beta_t)^\\top\\) and also write the models as\n\\[\n\\begin{align}  \ny_t &= \\boldsymbol{x}_t^\\top\\boldsymbol{\\beta}_{t}   + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\    \n\\boldsymbol{\\beta}_{t} &= \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t, \\quad \\quad \\nu_t \\sim N(0, \\boldsymbol{W})\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}  \n\\boldsymbol{x}_t &= (1,x_t)^\\top  \\\\    \n\\boldsymbol{w}_t &= (\\eta_t,\\nu_t)^\\top  \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nNote this is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{F}_t &= \\boldsymbol{x}_t\\\\\n\\boldsymbol{G} &=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nand note now that \\(\\boldsymbol{F}\\) changes in every time period, hence the subscript \\(t\\).\n\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models, filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data.\nFiltering means to compute the sequence of instantaneous estimates of the unobserved state at every time point \\(t=1,2,\\ldots,T\\)\n\\[\n\\hat{\\boldsymbol{\\theta}}_{1|1},\\hat{\\boldsymbol{\\theta}}_{2|2},\\ldots,\\hat{\\boldsymbol{\\theta}}_{T|T}\n\\]\nWe will take a time series and compute the filtered estimates for the whole time series, but it is important to understand that filtering is often done in real-time, which means it is a continously ongoing process that returns filtered estimates of the state \\(\\boldsymbol{\\theta}_t\\) as time progresses and new measurements \\(y_t\\) come in. Think about a self-driving car that is continously trying to understand the environment (people, other cars, the road conditions etc). The environment is the state and the car uses its sensors to collect measurements. The filtering estimates tells the car about the best guess for the environment at every point in time.\nFor state-space models of the type discussed here (linear measurement equation and linear evolution of the state, with independent Normal measurement errors and state innovations), the filtered estimates are computed with one of the most famous algorithms in statistics: the Kalman filter.\nThe Kalman filter is a little messy to write up, we will do it for completeness, but we will use a package for it so don‚Äôt worry if the linear algebra is intidimating. We will use the notation \\(\\boldsymbol{\\mu}_{t|t}\\) instead of \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\), but they really mean the same.\n\ntime \\(t = 0\\). The Kalman filter starts with mean \\(\\boldsymbol{\\mu}_{0|0}\\) and covariance matrix \\(\\boldsymbol{\\Omega}_{0|0}\\) for the state at time \\(t=0\\). Think about \\(\\boldsymbol{\\mu}_{0|0}\\) as the best guess \\(\\boldsymbol{\\theta}_0\\) of the state vector at time \\(t=0\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\) representing how sure we can be about this guess2.\ntime \\(t = 1\\). The Kalman filter then uses the first measurement \\(y_1\\) to update \\(\\boldsymbol{\\mu}_{0|0} \\rightarrow \\boldsymbol{\\mu}_{1|1}\\) and \\(\\boldsymbol{\\Omega}_{0|0} \\rightarrow \\boldsymbol{\\Omega}_{1|1}\\) to represent the estimate and the uncertainty for \\(\\boldsymbol{\\theta}_1\\), the state at time \\(t=1\\).\ntime \\(t = 2,...,T\\). It then continues in this fashion using the next measurement \\(y_2\\) to compute \\(\\boldsymbol{\\mu}_{2|2}\\) and \\(\\boldsymbol{\\Omega}_{2|2}\\) and so on all the way to the end of the time series to finally get \\(\\boldsymbol{\\mu}_{T|T}\\) and \\(\\boldsymbol{\\Omega}_{T|T}\\).\n\nHere is the Kalman filter algorithm:\n\n\nInitialization: set \\(\\boldsymbol{\\mu}_{0|0}\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\)\nfor \\(t=1,\\ldots,T\\) do\n\nPrediction update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t-1} &= \\boldsymbol{G} \\boldsymbol{\\mu}_{t-1|t-1} \\\\  \n\\boldsymbol{\\Omega}_{t|t-1} &= \\boldsymbol{G}\\boldsymbol{\\Omega}_{t-1|t-1}  \\boldsymbol{G}^\\top + \\boldsymbol{W}\n\\end{align}\n\\]\nMeasurement update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t} &= \\boldsymbol{\\mu}_{t|t-1} + \\boldsymbol{K}_t ( y_t - \\boldsymbol{F} \\boldsymbol{\\mu}_{t|t-1}  )  \\\\  \n\\boldsymbol{\\Omega}_{t|t} &= (\\boldsymbol{I} - \\boldsymbol{K}_t \\boldsymbol{F} )\\boldsymbol{\\Omega}_{t|t-1}\n\\end{align}\n\\]\n\n\nwhere \\[\\boldsymbol{K}_t = \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top ( \\boldsymbol{F} \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top + \\boldsymbol{V})^{-1}\\] is the Kalman Gain.\n\nThe widget below lets you experiment with the Kalman filter for the local level model fitted to the Nile river data. In the widget we infer (filter) the local levels \\(\\mu_1,\\mu_2,\\ldots,\\mu_T\\) and can experiment with the measurement standard deviation \\(\\sigma_\\varepsilon\\), the standard deviation of the innovations to the local mean \\(\\sigma_\\eta\\), and also the initial guess for \\(\\mu_0\\) and the standard deviation \\(\\sigma_0\\) of that guess.\nHere are few things to try out in the widget below:\n\nIncrease the measurement standard deviation \\(\\sigma_\\varepsilon\\) and note how the filtered mean pays less and less attention to changes in the data (because the model believes that the data is very poor quality (noisy) and tells us basically nothing about the level). Then move \\(\\sigma_\\varepsilon\\) to smaller values and note how the filtered mean starts chasing the data (because the model believes that the data are super informative about the level).\nMake the standard deviation for the initial level \\(\\sigma_0\\) very small and then change the initial mean \\(\\mu_0\\) to see how this affects the filtered mean at the first part of the time series.\nMove the standard deviation of the innovations to the level \\(\\sigma_\\eta\\) small and note how the filtered mean becomes smoother and smoother over time."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#the-dlm-package-in-r",
    "href": "tutorial/statespace/statespace.html#the-dlm-package-in-r",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "The dlm package in R",
    "text": "The dlm package in R\nThe dlm package is a user-friendly R package for analyzing some state-space models. The package has a nice vignette that is worth reading if you plan to use the package more seriously.\n\nFiltering\nLet‚Äôs first do some filtering in the dlm package. Start by loading the dlm package:\n\n#install.packages(\"dlm\") # uncomment the first time to install.\nlibrary(dlm)\n\nWe now need to tell the dlm package what kind of state-space model we want to estimate. The means setting up the matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\). We will keep it simple and use the local level model as example, where all parameter matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\) are scalars (single numbers). As we have seen above, the local level model corresponds to a state-space model with parameters\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\eta^2\n\\end{align}\n\\]\nSo we only need to set \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) to start the fun. We will for now set \\(\\sigma_\\varepsilon^2 = 100^2\\) and \\(\\sigma_\\eta^2 = 100^2\\), and return to this when we learn how the dlm package can find maximum likelihood estimates for these parameters. Here is how you setup the local level model in the dlm package:\n\nmodel = dlm(FF = 1, V = 100^2, GG = 1, W = 100^2, m0 = 1000, C0 = 1000^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) were just set to some values above. Let‚Äôs instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 1000, C0 = 1000^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(0,0), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1] 15101.339  1467.049\n\n\nor the square roots, to get the maximum likelihood estimates of the standard deviations\n\nsqrt(exp(fit$par))\n\n[1] 122.88750  38.30208\n\n\nWe can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 1000, C0 = 1000^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lwd = 1.5, lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\nSmoothing\nWe can also use the dlm package to compute the smoothed retrospective estimates of the local level \\(\\mu_t\\) at time \\(t\\) using all the data from \\(t=1\\) until the end of the time series \\(T\\). We haven‚Äôt showed the mathematical algorithm for smoothing, but you can look it up in many books. Anyway, here is the smoothing results for the Nile data, using the function dlmSmooth from the dlm package. The filtered estimates are also shown.\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\",\"Smoothed\"), lty = 1, lwd = 1.5, col = c(\"steelblue\", \"orange\", \"red\"))\n\n\n\n\n\n\n\n\n\n\nForecasting\nWe can also use state-space models for forecasting. Here is how it is done in the dlm package.\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), lwd = 1.5,\n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, lwd = 1.5,\n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#non-gaussian-state-space-models",
    "href": "tutorial/statespace/statespace.html#non-gaussian-state-space-models",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Non-Gaussian state-space models",
    "text": "Non-Gaussian state-space models\n\nPoisson time series model\nA useful model for time series of counts \\(Y \\in \\{0,1,2,\\ldots \\}\\) is a Poisson distribution with time-varying intensity \\(\\lambda_t = \\exp(z_t)\\), where \\(z_t\\) is some continuous stochastic process with autocorrelation, most commonly a random walk:\n\\[\n\\begin{align} y_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= z_{t-1} + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nNote that because of the exponential function \\(\\lambda_t = \\exp(z_t)\\) is guaranteed to be positive for all \\(t\\), as required for the Poisson distribution. It is easily to simulate data from the Poisson time series model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimPoisTimeSeries &lt;- function(T, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the Poisson variables with different intensities, lambda_t = exp(z_t) for each time\n  lambda = exp(z)\n  return (rpois(T, lambda = lambda[2:(T+1)]))\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1) \nT = 100\nsigma_eta = 0.1\ny = simPoisTimeSeries(T, sigma_eta)\nplot(y, type = \"o\", pch = 19, col = \"steelblue\", yaxt = \"n\", xlab = \"time, t\", ylab = \"counts, y\", \n      main = paste(\"A simulated Poisson time series with sigma_eta =\", sigma_eta))\naxis(side = 2, at = seq(0,max(y)))\n\n\n\n\n\n\n\n\n\nWhat‚Äôs life without widgets? Here is one for a slightly more general Poisson time series model where the random walk is replaced by an autoregressive process of order 1:\n\\[\n\\begin{align}\ny_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= \\mu + \\phi(z_{t-1} -\\mu) + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\n\n\n\nStochastic volatility models\nMany time series, particularly in the finance, has a variance that is changing over time. Furthermore, it is common to find volatility clustering in the data, meaning that once the the variance is high (turbulent stock market) it tends to remain high for a while and vice versa. The basic stochastic volatility (SV) model tries to capture this:\n\\[\n\\begin{align}\ny_t &= \\mu + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= z_{t-1} + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere we have for simplicity assumed just a constant mean \\(\\mu\\), but we can extend this with and autoregressive process, or basically any model of your preference. The thing that set the SV model apart from the other model presented so far is that the variance of the measurement errors \\(Var(y_t)=Var(\\varepsilon_t) = \\exp(z_t)\\) is heteroscedastic, that is, it varies over time. The variance is driven by the \\(z_t\\) process, which here is modeled as a random walk, which will induce volatility clustering. Note again that we use the exponential function to ensure that the variance is positive for all \\(t\\). Here is code to simulate from this basic stochastic volatility model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVol &lt;- function(T, mu, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma¬≤_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\nLet‚Äôs use that function to simulate a time series and plot it:\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(2) \nT = 100\nmu = 3\nsigma_eta = 1\nsimuldata = simStochVol(T, mu, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nWe can replace the random walk for the \\(z_t\\) with a more well-behaved AR(1) process:\n\\[\n\\begin{align}\ny_t &= \\mu_y + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= \\mu_z + \\phi(z_{t-1} - \\mu_z) + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere \\(\\mu_y\\) is the mean of the time series \\(y\\) and \\(\\mu_z\\) is the mean of the (log) variance process \\(z_t\\). The parameter \\(\\mu_z\\) therefore determines how much variance \\(y_t\\) has on average and \\(\\phi\\) determines how much volatility clustering there is. A \\(\\phi\\) close to 1 gives long periods of persistently large or small variance. Here is the code:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVolAR &lt;- function(T, mu_y, mu_z, phi, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = mu_z + phi*(z[t-1] - mu_z) + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma¬≤_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu_y, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1)\nT = 1000\nmu_y = 3\nmu_z = -1\nphi = 0.95\nsigma_eta = 1\nsimuldata = simStochVolAR(T, mu_y, mu_z, phi, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nWidget time!"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#bonus-implementing-the-kalman-filter-from-scratch",
    "href": "tutorial/statespace/statespace.html#bonus-implementing-the-kalman-filter-from-scratch",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Bonus: Implementing the Kalman filter from scratch",
    "text": "Bonus: Implementing the Kalman filter from scratch\nFor the curious, the code below implements the Kalman filter from scratch in R. Let us first implement a function kalmanfilter_update that does the update for a single time step:\n\nkalmanfilter_update &lt;- function(mu, Omega, y, G, C, V, W) {\n  \n  # Prediction step - moving state forward without new measurement\n  muPred &lt;- G %*% mu\n  omegaPred &lt;- G %*% Omega %*% t(G) + W\n  \n  # Measurement update - updating the N(muPred, omegaPred) prior with the new data point\n  K &lt;- omegaPred %*% t(F) / (F %*% omegaPred %*% t(F) + V) # Kalman Gain\n  mu &lt;- muPred + K %*% (y - F %*% muPred)\n  Omega &lt;- (diag(length(mu)) - K %*% F) %*% omegaPred\n  \n  return(list(mu, Omega))\n}\n\nThen we implement a function that does all the Kalman iterations, using the kalmanfilter_update function above:\n\nkalmanfilter &lt;- function(Y, G, F, V, W, mu0, Sigma0) {\n  T &lt;- dim(Y)[1]  # Number of time steps\n  n &lt;- length(mu0)  # Dimension of the state vector\n  \n  # Storage for the mean and covariance state vector trajectory over time\n  mu_filter &lt;- matrix(0, nrow = T, ncol = n)\n  Sigma_filter &lt;- array(0, dim = c(n, n, T))\n  \n  # The Kalman iterations\n  mu &lt;- mu0\n  Sigma &lt;- Sigma0\n  for (t in 1:T) {\n    result &lt;- kalmanfilter_update(mu, Sigma, t(Y[t, ]), G, F, V, W)\n    mu &lt;- result[[1]]\n    Sigma &lt;- result[[2]]\n    mu_filter[t, ] &lt;- mu\n    Sigma_filter[,,t] &lt;- Sigma\n  }\n  \n  return(list(mu_filter, Sigma_filter))\n}\n\nLet‚Äôs try it out on the Nile river data:\n\n# Analyzing the Nile river data\nprettycolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\")\ny = as.vector(Nile)\nV = 100^2\nW = 100^2\nmu0 = 1000\nSigma0 = 1000^2\n\n# Set up state-space model for local level model\nT = length(y)\nG = 1\nF = 1\nY = matrix(0,T,1)\nY[,1] = y\nfilterRes = kalmanfilter(Y, G, F, V, W, mu0, Sigma0)\nmeanFilter = filterRes[[1]]\nstd_filter = sqrt(filterRes[[2]][,,, drop =TRUE])\n\nplot(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\npolygon(c(seq(1:T), rev(seq(1:T))), \n        c(meanFilter - 1.96*std_filter, rev(meanFilter + 1.96*std_filter)), \n        col = \"#F0F0F0\", border = NA)\nlines(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\nlines(seq(1:T), meanFilter, type = \"l\", col = prettycolors[3], lwd = 1.5)\nlegend(\"topright\", legend = c(\"time series\", \"filter mean\", \"95% intervals\"), lty = 1, lwd = 1.5,\n    col = c(prettycolors[1], prettycolors[3], \"#F0F0F0\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#footnotes",
    "href": "tutorial/statespace/statespace.html#footnotes",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClicking on the  below the widget will take you to the Observable notebook of the widget where you can also change the locations of the thresholds, \\(t_1,t_2,\\ldots,t_{K-1}\\), if you are really into that sort of thing.‚Ü©Ô∏é\nIts all about that Bayes\nThe Kalman filter is often presented from a frequentist point of view in statistics, where the Kalman filtered estimates are the optimal estimates in the mean square error sense.\n\nThe Kalman filter can also be derived as simple Bayesian updating, using Bayes‚Äô theorem to update the information about the state as a new measurement comes in. The \\(\\boldsymbol{\\mu_{0|0}}\\) and \\(\\boldsymbol{\\Omega_{0|0}}\\) can be seen as the prior mean and prior covariance matrix summarizing your prior information about the state before collecting any measurements.\n\nThe Kalman filter is great. When something is great, Bayes usually lurks in the background! üòú‚Ü©Ô∏é"
  },
  {
    "objectID": "homeassignment.html",
    "href": "homeassignment.html",
    "title": "Home assignment for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "The course has a mandatory home assignment divided into two parts. You can view the problems as html, pdf or download the quarto file for the exercise.\n\nHome assignment - part 1\nhtml | pdf | quarto\nDeadline (Spring 2025 version): April 27 at 6 pm\nHome assignment - part 2\nhtml | pdf | quarto\nDeadline (Spring 2025 version): May 23 at 6 pm"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "Tutorials\n\nMaximum likelihood by numerical optimization html | quarto\nBootstrap html | quarto\nState-space models html | quarto (Not used in the course, but pretty cool)"
  },
  {
    "objectID": "computerlabs.html",
    "href": "computerlabs.html",
    "title": "Computer labs for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "Computer lab 1 - Probability models. Monte Carlo simulation\nMaterial: html\nComputer lab 2 - Numerical maximum likelihood.\nMaterial: html"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "This page contains a short description of the contents, reading instructions and additional material for each lecture.\n\n\nSchedule\nThe course schedule can be found on TimeEdit.\n\n\nLiterature\nThe MSA listed below are section numbers from the course book Wackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage.\nThe BLprequel listed below are section numbers from a book Bayesian Learning - the prequel that I have started writing for this course.\n\n\nLecture contents\nPreparatory - Basic maths (math check/self-study)\nThis is not a lecture, but check that you remember this prerequisite high school maths, or otherwise freshen it up at the start of the course.\nRead: BLprequel 1.1-1.7.\nLecture 1 - Differentiation\nRead: BLprequel 1.8-1.14 | Slides\nNotebooks and widgets: Exponential function | Logarithms | Derivatives\n\nLecture 2 - Optimization. Integration.\nRead: BLprequel 1.16 | Notebook on function optimization | Slides\nWidgets: Integrals | Common functions and their derivatives\nLecture 3 - Discrete random variables.\nRead: If needed, refresh basic probability in Ch. 12-13 in the SDA1 course book MSA 3.1-3.6, 3.8, 3.11 | Slides\nWidgets: Bernoulli | Binomial | Geometric | Poisson | Negative binomial | Chebychev‚Äôs inequality\nExtras: List with 50+ statistical distribution widgets\nLecture 4 - Continuous random variables.\nRead: MSA 4.1-4.8, 4.10 | Slides\nWidgets: Normal | Exponential | Beta | Student-t | Gamma\nExtras: List with 50+ statistical distribution widgets\nLecture 5 - Joint and conditional distributions. Covariance and correlation. Bayes theorem.\nRead: MSA 5.1-5.8, 5.11 | BLprequel 1.16 (double integrals) | Slides\n\nLecture 6 - Transformation of random variables. Monte Carlo simulation. Law of large numbers. Central limit theorem.\nRead: MSA 6.1-6.4, 7.3 | Law of large numbers notebook | central limit theorem notebook | Slides\nWidgets: Law of large numbers | central limit theorem\n\nLecture 7 - Point estimation. Maximum likelihood. Sampling distributions.\nRead: MSA 9.1-9.2, 9.3 (pages 448-451), 9.4, 9.7 | Sections 1-4 of tutorial on maximum likelihood | Slides\nWidgets: Sampling distribution and Likelihood | ML - Bernoulli data | ML - Poisson data\n\nLecture 8 - Vectors and matrices. Linear Regression. Multivariate normal distribution.\nRead: MSA A1.1-A1.7, 5.10, 11.10-11.11 | Slides\nWidgets: Bivariate normal distribution\nLecture 9 - Observed and Fisher information. Numerical optimization.\nRead: Sections 5-7 of tutorial on maximum likelihood | Slides |\nWidgets: Second derivative as function curvature | Likelihood and Information\nCode: Optim for Poisson model | Optim for Gamma model\nLecture 10 - Logistic, Poisson regression and beyond.\nRead: X | Slides\n\nLecture 11 - Nonlinear regression. Regularization.\nRead: X | Slides\nCode:\nData:\n\nLecture 12 - Time series. Autocorrelation function. Autoregressive models.\nRead: X | Slides\nCode:\nData:\n\nLecture 13 - Course summary and example exam.\nRead: X | Slides\nCode:\nData:"
  },
  {
    "objectID": "admin/KB.html",
    "href": "admin/KB.html",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "The course is part of the Master‚Äôs program in data science, statistics and decision analysis, 120 credits.\n\n\nThis course is a continuation of the course Statistics and Data Analysis for Computer and Systems Sciences, 15 credits. It provides advanced and expanded knowledge in probability theory, modelling, and statistical inference theory, which forms the foundation for modern machine learning and AI. Theoretical derivations are connected with practical applications in programming languages for data analysis and machine learning. Regression and classification models are a significant part of the course. Prediction-based methods such as cross-validation are used for model selection. The course offers an introduction to the mathematical methods that are essential tools for the course and ends with an introduction to time series analysis.\nThe following topics are covered:\n\nMathematical methods: derivatives, integrals, optimization, numerical optimization, vectors, and matrices.\nProbability theory: discrete and continuous stochastic variables, density and probability functions, distribution functions, multivariate distributions, multivariate normal distribution, marginal distributions, conditional distributions, independence, expected value, variance, and covariance, functions of stochastic variables, sampling distributions, law of large numbers, central limit theorem.\nModelling and prediction: linear and non-linear regression, dummy variables and interactions, model selection, cross-validation, overfitting, regularization, classification, logistic regression, multinomial logistic regression, Poisson regression.\nInference: point estimations, bias-variance trade-off, maximum likelihood (ML), likelihood theory, numerical optimization for ML estimations, bootstrap.\nTime series: trend and seasonality, autocorrelation, autoregressive models.\n\nThe examination consists of\n\nExam 1 (exam code 11ST): Statistical theory and modelling, exam.\nExam 2 (exam code 12SI): Statistical theory and modelling, home assignment.\n\n\n\n\nTo pass the course, students should be able to:\nKnowledge and comprehension:\n\nDefine and describe theoretical concepts in probability.\nExplain the principles behind inference methods and their use in practical applications.\nSelect appropriate models and methods depending on the situation and research question within the course content.\n\nSkills and abilities:\n\nFormulate and solve problems of advanced nature within probability theory.\nFormulate and solve problems of advanced nature related to estimation, inference, and prediction.\nPerform calculations, simulations, and analyses in a programming language related to the course content.\n\nEvaluation ability and approach:\n\nInterpret, evaluate, and critically review results with respect to relevant scientific aspects.\n\n\n\n\n\n\n\nTeachers\nRoll\n\n\n\n\nMattias Villani\nCourse responsible, examiner and lecturer \n\n\nFasna Kottakkunnan\nExercises and Computer labs \n\n\nRalf Xhaferi\nExercises and Computer labs\n\n\n\nAll teachers have office hours by appointment. Contact us via email to schedule a meeting time (either on campus or via Zoom).\nThe Statistical Department is located at the newly built Campus Albano, Albanov√§gen 12, House 4, 6th floor. General information regarding the department can be found on the department website.\nAll course material - including slides, reading instructions and schedule link - are available on the course page.\nStudent-specific information, submission of the home assignments and messages during the course are handled on the learning platform Athena.\n\n\n\nAfter the course is completed, an evaluation of the course is conducted. The course evaluation is used as part of the quality work for the course and as a means of student influence. The evaluation is carried out by sending a survey via email to all registered course participants. The responses from the course participants are compiled and, along with the final report/course evaluation from the course responsible teacher, uploaded to Athena.\n\n\n\nThe teaching consists of 12 lectures (F1-F12), 8 exercises (√ñ1-√ñ8) and 3 computer lab sessions (DL1-DL3) according to the schedule. There are also eight scheduled slots for getting help from one of the teaching assistants (jour). Please see the link to TimeEdit on the course page for the current schedule.\nParticipation in all teaching activities is voluntary, but strongly recommended. Learning statistics takes a lot of practice and it will be hard to pass the course without the necessary effort.\n\n\n\nExam 1 is an individual written exam - using both pen and paper and computer - graded on a seven-point scale related to the course objectives:\n\n\n\n\n\n\nGrade Scale for Exam 1\n\n\n\nA¬† ¬†Excellent\nB ¬†¬†Very good\nC ¬†¬†Good\nD ¬†¬†Satisfactory\nE¬†¬†¬†Sufficient\nFx¬† Insufficient, requires some additional work\nF¬†¬†¬†Insufficient, requires much more work\n\n\nExam 2 is a group assignment, graded on a two-point scale: pass (G) or fail (U).\nTo pass the course, a minimum grade of E on Exam 1 and a pass grade on Exam 2 are required. The final grade for the course is determined by the grade on Exam 1.\n\nStudents who receive at least an E grade on the exam may not retake the exam for a higher grade.\nGrades of Fx and F on the exam are failing grades and require re-examination. Therefore, students who receive an Fx grade cannot retake the exam for a higher grade.\nStudents who receive an Fx or F grade on an exam have the right to undergo at least four additional exams as long as the course is offered to achieve at least an E grade.\nStudents who receive an Fx or F grade on two occasions from an examiner have the right to request that a different examiner be appointed to determine the grade at the next exam session. This request must be made in writing to the head of department.\nTwo examination opportunities are available for each exam during the current semester.\n\n\n\n\nThe exam (code 11ST) is an individual written exam, using both pen and paper and computer.\nThe exam duration is 5 hours.\nInternet access is not permitted during the exam.\nPersonal code from your own solutions to computer labs is allowed as a resource. The personal code file must be uploaded for verification before the exam (as instructed on Athena).\nCollaboration is not allowed during the exam, nor are any aids other than those permitted by the examiner.\nSpecial accommodations may be allowed upon request to the department‚Äôs study and career counselor and with the examiner‚Äôs approval. Contact the study and career counselor well in advance of the exam, preferably no later than three weeks before the exam date.\nRules governing exams at Stockholm University can be found at: Rules for on-site exams\n\n\n\n\n\n\n\nNote!\n\n\n\nRemember, you must register at least 10 days before the exam. If you have registered correctly, you will receive a confirmation email with an anonymous code. This confirmation serves as your receipt of registration. If you need to re-register for an old course code, you can only do so via email to expedition@stat.su.se. Failure to register means you cannot take the exam!\n\n\nSee TimeEdit for scheduled exam sessions.\n\n\n\n\nThe assignments are group work, with three (3) persons per group. Group division is done on Athena under the ‚ÄúCreate Workgroup‚Äù document in the Assignments folder.\nThe assignment consists of 2 subprojects presented as written reports in R. Instructions for the assignments will be available on Athena at the start of the course.\nCollaboration within the group is allowed, but individual assessment and grading within the group may occur. All group members are responsible for and must be able to account for all parts of the work presented in the reports. Collaboration between groups is permitted, but each group must submit their unique reports. All forms of plagiarism are prohibited, and text matching software may be used as needed. Read Rules and Handling Procedures for Disciplinary Matters.\nExam 2 is graded Pass (all subtasks in both projects approved) or Fail (at least one subtask fails). If one or more subtasks fail, there is an opportunity for correction during the current semester.\nNote! It is not possible to correct if the submission is only made at the second opportunity. This means that if you miss a submission and instead submit at a later time and fail, you cannot correct the assignment.\nNote! All subtasks in both projects must be completed and approved during the current semester for the entire assignment to be approved. Results from subtasks are not saved and cannot be transferred to future semesters.\n\n\n\n\nDeadline for submission of home assignment: TBD\nDeadline for possible correction: TBD\n\n\n\n\n\n\n\n\nThe exam (code 11ST) is an individual written exam, using both pen and paper and computer. The written exam covers material according to the course content.\n\n\n\n\n\n\nExam 1 Grading Criteria\n\n\n\nA (Excellent): The student can excellently use concepts within probability and inference theory not necessarily directly covered in the course. They can solve and interpret complex problems concerning random variables, distributions and estimators correctly and well-structured, set up simple statistical models for specific situations, and use R for calculations and data analysis. Requires at least 90% on the written exam.\nB (Very Good): The student can very effectively use concepts within probability and inference theory covered in the course. They can solve and interpret complex problems concerning random variables, distributions and estimators correctly and well-structured, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 80-89% on the written exam.\nC (Good): The student can effectively use concepts within probability and inference theory covered in the course. They can correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 70-79% on the written exam.\nD (Satisfactory): The student can satisfactorily use concepts within probability and inference theory covered in the course. They can correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 60-69% on the written exam.\nE (Sufficient): The student can sufficiently use concepts within probability and inference theory covered in the course. They can mainly correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 50-59% on the written exam.\nFx (Fail, additional work required): The student‚Äôs performance is insufficient according to at least one criterion for E. Equivalent to 40-49% of points on the written exam.\nF (Fail, much more work required): The student‚Äôs performance shows clear deficiencies according to the criteria for E. Equivalent to 0-39% of points on the written exam.\n\n\n\n\n\nGrading for the assignment is Pass (G) or Fail (U). The following grading criteria apply:\n\n\n\n\n\n\nExam 2 (12ST) Grading Criteria\n\n\n\nG (Pass): The student has set up appropriate statistical models for given situations, demonstrated sufficient ability to use statistical terminology, utilized R correctly, and presented the results in well-written reports following the instructions. All subtasks should be correctly solved.\nU (Fail): The student‚Äôs performance is insufficient according to at least one criterion for Pass.\n\n\n\n\n\n\nThe main book for the course is:\n\nWackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage.\n\nOther course materials such as supplementary materials, lecture notes, exercises, and instructions for assignments will be posted on the course web page. Links to selected online sources will also be provided there."
  },
  {
    "objectID": "admin/KB.html#course-contents",
    "href": "admin/KB.html#course-contents",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "This course is a continuation of the course Statistics and Data Analysis for Computer and Systems Sciences, 15 credits. It provides advanced and expanded knowledge in probability theory, modelling, and statistical inference theory, which forms the foundation for modern machine learning and AI. Theoretical derivations are connected with practical applications in programming languages for data analysis and machine learning. Regression and classification models are a significant part of the course. Prediction-based methods such as cross-validation are used for model selection. The course offers an introduction to the mathematical methods that are essential tools for the course and ends with an introduction to time series analysis.\nThe following topics are covered:\n\nMathematical methods: derivatives, integrals, optimization, numerical optimization, vectors, and matrices.\nProbability theory: discrete and continuous stochastic variables, density and probability functions, distribution functions, multivariate distributions, multivariate normal distribution, marginal distributions, conditional distributions, independence, expected value, variance, and covariance, functions of stochastic variables, sampling distributions, law of large numbers, central limit theorem.\nModelling and prediction: linear and non-linear regression, dummy variables and interactions, model selection, cross-validation, overfitting, regularization, classification, logistic regression, multinomial logistic regression, Poisson regression.\nInference: point estimations, bias-variance trade-off, maximum likelihood (ML), likelihood theory, numerical optimization for ML estimations, bootstrap.\nTime series: trend and seasonality, autocorrelation, autoregressive models.\n\nThe examination consists of\n\nExam 1 (exam code 11ST): Statistical theory and modelling, exam.\nExam 2 (exam code 12SI): Statistical theory and modelling, home assignment."
  },
  {
    "objectID": "admin/KB.html#learning-outcomes",
    "href": "admin/KB.html#learning-outcomes",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "To pass the course, students should be able to:\nKnowledge and comprehension:\n\nDefine and describe theoretical concepts in probability.\nExplain the principles behind inference methods and their use in practical applications.\nSelect appropriate models and methods depending on the situation and research question within the course content.\n\nSkills and abilities:\n\nFormulate and solve problems of advanced nature within probability theory.\nFormulate and solve problems of advanced nature related to estimation, inference, and prediction.\nPerform calculations, simulations, and analyses in a programming language related to the course content.\n\nEvaluation ability and approach:\n\nInterpret, evaluate, and critically review results with respect to relevant scientific aspects."
  },
  {
    "objectID": "admin/KB.html#teachers-and-general-information",
    "href": "admin/KB.html#teachers-and-general-information",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "Teachers\nRoll\n\n\n\n\nMattias Villani\nCourse responsible, examiner and lecturer \n\n\nFasna Kottakkunnan\nExercises and Computer labs \n\n\nRalf Xhaferi\nExercises and Computer labs\n\n\n\nAll teachers have office hours by appointment. Contact us via email to schedule a meeting time (either on campus or via Zoom).\nThe Statistical Department is located at the newly built Campus Albano, Albanov√§gen 12, House 4, 6th floor. General information regarding the department can be found on the department website.\nAll course material - including slides, reading instructions and schedule link - are available on the course page.\nStudent-specific information, submission of the home assignments and messages during the course are handled on the learning platform Athena."
  },
  {
    "objectID": "admin/KB.html#course-evaluation",
    "href": "admin/KB.html#course-evaluation",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "After the course is completed, an evaluation of the course is conducted. The course evaluation is used as part of the quality work for the course and as a means of student influence. The evaluation is carried out by sending a survey via email to all registered course participants. The responses from the course participants are compiled and, along with the final report/course evaluation from the course responsible teacher, uploaded to Athena."
  },
  {
    "objectID": "admin/KB.html#teaching-and-mandatory-participation",
    "href": "admin/KB.html#teaching-and-mandatory-participation",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "The teaching consists of 12 lectures (F1-F12), 8 exercises (√ñ1-√ñ8) and 3 computer lab sessions (DL1-DL3) according to the schedule. There are also eight scheduled slots for getting help from one of the teaching assistants (jour). Please see the link to TimeEdit on the course page for the current schedule.\nParticipation in all teaching activities is voluntary, but strongly recommended. Learning statistics takes a lot of practice and it will be hard to pass the course without the necessary effort."
  },
  {
    "objectID": "admin/KB.html#knowledge-check-and-examination",
    "href": "admin/KB.html#knowledge-check-and-examination",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "Exam 1 is an individual written exam - using both pen and paper and computer - graded on a seven-point scale related to the course objectives:\n\n\n\n\n\n\nGrade Scale for Exam 1\n\n\n\nA¬† ¬†Excellent\nB ¬†¬†Very good\nC ¬†¬†Good\nD ¬†¬†Satisfactory\nE¬†¬†¬†Sufficient\nFx¬† Insufficient, requires some additional work\nF¬†¬†¬†Insufficient, requires much more work\n\n\nExam 2 is a group assignment, graded on a two-point scale: pass (G) or fail (U).\nTo pass the course, a minimum grade of E on Exam 1 and a pass grade on Exam 2 are required. The final grade for the course is determined by the grade on Exam 1.\n\nStudents who receive at least an E grade on the exam may not retake the exam for a higher grade.\nGrades of Fx and F on the exam are failing grades and require re-examination. Therefore, students who receive an Fx grade cannot retake the exam for a higher grade.\nStudents who receive an Fx or F grade on an exam have the right to undergo at least four additional exams as long as the course is offered to achieve at least an E grade.\nStudents who receive an Fx or F grade on two occasions from an examiner have the right to request that a different examiner be appointed to determine the grade at the next exam session. This request must be made in writing to the head of department.\nTwo examination opportunities are available for each exam during the current semester.\n\n\n\n\nThe exam (code 11ST) is an individual written exam, using both pen and paper and computer.\nThe exam duration is 5 hours.\nInternet access is not permitted during the exam.\nPersonal code from your own solutions to computer labs is allowed as a resource. The personal code file must be uploaded for verification before the exam (as instructed on Athena).\nCollaboration is not allowed during the exam, nor are any aids other than those permitted by the examiner.\nSpecial accommodations may be allowed upon request to the department‚Äôs study and career counselor and with the examiner‚Äôs approval. Contact the study and career counselor well in advance of the exam, preferably no later than three weeks before the exam date.\nRules governing exams at Stockholm University can be found at: Rules for on-site exams\n\n\n\n\n\n\n\nNote!\n\n\n\nRemember, you must register at least 10 days before the exam. If you have registered correctly, you will receive a confirmation email with an anonymous code. This confirmation serves as your receipt of registration. If you need to re-register for an old course code, you can only do so via email to expedition@stat.su.se. Failure to register means you cannot take the exam!\n\n\nSee TimeEdit for scheduled exam sessions.\n\n\n\n\nThe assignments are group work, with three (3) persons per group. Group division is done on Athena under the ‚ÄúCreate Workgroup‚Äù document in the Assignments folder.\nThe assignment consists of 2 subprojects presented as written reports in R. Instructions for the assignments will be available on Athena at the start of the course.\nCollaboration within the group is allowed, but individual assessment and grading within the group may occur. All group members are responsible for and must be able to account for all parts of the work presented in the reports. Collaboration between groups is permitted, but each group must submit their unique reports. All forms of plagiarism are prohibited, and text matching software may be used as needed. Read Rules and Handling Procedures for Disciplinary Matters.\nExam 2 is graded Pass (all subtasks in both projects approved) or Fail (at least one subtask fails). If one or more subtasks fail, there is an opportunity for correction during the current semester.\nNote! It is not possible to correct if the submission is only made at the second opportunity. This means that if you miss a submission and instead submit at a later time and fail, you cannot correct the assignment.\nNote! All subtasks in both projects must be completed and approved during the current semester for the entire assignment to be approved. Results from subtasks are not saved and cannot be transferred to future semesters.\n\n\n\n\nDeadline for submission of home assignment: TBD\nDeadline for possible correction: TBD"
  },
  {
    "objectID": "admin/KB.html#grading-criteria",
    "href": "admin/KB.html#grading-criteria",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "The exam (code 11ST) is an individual written exam, using both pen and paper and computer. The written exam covers material according to the course content.\n\n\n\n\n\n\nExam 1 Grading Criteria\n\n\n\nA (Excellent): The student can excellently use concepts within probability and inference theory not necessarily directly covered in the course. They can solve and interpret complex problems concerning random variables, distributions and estimators correctly and well-structured, set up simple statistical models for specific situations, and use R for calculations and data analysis. Requires at least 90% on the written exam.\nB (Very Good): The student can very effectively use concepts within probability and inference theory covered in the course. They can solve and interpret complex problems concerning random variables, distributions and estimators correctly and well-structured, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 80-89% on the written exam.\nC (Good): The student can effectively use concepts within probability and inference theory covered in the course. They can correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 70-79% on the written exam.\nD (Satisfactory): The student can satisfactorily use concepts within probability and inference theory covered in the course. They can correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 60-69% on the written exam.\nE (Sufficient): The student can sufficiently use concepts within probability and inference theory covered in the course. They can mainly correctly solve and interpret problems concerning random variables, distributions and estimators, set up simple statistical models for some concrete situations, and use R for calculations and data analysis. Awarded for 50-59% on the written exam.\nFx (Fail, additional work required): The student‚Äôs performance is insufficient according to at least one criterion for E. Equivalent to 40-49% of points on the written exam.\nF (Fail, much more work required): The student‚Äôs performance shows clear deficiencies according to the criteria for E. Equivalent to 0-39% of points on the written exam.\n\n\n\n\n\nGrading for the assignment is Pass (G) or Fail (U). The following grading criteria apply:\n\n\n\n\n\n\nExam 2 (12ST) Grading Criteria\n\n\n\nG (Pass): The student has set up appropriate statistical models for given situations, demonstrated sufficient ability to use statistical terminology, utilized R correctly, and presented the results in well-written reports following the instructions. All subtasks should be correctly solved.\nU (Fail): The student‚Äôs performance is insufficient according to at least one criterion for Pass."
  },
  {
    "objectID": "admin/KB.html#course-literature",
    "href": "admin/KB.html#course-literature",
    "title": "Department of Statistics, VT 2025",
    "section": "",
    "text": "The main book for the course is:\n\nWackerley, Mendenhall and Scheaffer (2021). Mathematical Statistics with Applications, 7th edition, Cengage.\n\nOther course materials such as supplementary materials, lecture notes, exercises, and instructions for assignments will be posted on the course web page. Links to selected online sources will also be provided there."
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Programming for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future."
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html",
    "href": "tutorial/bootstrap/bootstrap.html",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "",
    "text": "In this tutorial you will learn about the bootstrap method for approximating the sampling distribution of any estimator, for example the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\nLet‚Äôs first load some libraries that we will use (install them using install.packages() if you haven‚Äôt already).\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html#footnotes",
    "href": "tutorial/bootstrap/bootstrap.html#footnotes",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdf‚Ü©Ô∏é"
  },
  {
    "objectID": "interactive_exercises/IEChapter4.html",
    "href": "interactive_exercises/IEChapter4.html",
    "title": "Interactive Exercises - Chapter 4",
    "section": "",
    "text": "Problem W4.1\nUse the widget for the gamma distribution in the scale parameterization (the one used in the course book) for this exercise. Note that the names for the two parameters in the Gamma distribution is not \\(\\alpha\\) and \\(\\beta\\) as in the book, but instead \\(\\alpha\\) and \\(\\theta\\). C‚Äôest la vie. ü§∑‚Äç‚ôÇÔ∏è\n\nStart with the \\(\\mathrm{Gamma}(3,1)\\) distribution and gradually move the first parameter \\(\\alpha\\) toward 1. What happens with the shape of the distribution at \\(\\alpha = 1\\)?\nLet us explore the effect of the second parameter, the scale parameter \\(\\theta\\).\n\n\nSet \\(\\alpha=2\\) and \\(\\theta=2\\). What is the mean and variance? What is \\(\\mathrm{Pr}(X\\leq 3)\\)?\nSet \\(\\alpha=4\\) and \\(\\theta=1\\), what is the mean and variance and \\(\\mathrm{Pr}(X\\leq 3)\\) now?\nWhat if \\(\\alpha=16\\) and \\(\\theta=0.25\\)?\n\n\n\n\n\n\n\nSolution W4.1a\n\n\n\n\n\nThe distribution for \\(\\alpha = 1\\) becomes highest in the point \\(x=0\\) with a monotonically decreasing density in \\(x\\). The Gamma distribution with \\(\\alpha = 1\\) is actually the exponential distribution. You can see why \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\nSolution W4.1b\n\n\n\n\n\nThe mean \\(E(X)= \\alpha \\theta\\) is the same in all the three settings and the variance \\(V(X)=\\alpha \\theta^2\\) decreases as \\(\\theta\\) becomes smaller, and so does \\(\\mathrm{Pr}(X\\leq 3)\\). This is why \\(\\theta\\) is called the scale parameter, it controls the scale (standard deviation) of the distribution.\n\n\n\n\n\n\nProblem W4.2\nUse the widget for the beta distribution for this exercise.\n\nStart with the \\(\\mathrm{Beta}(1,1)\\) distribution. Is there another name for this distribution?\nNow, change to \\(\\mathrm{Beta}(2,2)\\), then to \\(\\mathrm{Beta}(3,3)\\), then finally to \\(\\mathrm{Beta}(10,10)\\). What can you say about the shape for these settings?\nSet \\(\\alpha\\) and \\(\\beta\\) so that most of the density mass is on the right hand side, i.e.¬†for values close to 1. Give a configuration of \\(\\alpha\\) and \\(\\beta\\) achieves this. Finally, set \\(\\alpha\\) and \\(\\beta\\) so that the values close to \\(x=1\\) has the highest density and the density is monotonically decreasing toward \\(x=0\\).\nCan you make the density symmetric around \\(x=0.5\\) and bathtub shaped with most of the density close to 0 and 1?\n\n\n\n\n\n\nSolution W4.2a\n\n\n\n\n\nThe density becomes constant over the support (0,1). This is the uniform distribution on (0,1).\n\n\n\n\n\n\n\n\n\nSolution W4.2b\n\n\n\n\n\nThe Beta distribution is symmetric around \\(x=0.5\\) for all values of \\(\\alpha\\) and \\(\\beta\\) where \\(\\alpha=\\beta\\). When the parameters grows larger the density becomes more concentrated around \\(x=0.5\\).\n\n\n\n\n\n\n\n\n\nSolution W4.2c\n\n\n\n\n\nFor example \\(\\alpha =20\\) and \\(\\beta=2\\) gives most of the mass near \\(x=1\\). Moving \\(\\beta\\) below 1 (still keeping \\(\\alpha\\) at 20) gives a density that has its maximum at \\(x=1\\) and monotonically decreases as we move down toward \\(x=0\\).\n\n\n\n\n\n\n\n\n\nSolution W4.2d\n\n\n\n\n\nSetting \\(\\alpha=\\beta\\) and both parameters smaller than 1 gives a symmetric bathtub shape."
  },
  {
    "objectID": "interactive_exercises/IEChapter5.html",
    "href": "interactive_exercises/IEChapter5.html",
    "title": "Interactive Exercises - Chapter 5",
    "section": "",
    "text": "Problem W5.1\nUse the widget for the law of large numbers for this exercise with the population parameters \\(\\mu=3\\) and \\(\\sigma=0.2\\).\n\nWhat is the smallest sample size \\(n\\) that gives a probability of at most \\(0.01\\) for the event that the sample mean deviates from its mean \\(\\mu = 3\\) by at least \\(\\epsilon = 0.1\\) units? That is, use the widget to determine the smallest \\(n\\) for which \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\leq 0.01.\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=26\\) gives \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.01079\\] so this sample size is not large enough. However, for \\(n=27\\) we get \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.1) \\approx 0.009375,\\] which is smaller than the required probability of \\(0.01\\). So \\(n=27\\) is the smallest possible sample size. Check for yourself: \n\n\n\n\nLet‚Äôs be even more demanding now and require that the sample mean can deviate by at most \\(\\epsilon = 0.01\\) units from the mean \\(\\mu\\). What is now the smallest sample size \\(n\\) that achieves this?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample size \\(n=2654\\) is the smallest \\(n\\) and \\[\\mathrm{Pr}(\\vert \\bar{X}_n - 3 \\vert &gt; 0.01) \\approx 0.009999\\]\n\n\n\n\n\n\nProblem W5.2\nUse the widget for the central limit theorem for this exercise.\n\nChoose the Beta distribution with parameters \\(\\alpha=0.5\\) and \\(\\beta=0.5\\) as the data distribution. Set sample size \\(n=2\\) and look at the orange histogram that shows the sampling distribution of the sample mean for a sample of size \\(n=2\\). Does it look normally distributed? Continue to increase sample size \\(n\\) to 3, 4, 5 and so on. How large \\(n\\) do you need for the sampling distribution to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(n=2\\) the distribution is no longer bathtub shaped, but it is clearly not normal (yet). It is hard to say exactly of course, but already for \\(n=10\\) is the sampling distribution roughly bell shaped like the normal distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the chi-squared distribution with \\(\\nu=3\\) degrees of freedom.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt takes at least until \\(n=20\\) for the sampling distribution to no longer have the long right hand tail of the chi-squared distribution.\n\n\n\n\nrepeat Problem W5.2a, but now for the Cauchy distribution with location \\(m=0\\) and scale \\(\\gamma=1\\). How large must \\(n\\) be before the sampling distribution of \\(\\bar{X}\\) seems to be approximately normal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Cauchy distribution is one of the cases where the central limit theorem does not hold. No matter how large you make \\(n\\), the distribution of \\(\\bar{X}\\) will never be normal. The mean and variance of the Cauchy do not exist, which violates the assumptions of the theorem; it has so extremely heavy tails that the mean does not exist, even though the Cauchy distribution is symmetric around the location \\(m\\). ü§Ø"
  },
  {
    "objectID": "hidden/AssignmentPart1.html",
    "href": "hidden/AssignmentPart1.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "This exercise concerned with the exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) with (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization the parameter \\(\\beta\\) is called a scale parameter and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta.\\) If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density. [Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations/data points is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1/2)\\) , \\(\\mathrm{Expon}(\\beta = 1)\\) and \\(\\mathrm{Expon}(\\beta = 5)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)?\n[Hint: the sort function might be handy for the empirical cdf, and don‚Äôt forget about the so called p-functions in R.]\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how:\n\na sample median is defined and\nhow a median of a statistical distribution is defined.\n\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really seems to fulfill the requires property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}"
  },
  {
    "objectID": "hidden/AssignmentPart1.html#introduction",
    "href": "hidden/AssignmentPart1.html#introduction",
    "title": "Statistal Theory and Modeling, 7.5 hp",
    "section": "Introduction",
    "text": "Introduction\nThis is a journey into sound.\n\ncarSales = csv.read(\"~/Dropbox/Teaching/STM/data/car_prices.csv\")"
  },
  {
    "objectID": "data/car_prices.html",
    "href": "data/car_prices.html",
    "title": "Vehicle Sales Data",
    "section": "",
    "text": "Vehicle Sales Data\nSource: Kaggle https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data?resource=download"
  },
  {
    "objectID": "hidden/AssignmentPart1.html#problem-2",
    "href": "hidden/AssignmentPart1.html#problem-2",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2",
    "text": "Problem 2\n\nProblem 2a)\nThe file bugs.csv contains a dataset with with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs, which we will store in a vector y, for simplicity. Load the data by placing the file in your working directory and run the commands\n\ndata = read.csv(\"bugs.csv\", header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first the model\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here and \\(\\overset{iid}{\\sim}\\) means that the observations are assumed independent and identically distributed (that is, each observation has the same Poisson distribution).\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does the Poisson model with this estimate fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or proportions(table(y)) gives a table of proportions and barplot plots a bar chart, which is suitable for discrete data. A histogram is easier, however.]\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu)  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean. Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=10\\) and \\(r=100\\) (one curve for each) to the plot produce in Problem 2a). Which of these models to you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\)."
  },
  {
    "objectID": "hidden/AssignmentPart1Solutions.html",
    "href": "hidden/AssignmentPart1Solutions.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "This exercise concerned with the exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) with (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization the parameter \\(\\beta\\) is called a scale parameter and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta\\). If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\n\n# Solution\nlambda = 2\nx = rexp(n = 10000, rate = lambda)\nmean(x) # this is close to 1/lambda = 1/2, so it is indeed the rate parameterization.\n\n[1] 0.495699\n\n\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density. [Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.\n\n\n\n\n# Solution\nn = 200\nbeta_ = 2\nx = rexp(n, rate = 1/beta_)\nhist(x, 30, freq = FALSE, col = colors[5])\nxGrid = seq(0, 10, length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/beta_), lwd = 3, col = colors[3])\n\n\n\n\n\n\n\n\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\n\nset.seed(123)\nhist(x, 30, freq = FALSE, col = colors[5], ylim = c(0,1))\nxGrid = seq(0, max(x), length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/1), lwd = 3, col = colors[2])\nlines(xGrid, dexp(xGrid, rate = 1/2), lwd = 3, col = colors[3])\nlines(xGrid, dexp(xGrid, rate = 1/3), lwd = 3, col = colors[4])\nlegend(x = \"topright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(2,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\n\n\n\n\n\n\n\nThe data was simulated from \\(\\mathrm{Exp}(\\beta=2)\\) so we expect that distribution to give the best fit, and it does so visually.\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations/data points is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1/2)\\) , \\(\\mathrm{Expon}(\\beta = 1)\\) and \\(\\mathrm{Expon}(\\beta = 5)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)? [Hint: the sort function might be handy, and don‚Äôt forget about the p-functions in R.]\n\n\n\n\nx_sort = sort(x)\necdf = (1/n)*seq(1,n)\nplot(x_sort, ecdf, type = \"l\", lwd = 3, col = colors[1])\nlines(xGrid, pexp(xGrid, rate = 1/1), col = colors[2])\nlines(xGrid, pexp(xGrid, rate = 1/2), col = colors[3])\nlines(xGrid, pexp(xGrid, rate = 1/3), col = colors[4])\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\n\n\n\n\n\n\n\nThe best fit is again seen to be the \\(\\mathrm{Exp}(\\beta=2)\\) model. The other two distributions do not fit the empirical cdf well at all.\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how a sample median is defined and how a median of a statistical distribution is defined.\n\n\n\n\n# Solution:\nmessage(paste(\"the median of the sample is\", median(x)))\n\nthe median of the sample is 1.21345029212534\n\nmessage(paste(\"the median of the Expon(1) distribution is \", qexp(0.5, rate = 1/1)))\n\nthe median of the Expon(1) distribution is  0.693147180559945\n\nmessage(paste(\"the median of the Expon(2) distribution is \", qexp(0.5, rate = 1/2)))\n\nthe median of the Expon(2) distribution is  1.38629436111989\n\nmessage(paste(\"the median of the Expon(3) distribution is \", qexp(0.5, rate = 1/3)))\n\nthe median of the Expon(3) distribution is  2.07944154167984\n\n\nThe sample median is the midpoint of an ordered sample of observations. The median for a statistical distribution is the point which has 0.5 (50%) of the probability mass to the left of the point, i.e.¬†a point \\(x=x_{\\mathrm{median}}\\) such that \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})=0.5\\). For a discrete distribution we may not be able to find a point which has that property (since the cdf jumps at every observed value) and we then define the median as the smallest \\(x\\) which satisfies \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})\\geq 0.5\\).\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really seems to fulfill the requires property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\n\ndelta_x = 0.01 # This is the small rectangle width\nxStar = seq(0, 50, by = delta_x)\nfGrid = dexp(xStar, rate = 1/2) # This is the density evaluated at each xStar\nsum(fGrid*delta_x)\n\n[1] 1.002502\n\n\nIt seems to integrate to one. Note that the support of the exponential distribution is from \\(0\\) to \\(\\infty\\), so we don‚Äôt need to consider negative \\(x\\)-values.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}\n\n\n\n\nHere is the rectangle sum approximation\n\nsum(xStar*fGrid*delta_x)\n\n[1] 1.999996\n\n\nand here is the result using integrate\n\nf &lt;- function(x){\n  return(x*dexp(x, rate = 1/2)) # Note that the function we want to integrate here is really x times the density f(x)\n}\nintegrate(f, 0, Inf)\n\n2 with absolute error &lt; 7.7e-06"
  },
  {
    "objectID": "hidden/AssignmentPart1Solutions.html#problem-2",
    "href": "hidden/AssignmentPart1Solutions.html#problem-2",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2",
    "text": "Problem 2\n\nProblem 2a)\nThe file bugs.csv contains a dataset with with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs. which we will store in a vector y, for simplicity. Load the data by placing the file in your working directory and run the commands\n\ndata = read.csv(\"bugs.csv\", header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first an iid \\(\\mathrm{Poisson}(\\lambda)\\) model for the data, that is\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here.\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does the Poisson model with this estimate fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or proportions(table(y)) gives a table of proportions and barplot plots a bar chart, which is suitable for discrete data]\n\n\nSolution 2a)\n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nThe fit is rather terrible. Let‚Äôs check if the mean and variance are approximately equal in the data:\n\nmessage(paste(\"the mean is \", mean(y)))\n\nthe mean is  5.25274725274725\n\nmessage(paste(\"the variance is \", var(y)))\n\nthe variance is  29.2576312576313\n\n\nThe variance is much larger than the mean. The poisson model forces the mean to be the same as the variance, which does not agree with the data. The Poisson model is not working for this data.\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu)  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean. Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=10\\) and \\(r=100\\) (one curve for each) to the plot produce in Problem 2a). Which of these models to you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\).\n\n\nSolution 2b)\n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 1, mu = ybar), type=\"b\", col= colors[2], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 3, mu = ybar), type=\"b\", col= colors[4], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 100, mu = ybar), type=\"b\", col= colors[5], lwd = 2, pch = 19)\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"Poisson\", \"Negbin r = 1\", \"Negbin r = 3\", \"Negbin r = 100\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[3], colors[2], colors[4], colors[5]))\n\n\n\n\n\n\n\n\nThe negative binomial model with \\(r=1\\) seems to fit the data best. The model with \\(r=100\\) is closest to the Poisson. This is because the \\(\\mathrm{Negbin}(r,\\mu)\\) model approaches the \\(\\mathrm{Poisson}(\\lambda=\\mu)\\) as \\(r \\rightarrow \\infty\\) ."
  },
  {
    "objectID": "hidden/AssignmentPart1.html#problem-3-to-be-done-after-lecture-6",
    "href": "hidden/AssignmentPart1.html#problem-3-to-be-done-after-lecture-6",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 [To be done after Lecture 6]",
    "text": "Problem 3 [To be done after Lecture 6]\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulation. Plot a histogram. Use the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram.\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]"
  },
  {
    "objectID": "hidden/AssignmentPart1Solutions.html#problem-3-to-be-done-after-lecture-6",
    "href": "hidden/AssignmentPart1Solutions.html#problem-3-to-be-done-after-lecture-6",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 [To be done after Lecture 6]",
    "text": "Problem 3 [To be done after Lecture 6]\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulation. Plot a histogram. Use the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram.\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]"
  },
  {
    "objectID": "hidden/AssignmentPart1Solutions.html#problem-3---solution",
    "href": "hidden/AssignmentPart1Solutions.html#problem-3---solution",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 - Solution",
    "text": "Problem 3 - Solution"
  },
  {
    "objectID": "assignment/AssignmentPart1.html",
    "href": "assignment/AssignmentPart1.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "The exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) has (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization, the parameter \\(\\beta\\) is called a scale parameter, and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta.\\) If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density.\n[Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.]\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf, if you are curious. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1)\\) , \\(\\mathrm{Expon}(\\beta = 2)\\) and \\(\\mathrm{Expon}(\\beta = 3)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)?\n[Hint: the sort function might be handy for the empirical cdf, and don‚Äôt forget about the so called p-functions in R.]\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how:\n\na sample median is defined and\nhow a median of a statistical distribution is defined.\n\n[Hint: recall the so called q-functions in R].\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really fulfills the required property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n(Here we actually know the result, the expected value of \\(\\mathrm{Expon}(\\beta)\\) is \\(\\beta\\), but numerical integration technique can be used for the expectation of any function, for example if you are interested in \\(\\mathbb{E}(\\log (X))\\), when \\(X \\sim \\mathrm{Expon}(\\beta)\\)).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyCubicFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}\nmyCubicFunction(3)\n\n[1] 27"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-2",
    "href": "assignment/AssignmentPart1.html#problem-2",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2",
    "text": "Problem 2\n\nProblem 2a)\nThe file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs, which we will store in a vector y, for simplicity. Load the data like this:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first the model\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here and \\(\\overset{iid}{\\sim}\\) means that the observations are assumed independent and identically distributed (that is, each observation is assumed to come from the same Poisson distribution).\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does this Poisson model fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or use proportions(table(y)) to compute a table of proportions and then use barplot to plot a bar chart, which is suitable for discrete data. A histogram is easier, however.]\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu),  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean \\(\\bar y\\). Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=3\\) and \\(r=100\\) (one curve for each) to the plot you did in Problem 2a). Which of these models do you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\).]"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-3-to-be-done-after-lecture-6",
    "href": "assignment/AssignmentPart1.html#problem-3-to-be-done-after-lecture-6",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 [To be done after Lecture 6]",
    "text": "Problem 3 [To be done after Lecture 6]\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulation. Plot a histogram. Use the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram.\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-3",
    "href": "assignment/AssignmentPart1.html#problem-3",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3",
    "text": "Problem 3\n\nProblem 3a)\n\nThis problem is to be done after Lecture 6.\n\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulation. Plot a histogram.\n\n\nProblem 3b)\nUse the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram from Problem 3a).\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]\n\n\nProblem 3c)\nUse (Monte Carlo) simulation with \\(m=10000\\) random draws to estimate \\(\\mathrm{E}(Y)\\), where, as before, \\(Y=\\exp(X)\\) and \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of \\(10,20,30,\\ldots,9900, 10000\\). Does the estimate seem to converge (settle down) to the true expectation, which happens to be \\(\\mathrm{E}(Y)=\\exp(\\frac{1}{2})\\)? [How do I know that this is the true expected value? See this: https://en.wikipedia.org/wiki/Log-normal_distribution]"
  },
  {
    "objectID": "hidden/SolutionsPart1.html",
    "href": "hidden/SolutionsPart1.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "The exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) has (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization, the parameter \\(\\beta\\) is called a scale parameter, and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta.\\) If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\n\n# Solution\nlambda = 2\nx = rexp(n = 10000, rate = lambda)\nmean(x) # this is close to 1/lambda = 1/2, so it is indeed the rate parameterization.\n\n[1] 0.4957794\n\n\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density.\n[Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.]\n\n\n\n\n# Solution\nn = 200\nbeta_ = 2\nx = rexp(n, rate = 1/beta_)\nhist(x, 30, freq = FALSE, col = colors[5])\nxGrid = seq(0, 10, length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/beta_), lwd = 3, col = colors[3])\n\n\n\n\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\n\nset.seed(123)\nhist(x, 30, freq = FALSE, col = colors[5], ylim = c(0,1))\nxGrid = seq(0, max(x), length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/1), lwd = 3, col = colors[2])\nlines(xGrid, dexp(xGrid, rate = 1/2), lwd = 3, col = colors[3])\nlines(xGrid, dexp(xGrid, rate = 1/3), lwd = 3, col = colors[4])\nlegend(x = \"topright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(2,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\nWarning in strheight(legend, units = \"user\", cex = cex)/yc: longer object\nlength is not a multiple of shorter object length\n\n\nWarning in left + xchar + xextra + (w0 * rep.int(0:(ncol - 1),\nrep.int(n.legpercol, : longer object length is not a multiple of shorter object\nlength\n\n\nWarning in (rep.int(1L:n.legpercol, ncol)[1L:n.leg] - 1 + !is.null(title)) * :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in top - 0.5 * yextra - ymax - (rep.int(1L:n.legpercol, ncol)[1L:n.leg]\n- : longer object length is not a multiple of shorter object length\n\n\nWarning in xt[ok.l] + x.off * xchar: longer object length is not a multiple of\nshorter object length\n\n\nWarning in x1 + dx: longer object length is not a multiple of shorter object\nlength\n\n\nWarning in xt + (seg.len + x.off) * xchar: longer object length is not a\nmultiple of shorter object length\n\n\nWarning in xt + x.intersp * xchar: longer object length is not a multiple of\nshorter object length\n\n\n\n\n\nThe data was simulated from \\(\\mathrm{Exp}(\\beta=2)\\) so we expect that distribution to give the best fit, and it does so visually.\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf, if you are curious. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1)\\) , \\(\\mathrm{Expon}(\\beta = 2)\\) and \\(\\mathrm{Expon}(\\beta = 3)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)?\n[Hint: the sort function might be handy for the empirical cdf, and don‚Äôt forget about the so called p-functions in R.]\n\n\n\n\nx_sort = sort(x)\necdf = (1/n)*seq(1,n)\nplot(x_sort, ecdf, type = \"l\", lwd = 3, col = colors[1])\nlines(xGrid, pexp(xGrid, rate = 1/1), col = colors[2])\nlines(xGrid, pexp(xGrid, rate = 1/2), col = colors[3])\nlines(xGrid, pexp(xGrid, rate = 1/3), col = colors[4])\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\nWarning in strheight(legend, units = \"user\", cex = cex)/yc: longer object\nlength is not a multiple of shorter object length\n\n\nWarning in left + xchar + xextra + (w0 * rep.int(0:(ncol - 1),\nrep.int(n.legpercol, : longer object length is not a multiple of shorter object\nlength\n\n\nWarning in (rep.int(1L:n.legpercol, ncol)[1L:n.leg] - 1 + !is.null(title)) * :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in top - 0.5 * yextra - ymax - (rep.int(1L:n.legpercol, ncol)[1L:n.leg]\n- : longer object length is not a multiple of shorter object length\n\n\nWarning in xt[ok.l] + x.off * xchar: longer object length is not a multiple of\nshorter object length\n\n\nWarning in x1 + dx: longer object length is not a multiple of shorter object\nlength\n\n\nWarning in xt + (seg.len + x.off) * xchar: longer object length is not a\nmultiple of shorter object length\n\n\nWarning in xt + x.intersp * xchar: longer object length is not a multiple of\nshorter object length\n\n\n\n\n\nThe best fit is again seen to be the \\(\\mathrm{Exp}(\\beta=2)\\) model. The other two distributions do not fit the empirical cdf well at all.\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how:\n\na sample median is defined and\nhow a median of a statistical distribution is defined.\n\n[Hint: recall the so called q-functions in R].\n\n\n\n\n# Solution:\nmessage(paste(\"the median of the sample is\", median(x)))\n\nthe median of the sample is 1.10958895180374\n\nmessage(paste(\"the median of the Expon(1) distribution is \", qexp(0.5, rate = 1/1)))\n\nthe median of the Expon(1) distribution is  0.693147180559945\n\nmessage(paste(\"the median of the Expon(2) distribution is \", qexp(0.5, rate = 1/2)))\n\nthe median of the Expon(2) distribution is  1.38629436111989\n\nmessage(paste(\"the median of the Expon(3) distribution is \", qexp(0.5, rate = 1/3)))\n\nthe median of the Expon(3) distribution is  2.07944154167984\n\n\nThe sample median is the midpoint of an ordered sample of observations. The median for a statistical distribution is the point which has 0.5 (50%) of the probability mass to the left of the point, i.e.¬†a point \\(x=x_{\\mathrm{median}}\\) such that \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})=0.5\\). For a discrete distribution we may not be able to find a point which has that property (since the cdf jumps at every observed value) and we then define the median as the smallest \\(x\\) which satisfies \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})\\geq 0.5\\).\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really fulfills the required property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\n\ndelta_x = 0.01 # This is the small rectangle width\nxStar = seq(0, 50, by = delta_x)\nfGrid = dexp(xStar, rate = 1/2) # This is the density evaluated at each xStar\nsum(fGrid*delta_x)\n\n[1] 1.002502\n\n\nIt seems to integrate to one. Note that the support of the exponential distribution is from \\(0\\) to \\(\\infty\\), so we don‚Äôt need to consider negative \\(x\\)-values.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n(Here we actually know the result, the expected value of \\(\\mathrm{Expon}(\\beta)\\) is \\(\\beta\\), but numerical integration technique can be used for the expectation of any function, for example if you are interested in \\(\\mathbb{E}(\\log (X))\\), when \\(X \\sim \\mathrm{Expon}(\\beta)\\)).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyCubicFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}\nmyCubicFunction(3)\n\n[1] 27\n\n\n\n\n\nHere is the rectangle sum approximation\n\nsum(xStar*fGrid*delta_x)\n\n[1] 1.999996\n\n\nand here is the result using integrate\n\nf &lt;- function(x){\n  return(x*dexp(x, rate = 1/2)) # Note that the function we want to integrate here is really x times the density f(x)\n}\nintegrate(f, 0, Inf)\n\n2 with absolute error &lt; 7.7e-06"
  },
  {
    "objectID": "hidden/SolutionsPart1.html#problem-2",
    "href": "hidden/SolutionsPart1.html#problem-2",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2",
    "text": "Problem 2\n\nProblem 2a)\nThe file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs, which we will store in a vector y, for simplicity. Load the data like this:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first the model\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here and \\(\\overset{iid}{\\sim}\\) means that the observations are assumed independent and identically distributed (that is, each observation is assumed to come from the same Poisson distribution).\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does this Poisson model fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or use proportions(table(y)) to compute a table of proportions and then use barplot to plot a bar chart, which is suitable for discrete data. A histogram is easier, however.]\n\n\n Solution 2a) \n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nThe fit is rather terrible. Let‚Äôs check if the mean and variance are approximately equal in the data:\n\nmessage(paste(\"the mean is \", mean(y)))\n\nthe mean is  5.25274725274725\n\nmessage(paste(\"the variance is \", var(y)))\n\nthe variance is  29.2576312576313\n\n\nThe variance is much larger than the mean. The poisson model forces the mean to be the same as the variance, which does not agree with the data. The Poisson model is not working for this data.\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu),  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean \\(\\bar y\\). Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=3\\) and \\(r=100\\) (one curve for each) to the plot you did in Problem 2a). Which of these models do you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\).]\n\n\n Solution 2b) \n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 1, mu = ybar), type=\"b\", col= colors[2], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 3, mu = ybar), type=\"b\", col= colors[4], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 100, mu = ybar), type=\"b\", col= colors[5], lwd = 2, pch = 19)\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"Poisson\", \"Negbin r = 1\", \"Negbin r = 3\", \"Negbin r = 100\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[3], colors[2], colors[4], colors[5]))\n\n\n\n\n\n\n\n\nThe negative binomial model with \\(r=1\\) seems to fit the data best. The model with \\(r=100\\) is closest to the Poisson. This is because the \\(\\mathrm{Negbin}(r,\\mu)\\) model approaches the \\(\\mathrm{Poisson}(\\lambda=\\mu)\\) as \\(r \\rightarrow \\infty\\) ."
  },
  {
    "objectID": "hidden/SolutionsPart1.html#problem-3",
    "href": "hidden/SolutionsPart1.html#problem-3",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3",
    "text": "Problem 3\n\nProblem 3a)\n\nThis problem is to be done after Lecture 6.\n\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulating 10000 draws. Plot a histogram with 100 bins.\n\n\n Solution 3a) \n\nx = rnorm(10000, mean = 0, sd = 1)\ny = exp(x)\nhist(y, 200, xlim = c(0, 20), freq = FALSE, col = colors[5])\n\n\n\n\n\n\n\n\n\n\nProblem 3b)\nUse the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram from Problem 3a).\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]\n\n\n Solution 3b) \nLet \\(Y=h(X)\\) where the function \\(h(x) = \\exp (x)\\). Since the exponential function is a monotonically increasing function, the method of transformation says that the density for \\(Y\\) is given by \\[\nf_Y(y) = f_X(h^{-1}(y))\\cdot \\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y}\n\\] where \\(f_X(x)\\) is the density for \\(X\\sim N(0,1)\\) here and, \\(f_Y(y)\\) is the density for \\(Y=\\exp(X)\\) that we want. The function \\(f^{-1}(y)\\) is the inverse function to the function \\(y=h(x)\\). Since \\(h(x) = \\exp (x)\\), the inverse function is the natural logarithm function \\(x = \\log(y)\\). We know the derivative of the log function: \\[\n\\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y} = \\frac{\\mathrm{d}  \\log(y)}{\\mathrm{d} y} = \\frac{1}{y}\n\\] So all we need to do now is to pop in the expressions for the inverse function \\(x = f^{-1}(y) = \\log(y)\\) and its derivative \\(\\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y}\\) into the normal density function \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big(-\\frac{1}{2}x^2\\Big)\n\\] Here we go: \\[\nf_Y(y) = f_X(h^{-1}(y))\\cdot \\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y} = \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big(-\\frac{1}{2}(\\log(y))^2\\Big)\\cdot \\frac{1}{y}\n\\] To plot this I will make a function for the new density\n\nf_Y &lt;- function(y){\n  \n  # This version that I have commented out is a direct implementation of the \n  # formula above, but there is a more clever way ...\n  # pdf = (1/(sqrt(2*pi)))*exp(-0.5*(log(y))^2)\n  \n  # This version is more clever, using that the normal pdf is available in R:\n  pdf = dnorm(log(y), mean = 0, sd = 1)*(1/y)\n  return(pdf)\n}\n\nHere is the plot of this derived density overlayed on the histogram. As expected, they match, we did the derivation correctly!\n\nhist(y, 200, xlim = c(0, 20), freq = FALSE, col = colors[5], ylim = c(0,0.7))\nyGrid = seq(0, 20, length = 1000)\nlines(yGrid, f_Y(yGrid), col = colors[3], type = \"l\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\nProblem 3c)\nUse (Monte Carlo) simulation with \\(m=10000\\) random draws from Problem 3c) to estimate \\(\\mathrm{E}(Y)\\), where, as before, \\(Y=\\exp(X)\\) and \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of \\(10,20,30,\\ldots,9900, 10000\\). Does the estimate seem to converge (settle down) to the true expectation, which happens to be \\(\\mathrm{E}(Y)=\\exp(\\frac{1}{2})\\)? [How do I know that this is the true expected value? See this: https://en.wikipedia.org/wiki/Log-normal_distribution]\n\n\n Solution 3c) \nThe Monte Carlo estimate is just to compute the sample mean of the computed \\(y=\\exp(x)\\) draws:\n\nmean(y)\n\n[1] 1.646379\n\n\nWe check convergence by computing this estimate with increasing sample sizes. The estimate seems to settle down, i.e.¬†converge.\n\nsampleSizes = seq(10, 10000,by = 10)\nestimates = rep(NA, length(sampleSizes))\ncount = 0\nfor (m in sampleSizes){\n  count = count + 1\n  estimates[count] = mean(y[1:m])\n}\nplot(sampleSizes, estimates, type = \"l\", lwd = 2, col = colors[1], xlab = \"Number of Monte Carlo draws\", ylab = \"Estimate\")\nabline(h = exp(1/2), lwd = 1, col = colors[3])\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Monte Carlo estimate\", \"True value\"),\n        lwd = c(2,1), cex = c(1,1), col = c(colors[1], colors[3]))"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-3---transforming-random-variables",
    "href": "assignment/AssignmentPart1.html#problem-3---transforming-random-variables",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 - Transforming random variables",
    "text": "Problem 3 - Transforming random variables\n\nProblem 3a)\n\nThis problem is to be done after Lecture 6.\n\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulating 10000 draws. Plot a histogram with 100 bins.\n\n\nProblem 3b)\nUse the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(y)=\\frac{1}{\\sqrt{2\\pi}y}\\exp\\Big(-\\frac{1}{2}(\\log(y))^2\\Big)\n\\]\nOverlay a plot of this density in the histogram from Problem 3a).\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]\n\n\nProblem 3c)\nUse (Monte Carlo) simulation with \\(m=10000\\) random draws to estimate \\(\\mathrm{E}(Y)\\), where, as before, \\(Y=\\exp(X)\\) and \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of \\(10,20,30,\\ldots,9900, 10000\\). Does the estimate seem to converge (settle down) to the true expectation, which happens to be \\(\\mathrm{E}(Y)=\\exp(\\frac{1}{2})\\)? [How do I know that this is the true expected value? See this: https://en.wikipedia.org/wiki/Log-normal_distribution]"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-2---probability-models-for-count-data",
    "href": "assignment/AssignmentPart1.html#problem-2---probability-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2 - Probability models for count data",
    "text": "Problem 2 - Probability models for count data\n\nProblem 2a)\nThe file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs, which we will store in a vector y, for simplicity. Load the data like this:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first the model\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here and \\(\\overset{iid}{\\sim}\\) means that the observations are assumed independent and identically distributed (that is, each observation is assumed to come from the same Poisson distribution).\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does this Poisson model fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or use proportions(table(y)) to compute a table of proportions and then use barplot to plot a bar chart, which is suitable for discrete data. A histogram is easier, however.]\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu),  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean \\(\\bar y\\). Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=3\\) and \\(r=100\\) (one curve for each) to the plot you did in Problem 2a). Which of these models do you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\).]"
  },
  {
    "objectID": "hidden/SolutionsPart1.html#problem-2---probability-models-for-count-data",
    "href": "hidden/SolutionsPart1.html#problem-2---probability-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 2 - Probability models for count data",
    "text": "Problem 2 - Probability models for count data\n\nProblem 2a)\nThe file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Here we will only analyze the variable nBugs, which we will store in a vector y, for simplicity. Load the data like this:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", header = TRUE)\ny = data$nBugs # number of bugs, a vector with n = 91 observations\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent and identically distributed. Consider first the model\n\\[Y_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{Poisson}(\\lambda)\\]\nwhere \\(n=91\\) here and \\(\\overset{iid}{\\sim}\\) means that the observations are assumed independent and identically distributed (that is, each observation is assumed to come from the same Poisson distribution).\nSince \\(\\lambda\\) is the mean in the \\(\\mathrm{Poisson}(\\lambda)\\) distribution, a reasonable estimator of \\(\\lambda\\) is the sample mean \\(\\bar y\\). Plot a histogram of the data and overlay the density of Poisson distribution with \\(\\lambda = \\bar y\\). Does this Poisson model fit the data well. If not, why?\n[Hint: either use a histogram when plotting the data, or use proportions(table(y)) to compute a table of proportions and then use barplot to plot a bar chart, which is suitable for discrete data. A histogram is easier, however.]\n\n\nSolution 2a)\n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\n\n\n\n\nThe fit is rather terrible. Let‚Äôs check if the mean and variance are approximately equal in the data:\n\nmessage(paste(\"the mean is \", mean(y)))\n\nthe mean is  5.25274725274725\n\nmessage(paste(\"the variance is \", var(y)))\n\nthe variance is  29.2576312576313\n\n\nThe variance is much larger than the mean. The poisson model forces the mean to be the same as the variance, which does not agree with the data. The Poisson model is not working for this data.\n\n\nProblem 2b)\nLet us now try to with a negative binomial model for the data. We will use the variant that counts the number of failures until \\(r\\) successes has been observed, and we will use the alternative parameterization with an explicit parameter \\(\\mu\\) for the mean. So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu),  \n\\]\nwhere each random variable \\(Y_i\\) can take on values in the set \\(\\{0,1,2,\\ldots\\}\\). Since \\(\\mu\\) is the mean, we will estimate it with the sample mean \\(\\bar y\\). Add the probability function from the negative binomial model for three different \\(r\\) values: \\(r=1\\), \\(r=3\\) and \\(r=100\\) (one curve for each) to the plot you did in Problem 2a). Which of these models do you prefer? Why? Which of the negative binomial models is closest to the Poisson model? Why?\n\n[hint: note that R has the dnbinom function that can be called with the mean parameterization. For example, dnbinom(1, size = 3, mu = 2) give the probability \\(\\mathrm{Pr}(Y=1)\\) when \\(Y\\sim \\mathrm{NegBin}(r = 3,\\mu = 2)\\), so that the argument size is the parameter \\(r\\).]\n\n\nSolution 2b)\n\nybar = mean(y)\nhist(y, 20, freq = FALSE, col = colors[1], ylim = c(0,0.25))\nyGrid = seq(0, max(y))\nlines(yGrid, dpois(yGrid, lambda = ybar), type=\"b\", col= colors[3], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 1, mu = ybar), type=\"b\", col= colors[2], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 3, mu = ybar), type=\"b\", col= colors[4], lwd = 2, pch = 19)\nlines(yGrid, dnbinom(yGrid, size = 100, mu = ybar), type=\"b\", col= colors[5], lwd = 2, pch = 19)\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"Poisson\", \"Negbin r = 1\", \"Negbin r = 3\", \"Negbin r = 100\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[3], colors[2], colors[4], colors[5]))\n\nWarning in strheight(legend, units = \"user\", cex = cex)/yc: longer object\nlength is not a multiple of shorter object length\n\n\nWarning in left + xchar + xextra + (w0 * rep.int(0:(ncol - 1),\nrep.int(n.legpercol, : longer object length is not a multiple of shorter object\nlength\n\n\nWarning in (rep.int(1L:n.legpercol, ncol)[1L:n.leg] - 1 + !is.null(title)) * :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in top - 0.5 * yextra - ymax - (rep.int(1L:n.legpercol, ncol)[1L:n.leg]\n- : longer object length is not a multiple of shorter object length\n\n\nWarning in xt[ok.l] + x.off * xchar: longer object length is not a multiple of\nshorter object length\n\n\nWarning in x1 + dx: longer object length is not a multiple of shorter object\nlength\n\n\nWarning in xt + (seg.len + x.off) * xchar: longer object length is not a\nmultiple of shorter object length\n\n\nWarning in xt + x.intersp * xchar: longer object length is not a multiple of\nshorter object length\n\n\n\n\n\nThe negative binomial model with \\(r=1\\) seems to fit the data best. The model with \\(r=100\\) is closest to the Poisson. This is because the \\(\\mathrm{Negbin}(r,\\mu)\\) model approaches the \\(\\mathrm{Poisson}(\\lambda=\\mu)\\) as \\(r \\rightarrow \\infty\\) ."
  },
  {
    "objectID": "hidden/SolutionsPart1.html#problem-3---transforming-random-variables",
    "href": "hidden/SolutionsPart1.html#problem-3---transforming-random-variables",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 - Transforming random variables",
    "text": "Problem 3 - Transforming random variables\n\nProblem 3a)\n\nThis problem is to be done after Lecture 6.\n\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulating 10000 draws. Plot a histogram with 100 bins.\n\n\nSolution 3a)\n\nx = rnorm(10000, mean = 0, sd = 1)\ny = exp(x)\nhist(y, 200, xlim = c(0, 20), freq = FALSE, col = colors[5])\n\n\n\n\n\n\nProblem 3b)\nUse the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(x)=\\frac{1}{\\sqrt{2\\pi}x}\\exp\\Big(-\\frac{1}{2}(\\log(x)-\\mu)^2\\Big)\n\\]\nOverlay a plot of this density in the histogram from Problem 3a).\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]\n\n\nSolution 3b)\nLet \\(Y=h(X)\\) where the function \\(h(x) = \\exp (x)\\). Since the exponential function is a monotonically increasing function, the method of transformation says that the density for \\(Y\\) is given by \\[\nf_Y(y) = f_X(h^{-1}(y))\\cdot \\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y}\n\\] where \\(f_X(x)\\) is the density for \\(X\\sim N(0,1)\\) here and, \\(f_Y(y)\\) is the density for \\(Y=\\exp(X)\\) that we want. The function \\(f^{-1}(y)\\) is the inverse function to the function \\(y=h(x)\\). Since \\(h(x) = \\exp (x)\\), the inverse function is the natural logarithm function \\(x = \\log(y)\\). We know the derivative of the log function: \\[\n\\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y} = \\frac{\\mathrm{d}  \\log(y)}{\\mathrm{d} y} = \\frac{1}{y}\n\\] So all we need to do now is to pop in the expressions for the inverse function \\(x = f^{-1}(y) = \\log(y)\\) and its derivative \\(\\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y}\\) into the normal density function \\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big(-\\frac{1}{2}x^2\\Big)\n\\] Here we go: \\[\nf_Y(y) = f_X(h^{-1}(y))\\cdot \\frac{\\mathrm{d} f^{-1}(y)}{\\mathrm{d} y} = \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big(-\\frac{1}{2}(\\log(y))^2\\Big)\\cdot \\frac{1}{y}\n\\] To plot this I will make a function for the new density\n\nf_Y &lt;- function(y){\n  \n  # This version that I have commented out is a direct implementation of the \n  # formula above, but there is a more clever way ...\n  # pdf = (1/(sqrt(2*pi)))*exp(-0.5*(log(y))^2)\n  \n  # This version is more clever, using that the normal pdf is available in R:\n  pdf = dnorm(log(y), mean = 0, sd = 1)*(1/y)\n  return(pdf)\n}\n\nHere is the plot of this derived density overlayed on the histogram. As expected, they match, we did the derivation correctly!\n\nhist(y, 200, xlim = c(0, 20), freq = FALSE, col = colors[5], ylim = c(0,0.7))\nyGrid = seq(0, 20, length = 1000)\nlines(yGrid, f_Y(yGrid), col = colors[3], type = \"l\", lwd = 3)\n\n\n\n\n\n\nProblem 3c)\nUse (Monte Carlo) simulation with \\(m=10000\\) random draws from Problem 3c) to estimate \\(\\mathrm{E}(Y)\\), where, as before, \\(Y=\\exp(X)\\) and \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of \\(10,20,30,\\ldots,9900, 10000\\). Does the estimate seem to converge (settle down) to the true expectation, which happens to be \\(\\mathrm{E}(Y)=\\exp(\\frac{1}{2})\\)? [How do I know that this is the true expected value? See this: https://en.wikipedia.org/wiki/Log-normal_distribution]\n\n\nSolution 3c)\nThe Monte Carlo estimate is just to compute the sample mean of the computed \\(y=\\exp(x)\\) draws:\n\nmean(y)\n\n[1] 1.646379\n\n\nWe check convergence by computing this estimate with increasing sample sizes. The estimate seems to settle down, i.e.¬†converge.\n\nsampleSizes = seq(10, 10000,by = 10)\nestimates = rep(NA, length(sampleSizes))\ncount = 0\nfor (m in sampleSizes){\n  count = count + 1\n  estimates[count] = mean(y[1:m])\n}\nplot(sampleSizes, estimates, type = \"l\", lwd = 2, col = colors[1], xlab = \"Number of Monte Carlo draws\", ylab = \"Estimate\")\nabline(h = exp(1/2), lwd = 1, col = colors[3])\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Monte Carlo estimate\", \"True value\"),\n        lwd = c(2,1), cex = c(1,1), col = c(colors[1], colors[3]))"
  },
  {
    "objectID": "assignment/AssignmentPart1.html#problem-1---exponential-distribution-and-numerical-integration",
    "href": "assignment/AssignmentPart1.html#problem-1---exponential-distribution-and-numerical-integration",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "The exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) has (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization, the parameter \\(\\beta\\) is called a scale parameter, and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta.\\) If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density.\n[Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.]\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf, if you are curious. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1)\\) , \\(\\mathrm{Expon}(\\beta = 2)\\) and \\(\\mathrm{Expon}(\\beta = 3)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)?\n[Hint: the sort function might be handy for the empirical cdf, and don‚Äôt forget about the so called p-functions in R.]\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how:\n\na sample median is defined and\nhow a median of a statistical distribution is defined.\n\n[Hint: recall the so called q-functions in R].\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really fulfills the required property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n(Here we actually know the result, the expected value of \\(\\mathrm{Expon}(\\beta)\\) is \\(\\beta\\), but numerical integration technique can be used for the expectation of any function, for example if you are interested in \\(\\mathbb{E}(\\log (X))\\), when \\(X \\sim \\mathrm{Expon}(\\beta)\\)).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyCubicFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}\nmyCubicFunction(3)\n\n[1] 27"
  },
  {
    "objectID": "hidden/SolutionsPart1.html#problem-1---exponential-distribution-and-numerical-integration",
    "href": "hidden/SolutionsPart1.html#problem-1---exponential-distribution-and-numerical-integration",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "The exponential distribution, \\(X\\sim \\mathrm{Expon}(\\beta)\\) has (probability) density function\n\\[\nf(x) = \\frac{1}{\\beta}e^{-\\frac{x}{\\beta}}\\text{ for }x\\geq0\n\\]\nIn this parameterization, the parameter \\(\\beta\\) is called a scale parameter, and here \\(\\mathbb{E}(X)=\\beta\\). This is the parameterization used in the course book.\nR (and Wikipedia) instead uses the alternative parameterization with a rate parameter \\(\\lambda\\) and the density function\n\\[\nf(x)=\\lambda e^{-\\lambda x} \\text{ for }x\\geq0.\n\\]\nIn this parameterization \\(\\mathbb{E}(X) = \\frac{1}{\\lambda}\\). So, the connection between the two parameterization is that \\(\\lambda = \\frac{1}{\\beta}\\).\nWe will use the parameterization in the course book with the scale parameter \\(\\beta.\\) If you want to simulate 10 random numbers from the \\(X\\sim \\mathrm{Expon}(\\beta)\\) with \\(\\beta=2\\) you have to use the command rexp(n = 10, rate = 1/2), since \\(\\beta=2\\) implies \\(\\lambda = 1/2\\). The names of the arguments can be left out so rexp(10, 1/2) also works (but then you have to write the arguments in that exact order).\n\n\nA good way to check which parameterization is actually used in a given programming language is to simulate a large number of random numbers (also called draws) from the distribution and then compute the usual sample mean\n\\[\\bar{x}=\\frac{1}{n }\\sum_{i=1}^n x_i\\]\nof those random numbers. According to the law of large numbers, this sample mean should be close to the ‚Äútheoretical‚Äù/population mean of \\(\\mathbb{E}(X)\\) in the given parameterization.\nSimulate \\(n=10000\\) random numbers from the exponential distribution with rate \\(\\lambda = 2\\) to verify that R is indeed using the rate parameterization.\n\n\n\n\n# Solution\nlambda = 2\nx = rexp(n = 10000, rate = lambda)\nmean(x) # this is close to 1/lambda = 1/2, so it is indeed the rate parameterization.\n\n[1] 0.4957794\n\n\n\n\n\nSimulate 200 draws (random numbers) from the \\(X\\sim \\mathrm{Expon}(\\beta = 2)\\) distribution. Plot a histogram of the draws (use 30 histogram bins/cells) and overlay the theoretical probability density function (pdf) for the \\(\\mathrm{Expon}(\\beta = 2)\\) distribution as a curve. Note that you have to use the argument freq=FALSE in the hist function, otherwise the vertical scale will be counts within each bin in the histogram, and you want really want the height of the histogram bars to represent the density.\n[Hint: evaluate the dexp density function over a fine grid of \\(x\\)-values to plot the pdf.]\n\n\n\n\n# Solution\nn = 200\nbeta_ = 2\nx = rexp(n, rate = 1/beta_)\nhist(x, 30, freq = FALSE, col = colors[5])\nxGrid = seq(0, 10, length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/beta_), lwd = 3, col = colors[3])\n\n\n\n\n\n\n\nOverlay two more pdf curves: one for \\(\\mathrm{Expon}(\\beta = 1)\\) and the other for \\(\\mathrm{Expon}(\\beta = 3)\\). Use different colors. Which of the three pdf curves fit the data (histogram) best? Why do you think that is?\n\n\n\n\nset.seed(123)\nhist(x, 30, freq = FALSE, col = colors[5], ylim = c(0,1))\nxGrid = seq(0, max(x), length = 1000)\nlines(xGrid, dexp(xGrid, rate = 1/1), lwd = 3, col = colors[2])\nlines(xGrid, dexp(xGrid, rate = 1/2), lwd = 3, col = colors[3])\nlines(xGrid, dexp(xGrid, rate = 1/3), lwd = 3, col = colors[4])\nlegend(x = \"topright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(2,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\nWarning in strheight(legend, units = \"user\", cex = cex)/yc: longer object\nlength is not a multiple of shorter object length\n\n\nWarning in left + xchar + xextra + (w0 * rep.int(0:(ncol - 1),\nrep.int(n.legpercol, : longer object length is not a multiple of shorter object\nlength\n\n\nWarning in (rep.int(1L:n.legpercol, ncol)[1L:n.leg] - 1 + !is.null(title)) * :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in top - 0.5 * yextra - ymax - (rep.int(1L:n.legpercol, ncol)[1L:n.leg]\n- : longer object length is not a multiple of shorter object length\n\n\nWarning in xt[ok.l] + x.off * xchar: longer object length is not a multiple of\nshorter object length\n\n\nWarning in x1 + dx: longer object length is not a multiple of shorter object\nlength\n\n\nWarning in xt + (seg.len + x.off) * xchar: longer object length is not a\nmultiple of shorter object length\n\n\nWarning in xt + x.intersp * xchar: longer object length is not a multiple of\nshorter object length\n\n\n\n\n\nThe data was simulated from \\(\\mathrm{Exp}(\\beta=2)\\) so we expect that distribution to give the best fit, and it does so visually.\n\n\n\nThe empirical cumulative density function (cdf) from a sample with \\(n\\) observations is given by\n\\[\n\\hat{F}_n(x) = \\frac{\\text{number of elements in the sample }\\leq x}{n}\n\\]\nPlot the empirical cdf for the \\(n=200\\) observations that you simulated in Problem 1b); see https://en.wikipedia.org/wiki/Empirical_distribution_function for a little information about the empirical cdf, if you are curious. Overlay the cdf from the three distributions above: \\(\\mathrm{Expon}(\\beta = 1)\\) , \\(\\mathrm{Expon}(\\beta = 2)\\) and \\(\\mathrm{Expon}(\\beta = 3)\\). Which distribution seems to fit best? Does it match with your conclusion from Problem 1c)?\n[Hint: the sort function might be handy for the empirical cdf, and don‚Äôt forget about the so called p-functions in R.]\n\n\n\n\nx_sort = sort(x)\necdf = (1/n)*seq(1,n)\nplot(x_sort, ecdf, type = \"l\", lwd = 3, col = colors[1])\nlines(xGrid, pexp(xGrid, rate = 1/1), col = colors[2])\nlines(xGrid, pexp(xGrid, rate = 1/2), col = colors[3])\nlines(xGrid, pexp(xGrid, rate = 1/3), col = colors[4])\nlegend(x = \"bottomright\", inset=.05, legend = c(\"Data\", \"beta = 1\", \"beta = 2\", \"beta = 3\"),\n        lwd = c(3,2,2,2), cex = c(1,1,1), col = c(colors[1], colors[2], colors[3], colors[4]))\n\nWarning in strheight(legend, units = \"user\", cex = cex)/yc: longer object\nlength is not a multiple of shorter object length\n\n\nWarning in left + xchar + xextra + (w0 * rep.int(0:(ncol - 1),\nrep.int(n.legpercol, : longer object length is not a multiple of shorter object\nlength\n\n\nWarning in (rep.int(1L:n.legpercol, ncol)[1L:n.leg] - 1 + !is.null(title)) * :\nlonger object length is not a multiple of shorter object length\n\n\nWarning in top - 0.5 * yextra - ymax - (rep.int(1L:n.legpercol, ncol)[1L:n.leg]\n- : longer object length is not a multiple of shorter object length\n\n\nWarning in xt[ok.l] + x.off * xchar: longer object length is not a multiple of\nshorter object length\n\n\nWarning in x1 + dx: longer object length is not a multiple of shorter object\nlength\n\n\nWarning in xt + (seg.len + x.off) * xchar: longer object length is not a\nmultiple of shorter object length\n\n\nWarning in xt + x.intersp * xchar: longer object length is not a multiple of\nshorter object length\n\n\n\n\n\nThe best fit is again seen to be the \\(\\mathrm{Exp}(\\beta=2)\\) model. The other two distributions do not fit the empirical cdf well at all.\n\n\n\nCompare the sample median from the \\(n=200\\) observations to the theoretical medians for each of the above three distributions. Explain both how:\n\na sample median is defined and\nhow a median of a statistical distribution is defined.\n\n[Hint: recall the so called q-functions in R].\n\n\n\n\n# Solution:\nmessage(paste(\"the median of the sample is\", median(x)))\n\nthe median of the sample is 1.10958895180374\n\nmessage(paste(\"the median of the Expon(1) distribution is \", qexp(0.5, rate = 1/1)))\n\nthe median of the Expon(1) distribution is  0.693147180559945\n\nmessage(paste(\"the median of the Expon(2) distribution is \", qexp(0.5, rate = 1/2)))\n\nthe median of the Expon(2) distribution is  1.38629436111989\n\nmessage(paste(\"the median of the Expon(3) distribution is \", qexp(0.5, rate = 1/3)))\n\nthe median of the Expon(3) distribution is  2.07944154167984\n\n\nThe sample median is the midpoint of an ordered sample of observations. The median for a statistical distribution is the point which has 0.5 (50%) of the probability mass to the left of the point, i.e.¬†a point \\(x=x_{\\mathrm{median}}\\) such that \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})=0.5\\). For a discrete distribution we may not be able to find a point which has that property (since the cdf jumps at every observed value) and we then define the median as the smallest \\(x\\) which satisfies \\(\\mathrm{Pr}(X \\leq x_{\\mathrm{median}})\\geq 0.5\\).\n\n\n\nVerify by numerical integration that the \\(\\mathrm{Expon}(\\beta = 2)\\) density in R really fulfills the required property of any density \\(\\int_{-\\infty}^\\infty f(x)dx=1\\). This entails doing a rectangle sum approximation as in the definition of the integral in Lecture 2 (do not use a built-in function or a package for numerical integration). Start with a rectangle width of \\(\\Delta x = 0.5\\) and then lower it until the integral seems to have converged.\n\n\n\n\ndelta_x = 0.01 # This is the small rectangle width\nxStar = seq(0, 50, by = delta_x)\nfGrid = dexp(xStar, rate = 1/2) # This is the density evaluated at each xStar\nsum(fGrid*delta_x)\n\n[1] 1.002502\n\n\nIt seems to integrate to one. Note that the support of the exponential distribution is from \\(0\\) to \\(\\infty\\), so we don‚Äôt need to consider negative \\(x\\)-values.\n\n\n\nCompute the expected value of the exponential distribution with \\(\\beta=2\\) using numerical integration, i.e.¬†using similar technique as in Problem 1f). Verify your result from the rectangle sum by using R‚Äôs built in numerical integration routine integrate (see ?integrate for the documentation).\n(Here we actually know the result, the expected value of \\(\\mathrm{Expon}(\\beta)\\) is \\(\\beta\\), but numerical integration technique can be used for the expectation of any function, for example if you are interested in \\(\\mathbb{E}(\\log (X))\\), when \\(X \\sim \\mathrm{Expon}(\\beta)\\)).\n[Hint1: note that integrate requires the function f(x) to be integrated as input argument, so you have to define such a function before calling the integrate function. To remind you of how functions are written in R, a toy function in R is given below.]\n[Hint2: don‚Äôt forget that I asking you to compute the expected value, not just to integrate the density function]\n\n# Just a toy function to show how functions are implemented in R.\nmyCubicFunction &lt;- function(x){\n  y = x^3\n  return(y)\n}\nmyCubicFunction(3)\n\n[1] 27\n\n\n\n\n\nHere is the rectangle sum approximation\n\nsum(xStar*fGrid*delta_x)\n\n[1] 1.999996\n\n\nand here is the result using integrate\n\nf &lt;- function(x){\n  return(x*dexp(x, rate = 1/2)) # Note that the function we want to integrate here is really x times the density f(x)\n}\nintegrate(f, 0, Inf)\n\n2 with absolute error &lt; 7.7e-06"
  },
  {
    "objectID": "assignment/AssignmentPart2.html",
    "href": "assignment/AssignmentPart2.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Note\n\n\n\nThis is the second part of the home assignment for the course. The first part had Problems 1-3, so I will continue with this numbering, and the first problem here will therefore be Problem 4."
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-4---regression-models-for-count-data",
    "href": "assignment/AssignmentPart2.html#problem-4---regression-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - Regression models for count data",
    "text": "Problem 4 - Regression models for count data\n\nProblem 4a)\nThe file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. Load the data like this:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations\nX = data[,-1]  # 91 x 5 matrix with covariates\n\nIn this assignment you will model the number of bugs (y) as a function of the following covariates in the matrix X:\n\nintercept - this is columns of ones to model the intercept\nnCommit - the number of commits since the last release of the software\npropC - proportion of C/C++ code in the project\npropJava - proportion of Java code in the project\ncomplexity - a measure of code complexity that takes into account the frequency of if statements etc.\n\nYou can ignore that some of the observations actually comes from the same project at different releases, and assume that the observations are independent. In the first part of the assignment you also assumed that the observations were identically distributed from a Poisson distribution with the same parameter for all observations:\n\\[Y_i \\overset{iid}{\\sim}\\mathrm{Poisson}(\\mu)\\]\nThe Poisson parameter is here called \\(\\mu\\) instead of the more common \\(\\lambda\\).\nIn this part of the assignment, we will relax the assumption of a common distribution and instead use regression models that model the conditional distribution of \\(y_i\\) given the covariate values \\(\\textbf{x}_i\\) for the \\(i\\)th project/release. Since the response data are still counts, let us consider the Poisson regression model\n\\[Y_i \\vert \\boldsymbol{x}_i \\overset{ind}{\\sim}\\mathrm{Poisson}(\\mu(\\boldsymbol{x}_i))\\]\nwhere each data point has its own Poisson parameter \\(\\mu_i\\). The Poisson parameter is here called \\(\\mu\\) instead of the more common \\(\\lambda\\). Note that the model says that the data are \\(\\overset{ind}{\\sim}\\), meaning that they are independent, but not necessarily identically distributed. Since the model is Poisson conditional on the covariates we have the conditional mean\n\\[\n\\mu(\\boldsymbol{x}_i) = \\mathbb{E}(Y_i\\vert \\boldsymbol{x}_i)\n\\]\nWe need to make sure that this mean is always positive, since that is a requirement of the Poisson distribution. We will here model the mean as an exponential function of the covariates\n\\[\n\\mu(\\boldsymbol{x}_i) = \\exp\\big( \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\n\\]\nwhere \\(\\boldsymbol{\\beta} = (\\beta_0,\\beta_1,\\ldots,\\beta_p)^\\top\\) is a vector with regression coefficients (including the intercept \\(\\beta_0\\)- This is nice: the covariates and regression coefficients in are free to take on any real values while the mean is guaranteed to be postive since the output from the exponential function is always positive.\nLet‚Äôs summarize the Poisson regression model\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{ind}{\\sim}\\mathrm{Poisson} \\Big( \\exp\\big( \\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big) \\Big)\n\\]"
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-3---transforming-random-variables",
    "href": "assignment/AssignmentPart2.html#problem-3---transforming-random-variables",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 3 - Transforming random variables",
    "text": "Problem 3 - Transforming random variables\n\nProblem 3a)\n\nThis problem is to be done after Lecture 6.\n\nLet \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). We are now interested in the distribution of \\(Y=\\exp(X)\\). Obtain the distribution for \\(Y\\) by simulating 10000 draws. Plot a histogram with 100 bins.\n\n\nProblem 3b)\nUse the method of transformation (Section 6.4 in the course book) to show that the probability density for \\(Y\\) is given by\n\\[\nf(y)=\\frac{1}{\\sqrt{2\\pi}y}\\exp\\Big(-\\frac{1}{2}(\\log(y))^2\\Big)\n\\]\nOverlay a plot of this density in the histogram from Problem 3a).\n[hint: you can use LaTeX to write math in Quarto file (Google it), but it is also OK to just do the math on paper, take a photo and include the photo]\n\n\nProblem 3c)\nUse (Monte Carlo) simulation with \\(m=10000\\) random draws to estimate \\(\\mathrm{E}(Y)\\), where, as before, \\(Y=\\exp(X)\\) and \\(X \\sim \\mathrm{Normal}(\\mu = 0, \\sigma^2 = 1)\\). Check the convergence of the estimate by plotting the sequential Monte Carlo estimates for increasing Monte Carlo sample sizes of \\(10,20,30,\\ldots,9900, 10000\\). Does the estimate seem to converge (settle down) to the true expectation, which happens to be \\(\\mathrm{E}(Y)=\\exp(\\frac{1}{2})\\)? [How do I know that this is the true expected value? See this: https://en.wikipedia.org/wiki/Log-normal_distribution]"
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html",
    "href": "hidden/AssignmentPart2Solutions.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Note\n\n\n\nThis is the second part of the home assignment for the course. The first part had Problems 1-3, so I will continue with this numbering, and the first problem here will therefore be Problem 4."
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html#problem-4---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "href": "hidden/AssignmentPart2Solutions.html#problem-4---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - Numerical maximum likelihood for negative binomial distribution",
    "text": "Problem 4 - Numerical maximum likelihood for negative binomial distribution\n\nProblem 4a)\nWe continue with the bugs data from the first part of the Assignment. The file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. We load the data:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations\nX = data[,-1]  # 91 x 5 matrix with covariates\nX = as.matrix(X) # X was initial a data frame, but we want it to be matrix\n\nIn this problem you will learn to find the maximum likelihood estimate numerically for models with a single parameter. We will only use the response variable \\(y\\), the number of bugs. Later in this assignment we will also use the covariates/features in matrix \\(\\mathbf{X}\\).\nWe use the negative binomial again, but this time we try to estimate the parameter \\(r\\) given that the mean parameter \\(\\mu\\) is fixed at the estimate \\(\\hat\\mu=\\bar y \\approx 5.2528\\). So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu=5.2528)  \n\\]\nand your task is to find the maximum likelihood estimate \\(\\hat r\\) of the parameter \\(r&gt;0\\) using numerical maximization in R. Set the argument lower to some small, but positive value, for example lower = 0.0001. lower = 0 will not work since the negative binomial distribution with \\(r=0\\) is not defined.\n\n\nSolution 4a)\nWe set up the log-likelihood function:\n\n# log-likelihood function for r, given a fixed value for mu\nlogLikNegBinOnly_r &lt;- function(r, mu, y){\n  logL = sum(dnbinom(y, size = r, mu = mu, log = TRUE))\n  return(logL)\n}\n\nAnd then run optim to get the maximum likelihood estimate for \\(r\\):\n\ninitVal = c(3) # This is the first guess in the algorithm\nmu = mean(y)\noptRes &lt;- optim(initVal, logLikNegBinOnly_r, gr = NULL, mu, y,\n                method = c(\"L-BFGS-B\"), \n                control = list(fnscale = -1), \n                hessian = TRUE,\n                lower = 0.0001) \nmessage(paste(\"The ML estimate of r is \"), optRes$par)\n\nThe ML estimate of r is 1.47374530718279\n\n\nThe estimate for \\(r\\) is rather small suggesting that the data is far from Poisson (since the Poisson is the special case of the negative binomial as \\(r \\rightarrow \\infty\\).\n\n\nProblem 4b)\nWhat is the standard error of the ML estimate \\(\\hat{r}\\) in Problem 4a)? Recall that the standard error is the standard deviation in the sampling distribution of an estimator. An approximate answer is enough, and you can obtain it numerically (no maths needed!).\n\n\nSolution 4b)\n\nJ = -optRes$hessian # this is the observed information\n\n# The standard error (= standard deviation of the sampling distribution) of the \n# ML estimator is approximately:\nstdML = sqrt(1/J) # 1/J is the approximate variance, so sqrt of that gives the approximate standard deviation of the Maximum likelihood estimator.\nmessage(paste(\"The standard error of the ML estimator for r is \"), stdML)\n\nThe standard error of the ML estimator for r is 0.287474622696235\n\n\n\n\nProblem 4c)\nLet us now try to estimate both parameters \\(\\mu\\) and \\(r\\) at the same time (jointly, as we say) using the maximum likelihood method. Use numerical maximization again and set lower = c(0.0001, 0.0001) to make sure that the estimated parameters are positive, as they should be in the negative binomial distribution.\n\n\nSolution 4c)\nWe set up the log-likelihood function. Note that the first argument must be a vector containing both parameters. The parameters r and mu are then unpacked inside the function.\n\n# log-likelihood function for both r and mu. \nlogLikNegBin &lt;- function(param, y){\n  r = param[1]\n  mu = param[2]\n  logL = sum(dnbinom(y, size = r, mu = mu, log = TRUE))\n  return(logL)\n}\n\nNow we use optim to find the ML estimates of both parameters:\n\ninitVal = c(3, 2) # This is the first guess in the algorithm. \noptRes &lt;- optim(initVal, logLikNegBin, gr = NULL, y,\n                method = c(\"L-BFGS-B\"), \n                control = list(fnscale = -1), \n                hessian = TRUE,\n                lower = c(0.0001, 0.0001)) \nmessage(paste(\"The ML estimate of r is \"), optRes$par[1])  \n\nThe ML estimate of r is 1.47374649903116\n\nmessage(paste(\"The ML estimate of mu is \"), optRes$par[2])  \n\nThe ML estimate of mu is 5.25275048301485\n\n\nThe ML estimate of \\(\\mu\\) turns out to be the sample mean (i.e.¬†same value as we used above) and the estimate for \\(r\\) remains the same when we estimate both parameters jointly. This is not true in any model, but happens here since there is no correlation between \\(\\hat r\\) and \\(\\hat\\mu\\) in the model (this can be seen as the off-diagonal element of covML below is essentially zero).\n\n\nProblem 4d)\nCompute the standard errors for \\(\\hat r\\) and for \\(\\hat\\mu\\).\n\n\nSolution 4d)\n\nJ = -optRes$hessian # this is the observed information\ncovML = solve(J)    # this is the covariance matrix of the MLE for (r,mu)\nvarML = diag(covML) # sampling distribution variances\nstdML = sqrt(varML) # standard errors\nmessage(paste(\"The standard error of the ML estimator of r is \", stdML[1]))\n\nThe standard error of the ML estimator of r is  0.287474987487543\n\nmessage(paste(\"The standard error of the ML estimator of mu is \", stdML[2]))\n\nThe standard error of the ML estimator of mu is  0.513281927138858"
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html#problem-5---regression-models-for-count-data",
    "href": "hidden/AssignmentPart2Solutions.html#problem-5---regression-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 5 - Regression models for count data",
    "text": "Problem 5 - Regression models for count data\nOk, now we make this more interesting by modeling the expected number of bugs as function of covariates/features in the X matrix loaded above:\n\nintercept - this is columns of ones to model the intercept\nnCommit - the number of commits since the last release of the software\npropC - proportion of C/C++ code in the project\npropJava - proportion of Java code in the project\ncomplexity - a measure of code complexity that takes into account the frequency of if statements etc.\n\n\nProblem 5a)\nUse numerical optimization to fit the Poisson regression model\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Poisson}\\Big(\\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nNote how each observation has its ‚Äúown‚Äù \\(\\lambda_i\\) parameter, which is modeled as the exponential of \\(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\). We use the exponential function to make sure that \\(\\lambda_i\\) is always positive, as it has to be in the Poisson model. Note that a Poisson regression is a model for the conditional expectation\n\\[\n\\mathbb{E}(Y_i \\vert \\boldsymbol{x}_i) = \\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big) = \\exp\\big( \\beta_0 + \\beta_1 x_{1,i} + \\ldots + \\beta_p x_{p,i} \\big).\n\\]\nThe covariates/features for the \\(i\\)th observation \\(\\boldsymbol{x}_i=(x_{1,i},\\ldots,x_{p,i})^\\top\\) is given the \\(i\\)th row of the matrix X . For example, for the second observation we have y[2] = 6, so six bugs in the second release, and the covariate values for this second release are:\n\nX[2,]\n\n intercept   nCommits      propC   propJava complexity \n 1.0000000  4.0000000  0.4150649  0.3295860  0.7031914 \n\n\nThat is, this release (observation) has 4 commits, approximately 41.4% C code, 32.9 % Java code and a Code Complexity of 0.7. The regression coefficients in \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) measures how the mean of the Poisson distribution \\(\\lambda_i\\) changes when the features change. For example, \\(\\beta_4\\) determines how sensitive the mean number of a bugs in a release is to the code complexity. However, since \\(\\lambda_i\\) is not a linear function of the features (as a result of the exponential function), the \\(\\beta\\) coefficients cannot be interpreted as in ordinary linear regression; see the Lecture on Poisson regression.\nOk, I can sense that you are eager to fit this model, so let‚Äôs do it! Use the bugs dataset to compute the maximum likelihood estimate of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) with numerical optimization. Note that since we have used the exponential function in the model, the elements of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) are free to take on any value, also negative values. So you can use method = c(\"BFGS\") and there is no need to use the lower argument in optim.\n\n\nSolution 5a)\n\n# log-likelihood function\nlogLikPoisReg &lt;- function(betaVect, y, X){\n  lambda = exp(X %*% betaVect) # Note the matrix multiplication and that lambda is here a 91-element vector \n  logL = sum(dpois(y, lambda = lambda, log = TRUE))\n  return(logL)\n}\n\n\ninitVal = rep(0,5) # This is the first guess in the algorithm. just a zero vector\noptRes &lt;- optim(initVal, logLikPoisReg, gr = NULL, y, X, \n                method = c(\"BFGS\"), \n                control = list(fnscale = -1), \n                hessian = TRUE) \nbetaHat = optRes$par\nbetaHat\n\n[1] -1.117837428 -0.009543965  1.456838249  3.200732352  2.145565785\n\n\n\n\nProblem 5b)\nUse the optimization output from the previous problem to get approximate standard errors for each estimated \\(\\beta\\) coefficient. Compute 95% confidence intervals for each \\(\\beta\\) parameter. Which covariates seem to be statistically different from zero? [Hint: ues the large-sample normal approximation of the sampling distribution, and use the confidence interval to check the statistical significance.]\n\n\nSolution 5b)\nThe standard errors are:\n\nJ = -optRes$hessian # this is the observed information\n\ncovML = solve(J)    # this is the covariance matrix of the MLE for (r,mu)\nvarML = diag(covML) # sampling distribution variances\nstdML = sqrt(varML) # standard errors\nround(stdML, digits = 3)\n\n[1] 0.423 0.014 0.574 0.666 0.255\n\n\nApproximate 95% confidence intervals can be computed from the normal approximation. Each row below gives the lower and upper limit of the confidence interval for a \\(\\beta\\) parameter. The point \\(\\beta=0\\) is only included in the interval for the covariate number of commits. All other covariates have regression coefficients which do no include the value zero and are therefore significantly different from zero at a 5% (1-0.95) signficance level. The estimates are positive, so the proportion of C and Java code and the Code Complexity increases the expected number of bugs.\n\ncbind(betaHat - 1.96*stdML, betaHat + 1.96*stdML)\n\n            [,1]        [,2]\n[1,] -1.94627601 -0.28939885\n[2,] -0.03632146  0.01723353\n[3,]  0.33246127  2.58121523\n[4,]  1.89566466  4.50580004\n[5,]  1.64566337  2.64546820\n\n\n\n\nProblem 5c)\nThe coming release, which we here number as observation number 92, has the following covariate vector:\n\nx92 = c(1, 10, 0.45, 0.5, 0.89)\n\nSo, the release is based on 10 commits, good proportions of C and Java code and a high code complexity of 0.89. What is the predicted number of bugs for this release?\n\n\nSolution 5c)\nWe naturally base our prediction on the maximum likelihood estimate. The expected number of bugs for the release is\n\\[\n\\mathbb{E}(Y \\vert \\boldsymbol{x}_{92})= \\lambda_{92} = \\exp(\\boldsymbol{x}_{92}^\\top \\hat{\\boldsymbol{\\beta}})\n\\]\nwhich is computed as\n\nbetaHat = optRes$par  # this is the ML estimate from before\nlambda92 = exp(x92 %*% betaHat)\nmessage(paste(\"The expected number of bugs for the next release is\", lambda92))\n\nThe expected number of bugs for the next release is 19.1484133158791\n\n\nWow, more than 19 bugs expected! Maybe better check that code one more time before the release!"
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html#problem-4---a-little-linear-algebra",
    "href": "hidden/AssignmentPart2Solutions.html#problem-4---a-little-linear-algebra",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - A little linear algebra",
    "text": "Problem 4 - A little linear algebra\n\nProblem 4a)\nLet \\(\\boldsymbol{a}=(1,0,2)^\\top\\) and \\(\\boldsymbol{b}=(1,0,-1)^\\top\\) be two vectors. The dot product is computes in R as\n\na = c(1,0,2)\nb = c(1,0,-1)\na%*%b\n\n     [,1]\n[1,]   -1\n\n\nNote that R returns the dot product as a matrix (with one row and one column) even though it is a scalar (a number). If you really want a number you can do return the first (and only) element like this:\n\n(a%*%b)[1]\n\n[1] -1\n\n\nAre \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) orthogonal?\n\n\nSolution 4a)\n\na = c(1,0,2)\nb = c(1,0,-1)\na%*%b\n\n     [,1]\n[1,]   -1\n\n\nThe dot product is not zero, so the vectors are not orthogonal.\n\n\nProblem 4b)\nSimulate a \\(10\\times 3\\) matrix \\(\\boldsymbol{X}\\) with standard normal N(0,1) random variables. Let \\(\\beta = (1,1,2)^\\top\\) be a vector. Compute the matrix-vector product \\(\\boldsymbol{\\mu} = \\boldsymbol{X}\\boldsymbol{\\beta}\\). When you code this, use the variable name mu for \\(\\boldsymbol{\\mu}\\) and b for \\(\\boldsymbol{\\beta}\\). Explain how the first element of \\(\\boldsymbol{\\mu}\\) relates to the elements in \\(\\boldsymbol{X}\\).\n\n\nSolution 4b)\n\nX = matrix(rnorm(10*3), 10, 3)\nb = c(1,1,2)\nmu = X %*% b\nmu\n\n             [,1]\n [1,]  0.05331692\n [2,]  0.87585616\n [3,]  4.43110848\n [4,] -2.54733433\n [5,]  2.18037639\n [6,]  2.85569899\n [7,]  0.74615168\n [8,]  1.85173230\n [9,] -1.11905100\n[10,] -1.14195613\n\n\nThe first element of mu is the dot product of the first row of X and b.\n\n\nProblem 4c)\nNow simulate a vector of errors \\(\\boldsymbol{\\varepsilon}\\) (use the variable name epsilon) from a normal distribution with mean zero and standard deviation \\(\\sigma = 0.1\\). Compute the vector of response observations \\(\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Compute the least squares estimate \\(\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\) based on the simulated \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\).\n\n\nSolution 4c)\n\nepsilon = rnorm(10, 0, 0.1)\ny = X %*% b + epsilon\nbetaHat = solve(t(X) %*% X) %*% t(X) %*% y\nbetaHat\n\n          [,1]\n[1,] 1.0194386\n[2,] 0.9934414\n[3,] 2.0171118\n\n\n\n\nProblem 4d)\nThe variance of the errors in \\(\\boldsymbol{\\varepsilon}\\) can be estimated from the vector of residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\) as follows\n\\[\ns^2 = \\frac{\\boldsymbol{e}^\\top \\boldsymbol{e}}{n-p}\n\\]\nThis is the same residual variance formula as you used in the previous statistics course, where it was written using sums instead of vectors\n\\[\ns^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-p}\n\\]\nThe covariance matrix of the least squares \\(\\hat\\beta\\) estimator can be estimated by the matrix formula\n\\[\ns^2(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\n\\]\nand the standard errors for each \\(\\hat\\beta\\) is therefore the square root of the diagonal elements of this covariance matrix. Compute the standard errors of the regression coefficients based on the 10 observations that you simulated in Problems 4c).\n\n\nSolution 4d)\n\nn = 10 \np = 3\ne = y - X %*% b\ns2 = (t(e) %*% e)[1]/(n-p)\ncovLS = s2*solve(t(X) %*% X)\nsqrt(diag(covLS))\n\n[1] 0.02486669 0.02681840 0.03318727"
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "href": "hidden/AssignmentPart2Solutions.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 5 - Numerical maximum likelihood for negative binomial distribution",
    "text": "Problem 5 - Numerical maximum likelihood for negative binomial distribution\n\nProblem 5a)\nWe continue with the bugs data from the first part of the Assignment. The file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. We load the data:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations\nX = data[,-1]  # 91 x 5 matrix with covariates\nX = as.matrix(X) # X was initial a data frame, but we want it to be matrix\n\nIn this problem you will learn to find the maximum likelihood estimate numerically for models with a single parameter. We will only use the response variable \\(y\\), the number of bugs. Later in this assignment we will also use the covariates/features in matrix \\(\\mathbf{X}\\).\nWe use the negative binomial again, but this time we try to estimate the parameter \\(r\\) given that the mean parameter \\(\\mu\\) is fixed at the estimate \\(\\hat\\mu=\\bar y \\approx 5.2528\\). So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu=5.2528)  \n\\]\nand your task is to find the maximum likelihood estimate \\(\\hat r\\) of the parameter \\(r&gt;0\\) using numerical maximization in R. Set the argument lower to some small, but positive value, for example lower = 0.0001. lower = 0 will not work since the negative binomial distribution with \\(r=0\\) is not defined.\n\n\nSolution 5a)\nWe set up the log-likelihood function:\n\n# log-likelihood function for r, given a fixed value for mu\nlogLikNegBinOnly_r &lt;- function(r, mu, y){\n  logL = sum(dnbinom(y, size = r, mu = mu, log = TRUE))\n  return(logL)\n}\n\nAnd then run optim to get the maximum likelihood estimate for \\(r\\):\n\ninitVal = c(3) # This is the first guess in the algorithm\nmu = mean(y)\noptRes &lt;- optim(initVal, logLikNegBinOnly_r, gr = NULL, mu, y,\n                method = c(\"L-BFGS-B\"), \n                control = list(fnscale = -1), \n                hessian = TRUE,\n                lower = 0.0001) \nmessage(paste(\"The ML estimate of r is \"), optRes$par)\n\nThe ML estimate of r is 1.47374530718279\n\n\nThe estimate for \\(r\\) is rather small suggesting that the data is far from Poisson (since the Poisson is the special case of the negative binomial as \\(r \\rightarrow \\infty\\).\n\n\nProblem 5b)\nWhat is the standard error of the ML estimate \\(\\hat{r}\\) in Problem 5a)? Recall that the standard error is the standard deviation in the sampling distribution of an estimator. An approximate answer is enough, and you can obtain it numerically (no maths needed!).\n\n\nSolution 5b)\n\nJ = -optRes$hessian # this is the observed information\n\n# The standard error (= standard deviation of the sampling distribution) of the \n# ML estimator is approximately:\nstdML = sqrt(1/J) # 1/J is the approximate variance, so sqrt of that gives the approximate standard deviation of the Maximum likelihood estimator.\nmessage(paste(\"The standard error of the ML estimator for r is \"), stdML)\n\nThe standard error of the ML estimator for r is 0.287474622696235\n\n\n\n\nProblem 5c)\nLet us now try to estimate both parameters \\(\\mu\\) and \\(r\\) at the same time (jointly, as we say) using the maximum likelihood method. Use numerical maximization again and set lower = c(0.0001, 0.0001) to make sure that the estimated parameters are positive, as they should be in the negative binomial distribution.\n\n\nSolution 5c)\nWe set up the log-likelihood function. Note that the first argument must be a vector containing both parameters. The parameters r and mu are then unpacked inside the function.\n\n# log-likelihood function for both r and mu. \nlogLikNegBin &lt;- function(param, y){\n  r = param[1]\n  mu = param[2]\n  logL = sum(dnbinom(y, size = r, mu = mu, log = TRUE))\n  return(logL)\n}\n\nNow we use optim to find the ML estimates of both parameters:\n\ninitVal = c(3, 2) # This is the first guess in the algorithm. \noptRes &lt;- optim(initVal, logLikNegBin, gr = NULL, y,\n                method = c(\"L-BFGS-B\"), \n                control = list(fnscale = -1), \n                hessian = TRUE,\n                lower = c(0.0001, 0.0001)) \nmessage(paste(\"The ML estimate of r is \"), optRes$par[1])  \n\nThe ML estimate of r is 1.47374649903116\n\nmessage(paste(\"The ML estimate of mu is \"), optRes$par[2])  \n\nThe ML estimate of mu is 5.25275048301485\n\n\nThe ML estimate of \\(\\mu\\) turns out to be the sample mean (i.e.¬†same value as we used above) and the estimate for \\(r\\) remains the same when we estimate both parameters jointly. This is not true in any model, but happens here since there is no correlation between \\(\\hat r\\) and \\(\\hat\\mu\\) in the model (this can be seen as the off-diagonal element of covML below is essentially zero).\n\n\nProblem 5d)\nCompute the standard errors for \\(\\hat r\\) and for \\(\\hat\\mu\\).\n\n\nSolution 5d)\n\nJ = -optRes$hessian # this is the observed information\ncovML = solve(J)    # this is the covariance matrix of the MLE for (r,mu)\nvarML = diag(covML) # sampling distribution variances\nstdML = sqrt(varML) # standard errors\nmessage(paste(\"The standard error of the ML estimator of r is \", stdML[1]))\n\nThe standard error of the ML estimator of r is  0.287474987487543\n\nmessage(paste(\"The standard error of the ML estimator of mu is \", stdML[2]))\n\nThe standard error of the ML estimator of mu is  0.513281927138858"
  },
  {
    "objectID": "hidden/AssignmentPart2Solutions.html#problem-6---regression-models-for-count-data",
    "href": "hidden/AssignmentPart2Solutions.html#problem-6---regression-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 6 - Regression models for count data",
    "text": "Problem 6 - Regression models for count data\nOk, now we make this more interesting by modeling the expected number of bugs as function of covariates/features in the X matrix loaded above:\n\nintercept - this is columns of ones to model the intercept\nnCommit - the number of commits since the last release of the software\npropC - proportion of C/C++ code in the project\npropJava - proportion of Java code in the project\ncomplexity - a measure of code complexity that takes into account the frequency of if statements etc.\n\n\nProblem 6a)\nUse numerical optimization to fit the Poisson regression model\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Poisson}\\Big(\\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nNote how each observation has its ‚Äúown‚Äù \\(\\lambda_i\\) parameter, which is modeled as the exponential of \\(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\). We use the exponential function to make sure that \\(\\lambda_i\\) is always positive, as it has to be in the Poisson model. Note that a Poisson regression is a model for the conditional expectation\n\\[\n\\mathbb{E}(Y_i \\vert \\boldsymbol{x}_i) = \\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big) = \\exp\\big( \\beta_0 + \\beta_1 x_{1,i} + \\ldots + \\beta_p x_{p,i} \\big).\n\\]\nThe covariates/features for the \\(i\\)th observation \\(\\boldsymbol{x}_i=(x_{1,i},\\ldots,x_{p,i})^\\top\\) is given the \\(i\\)th row of the matrix X . For example, for the second observation we have y[2] = 6, so six bugs in the second release, and the covariate values for this second release are:\n\nX[2,]\n\n intercept   nCommits      propC   propJava complexity \n 1.0000000  4.0000000  0.4150649  0.3295860  0.7031914 \n\n\nThat is, this release (observation) has 4 commits, approximately 41.4% C code, 32.9 % Java code and a Code Complexity of 0.7. The regression coefficients in \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) measures how the mean of the Poisson distribution \\(\\lambda_i\\) changes when the features change. For example, \\(\\beta_4\\) determines how sensitive the mean number of a bugs in a release is to the code complexity. However, since \\(\\lambda_i\\) is not a linear function of the features (as a result of the exponential function), the \\(\\beta\\) coefficients cannot be interpreted as in ordinary linear regression; see the Lecture on Poisson regression.\nOk, I can sense that you are eager to fit this model, so let‚Äôs do it! Use the bugs dataset to compute the maximum likelihood estimate of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) with numerical optimization. Note that since we have used the exponential function in the model, the elements of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) are free to take on any value, also negative values. So you can use method = c(\"BFGS\") and there is no need to use the lower argument in optim.\n\n\nSolution 6a)\n\n# log-likelihood function\nlogLikPoisReg &lt;- function(betaVect, y, X){\n  lambda = exp(X %*% betaVect) # Note the matrix multiplication and that lambda is here a 91-element vector \n  logL = sum(dpois(y, lambda = lambda, log = TRUE))\n  return(logL)\n}\n\n\ninitVal = rep(0,5) # This is the first guess in the algorithm. just a zero vector\noptRes &lt;- optim(initVal, logLikPoisReg, gr = NULL, y, X, \n                method = c(\"BFGS\"), \n                control = list(fnscale = -1), \n                hessian = TRUE) \nbetaHat = optRes$par\nbetaHat\n\n[1] -1.117837428 -0.009543965  1.456838249  3.200732352  2.145565785\n\n\n\n\nProblem 6b)\nUse the optimization output from the previous problem to get approximate standard errors for each estimated \\(\\beta\\) coefficient. Compute 95% confidence intervals for each \\(\\beta\\) parameter. Which covariates seem to be statistically different from zero? [Hint: ues the large-sample normal approximation of the sampling distribution, and use the confidence interval to check the statistical significance.]\n\n\nSolution 6b)\nThe standard errors are:\n\nJ = -optRes$hessian # this is the observed information\n\ncovML = solve(J)    # this is the covariance matrix of the MLE for (r,mu)\nvarML = diag(covML) # sampling distribution variances\nstdML = sqrt(varML) # standard errors\nround(stdML, digits = 3)\n\n[1] 0.423 0.014 0.574 0.666 0.255\n\n\nApproximate 95% confidence intervals can be computed from the normal approximation. Each row below gives the lower and upper limit of the confidence interval for a \\(\\beta\\) parameter. The point \\(\\beta=0\\) is only included in the interval for the covariate number of commits. All other covariates have regression coefficients which do no include the value zero and are therefore significantly different from zero at a 5% (1-0.95) signficance level. The estimates are positive, so the proportion of C and Java code and the Code Complexity increases the expected number of bugs.\n\ncbind(betaHat - 1.96*stdML, betaHat + 1.96*stdML)\n\n            [,1]        [,2]\n[1,] -1.94627601 -0.28939885\n[2,] -0.03632146  0.01723353\n[3,]  0.33246127  2.58121523\n[4,]  1.89566466  4.50580004\n[5,]  1.64566337  2.64546820\n\n\n\n\nProblem 6c)\nThe coming release, which we here number as observation number 92, has the following covariate vector:\n\nx92 = c(1, 10, 0.45, 0.5, 0.89)\n\nSo, the release is based on 10 commits, good proportions of C and Java code and a high code complexity of 0.89. What is the predicted number of bugs for this release?\n\n\nSolution 6c)\nWe naturally base our prediction on the maximum likelihood estimate. The expected number of bugs for the release is\n\\[\n\\mathbb{E}(Y \\vert \\boldsymbol{x}_{92})= \\lambda_{92} = \\exp(\\boldsymbol{x}_{92}^\\top \\hat{\\boldsymbol{\\beta}})\n\\]\nwhich is computed as\n\nbetaHat = optRes$par  # this is the ML estimate from before\nlambda92 = exp(x92 %*% betaHat)\nmessage(paste(\"The expected number of bugs for the next release is\", lambda92))\n\nThe expected number of bugs for the next release is 19.1484133158791\n\n\nWow, more than 19 bugs expected! Maybe better check that code one more time before the release!"
  },
  {
    "objectID": "hidden/AssignmentPart2.html",
    "href": "hidden/AssignmentPart2.html",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "",
    "text": "Note\n\n\n\nThis is the second part of the home assignment for the course. The first part had Problems 1-3, so I will continue with this numbering, and the first problem here will therefore be Problem 4."
  },
  {
    "objectID": "hidden/AssignmentPart2.html#problem-4---a-little-linear-algebra",
    "href": "hidden/AssignmentPart2.html#problem-4---a-little-linear-algebra",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - A little linear algebra",
    "text": "Problem 4 - A little linear algebra\n\nProblem 4a)\nLet \\(\\boldsymbol{a}=(1,0,2)^\\top\\) and \\(\\boldsymbol{b}=(1,0,-1)^\\top\\) be two vectors. The dot product is computes in R as\n\na = c(1,0,2)\nb = c(1,0,-1)\na%*%b\n\n     [,1]\n[1,]   -1\n\n\nNote that R returns the dot product as a matrix (with one row and one column) even though it is a scalar (a number). If you really want a number you can do return the first (and only) element like this:\n\n(a%*%b)[1]\n\n[1] -1\n\n\nAre \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) orthogonal?\n\n\nProblem 4b)\nSimulate a \\(10\\times 3\\) matrix \\(\\boldsymbol{X}\\) with standard normal N(0,1) random variables. Let \\(\\beta = (1,1,2)^\\top\\) be a vector. Compute the matrix-vector product \\(\\boldsymbol{\\mu} = \\boldsymbol{X}\\boldsymbol{\\beta}\\). When you code this, use the variable name mu for \\(\\boldsymbol{\\mu}\\) and b for \\(\\boldsymbol{\\beta}\\). Explain how the first element of \\(\\boldsymbol{\\mu}\\) relates to the elements in \\(\\boldsymbol{X}\\).\n\n\nProblem 4c)\nNow simulate a vector of errors \\(\\boldsymbol{\\varepsilon}\\) (use the variable name epsilon) from a normal distribution with mean zero and standard deviation \\(\\sigma = 0.1\\). Compute the vector of response observations \\(\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Compute the least squares estimate \\(\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\) based on the simulated \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\).\n\n\nProblem 4d)\nThe variance of the errors in \\(\\boldsymbol{\\varepsilon}\\) can be estimated from the vector of residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\) as follows\n\\[\ns^2 = \\frac{\\boldsymbol{e}^\\top \\boldsymbol{e}}{n-p}\n\\]\nThis is the same residual variance formula as you used in the previous statistics course, where it was written using sums instead of vectors\n\\[\ns^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-p}\n\\]\nThe covariance matrix of the least squares \\(\\hat\\beta\\) estimator can be estimated by the matrix formula\n\\[\ns^2(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\n\\]\nand the standard errors for each \\(\\hat\\beta\\) is therefore the square root of the diagonal elements of this covariance matrix. Compute the standard errors of the regression coefficients based on the 10 observations that you simulated in Problems 4c)."
  },
  {
    "objectID": "hidden/AssignmentPart2.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "href": "hidden/AssignmentPart2.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 5 - Numerical maximum likelihood for negative binomial distribution",
    "text": "Problem 5 - Numerical maximum likelihood for negative binomial distribution\n\nProblem 5a)\nWe continue with the bugs data from the first part of the Assignment. The file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. We load the data:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations\nX = data[,-1]  # 91 x 5 matrix with covariates\nX = as.matrix(X) # X was initial a data frame, but we want it to be matrix\n\nIn this problem you will learn to find the maximum likelihood estimate numerically for models with a single parameter. We will only use the response variable \\(y\\), the number of bugs. Later in this assignment we will also use the covariates/features in matrix \\(\\mathbf{X}\\).\nWe use the negative binomial again, but this time we try to estimate the parameter \\(r\\) given that the mean parameter \\(\\mu\\) is fixed at the estimate \\(\\hat\\mu=\\bar y \\approx 5.2528\\). So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu=5.2528)  \n\\]\nand your task is to find the maximum likelihood estimate \\(\\hat r\\) of the parameter \\(r&gt;0\\) using numerical maximization in R. Set the argument lower to some small, but positive value, for example lower = 0.0001. lower = 0 will not work since the negative binomial distribution with \\(r=0\\) is not defined.\n\n\nProblem 5b)\nWhat is the standard error of the ML estimate \\(\\hat{r}\\) in Problem 5a)? Recall that the standard error is the standard deviation in the sampling distribution of an estimator. An approximate answer is enough, and you can obtain it numerically (no maths needed!).\n\n\nProblem 5c)\nLet us now try to estimate both parameters \\(\\mu\\) and \\(r\\) at the same time (jointly, as we say) using the maximum likelihood method. Use numerical maximization again and set lower = c(0.0001, 0.0001) to make sure that the estimated parameters are positive, as they should be in the negative binomial distribution.\n\n\nProblem 5d)\nCompute the standard errors for \\(\\hat r\\) and for \\(\\hat\\mu\\)."
  },
  {
    "objectID": "hidden/AssignmentPart2.html#problem-6---regression-models-for-count-data",
    "href": "hidden/AssignmentPart2.html#problem-6---regression-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 6 - Regression models for count data",
    "text": "Problem 6 - Regression models for count data\nOk, now we make this more interesting by modeling the expected number of bugs as function of covariates/features in the X matrix loaded above:\n\nintercept - this is columns of ones to model the intercept\nnCommit - the number of commits since the last release of the software\npropC - proportion of C/C++ code in the project\npropJava - proportion of Java code in the project\ncomplexity - a measure of code complexity that takes into account the frequency of if statements etc.\n\n\nProblem 6a)\nUse numerical optimization to fit the Poisson regression model\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Poisson}\\Big(\\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nNote how each observation has its ‚Äúown‚Äù \\(\\lambda_i\\) parameter, which is modeled as the exponential of \\(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\). We use the exponential function to make sure that \\(\\lambda_i\\) is always positive, as it has to be in the Poisson model. Note that a Poisson regression is a model for the conditional expectation\n\\[\n\\mathbb{E}(Y_i \\vert \\boldsymbol{x}_i) = \\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big) = \\exp\\big( \\beta_0 + \\beta_1 x_{1,i} + \\ldots + \\beta_p x_{p,i} \\big).\n\\]\nThe covariates/features for the \\(i\\)th observation \\(\\boldsymbol{x}_i=(x_{1,i},\\ldots,x_{p,i})^\\top\\) is given the \\(i\\)th row of the matrix X . For example, for the second observation we have y[2] = 6, so six bugs in the second release, and the covariate values for this second release are:\n\nX[2,]\n\n intercept   nCommits      propC   propJava complexity \n 1.0000000  4.0000000  0.4150649  0.3295860  0.7031914 \n\n\nThat is, this release (observation) has 4 commits, approximately 41.4% C code, 32.9 % Java code and a Code Complexity of 0.7. The regression coefficients in \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) measures how the mean of the Poisson distribution \\(\\lambda_i\\) changes when the features change. For example, \\(\\beta_4\\) determines how sensitive the mean number of a bugs in a release is to the code complexity. However, since \\(\\lambda_i\\) is not a linear function of the features (as a result of the exponential function), the \\(\\beta\\) coefficients cannot be interpreted as in ordinary linear regression; see the Lecture on Poisson regression.\nOk, I can sense that you are eager to fit this model, so let‚Äôs do it! Use the bugs dataset to compute the maximum likelihood estimate of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) with numerical optimization. Note that since we have used the exponential function in the model, the elements of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) are free to take on any value, also negative values. So you can use method = c(\"BFGS\") and there is no need to use the lower argument in optim.\n\n\nProblem 6b)\nUse the optimization output from the previous problem to get approximate standard errors for each estimated \\(\\beta\\) coefficient. Compute 95% confidence intervals for each \\(\\beta\\) parameter. Which covariates seem to be statistically different from zero? [Hint: use the large-sample normal approximation of the sampling distribution, and use the confidence interval to check the statistical significance.]\n\n\nProblem 6c)\nThe coming release, which we here number as observation number 92, has the following covariate vector:\n\nx92 = c(1, 10, 0.45, 0.5, 0.89)\n\nSo, the release is based on 10 commits, good proportions of C and Java code and a high code complexity of 0.89. What is the predicted number of bugs for this release?\nBonus problem - but only if you feel up for it and have energy left. Otherwise, feel free to skip this.\nConsider the negative binomial regression\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{ind}}{\\sim} \\mathrm{NegBin}\\Big(r, \\mu_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nThe parameters in this model is therefore \\(r\\) and the regression coefficients in \\(\\boldsymbol{\\beta}\\). Find the maximum likelihood estimates of the parameters using numerical optimization for the bugs data. Note that you want to use lower = (0.0001, -Inf, -Inf, -Inf, -Inf, -Inf) here to make sure that \\(r\\) is positive and the regression coefficients are unrestricted."
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-4---a-little-linear-algebra",
    "href": "assignment/AssignmentPart2.html#problem-4---a-little-linear-algebra",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - A little linear algebra",
    "text": "Problem 4 - A little linear algebra\n\nProblem 4a)\nLet \\(\\boldsymbol{a}=(1,0,2)^\\top\\) and \\(\\boldsymbol{b}=(1,0,-1)^\\top\\) be two vectors. The dot product is computes in R as\n\na = c(1,0,2)\nb = c(1,0,-1)\na%*%b\n\n     [,1]\n[1,]   -1\n\n\nNote that R returns the dot product as a matrix (with one row and one column) even though it is a scalar (a number). If you really want a number you can do return the first (and only) element like this:\n\n(a%*%b)[1]\n\n[1] -1\n\n\nAre \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) orthogonal?\n\n\nProblem 4b)\nSimulate a \\(10\\times 3\\) matrix \\(\\boldsymbol{X}\\) with standard normal N(0,1) random variables. Let \\(\\beta = (1,1,2)^\\top\\) be a vector. Compute the matrix-vector product \\(\\boldsymbol{\\mu} = \\boldsymbol{X}\\boldsymbol{\\beta}\\). When you code this, use the variable name mu for \\(\\boldsymbol{\\mu}\\) and b for \\(\\boldsymbol{\\beta}\\). Explain how the first element of \\(\\boldsymbol{\\mu}\\) relates to the elements in \\(\\boldsymbol{X}\\).\n\n\nProblem 4c)\nNow simulate a vector of errors \\(\\boldsymbol{\\varepsilon}\\) (use the variable name epsilon) from a normal distribution with mean zero and standard deviation \\(\\sigma = 0.1\\). Compute the vector of response observations \\(\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Compute the least squares estimate \\(\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\) based on the simulated \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\).\n\n\nProblem 4d)\nThe variance of the errors in \\(\\boldsymbol{\\varepsilon}\\) can be estimated from the vector of residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\) as follows\n\\[\ns^2 = \\frac{\\boldsymbol{e}^\\top \\boldsymbol{e}}{n-p}\n\\]\nThis is the same residual variance formula as you used in the previous statistics course, where it was written using sums instead of vectors\n\\[\ns^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-p}\n\\]\nThe covariance matrix of the least squares \\(\\hat\\beta\\) estimator can be estimated by the matrix formula\n\\[\ns^2(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\n\\]\nand the standard errors for each \\(\\hat\\beta\\) is therefore the square root of the diagonal elements of this covariance matrix. Compute the standard errors of the regression coefficients based on the 10 observations that you simulated in Problems 4c)."
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "href": "assignment/AssignmentPart2.html#problem-5---numerical-maximum-likelihood-for-negative-binomial-distribution",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 5 - Numerical maximum likelihood for negative binomial distribution",
    "text": "Problem 5 - Numerical maximum likelihood for negative binomial distribution\n\nProblem 5a)\nWe continue with the bugs data from the first part of the Assignment. The file bugs.csv contains a dataset with the number of bugs and some other explanatory variables for \\(n=91\\) releases of several software projects. We load the data:\n\ndata = read.csv(\"https://github.com/StatisticsSU/STM/raw/main/assignment/bugs.csv\", \n                header = TRUE)\ny = data$nBugs # response variable: the number of bugs, a vector with n = 91 observations\nX = data[,-1]  # 91 x 5 matrix with covariates\nX = as.matrix(X) # X was initial a data frame, but we want it to be matrix\n\nIn this problem you will learn to find the maximum likelihood estimate numerically for models with a single parameter. We will only use the response variable \\(y\\), the number of bugs. Later in this assignment we will also use the covariates/features in matrix \\(\\mathbf{X}\\).\nWe use the negative binomial again, but this time we try to estimate the parameter \\(r\\) given that the mean parameter \\(\\mu\\) is fixed at the estimate \\(\\hat\\mu=\\bar y \\approx 5.2528\\). So the model is\n\\[\nY_1,Y_2,\\ldots,Y_n \\overset{iid}{\\sim}\\mathrm{NegBin}(r,\\mu=5.2528)  \n\\]\nand your task is to find the maximum likelihood estimate \\(\\hat r\\) of the parameter \\(r&gt;0\\) using numerical maximization in R. Set the argument lower to some small, but positive value, for example lower = 0.0001. lower = 0 will not work since the negative binomial distribution with \\(r=0\\) is not defined.\n\n\nProblem 5b)\nWhat is the standard error of the ML estimate \\(\\hat{r}\\) in Problem 5a)? Recall that the standard error is the standard deviation in the sampling distribution of an estimator. An approximate answer is enough, and you can obtain it numerically (no maths needed!).\n\n\nProblem 5c)\nLet us now try to estimate both parameters \\(\\mu\\) and \\(r\\) at the same time (jointly, as we say) using the maximum likelihood method. Use numerical maximization again and set lower = c(0.0001, 0.0001) to make sure that the estimated parameters are positive, as they should be in the negative binomial distribution.\n\n\nProblem 5d)\nCompute the standard errors for \\(\\hat r\\) and for \\(\\hat\\mu\\)."
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-6---regression-models-for-count-data",
    "href": "assignment/AssignmentPart2.html#problem-6---regression-models-for-count-data",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 6 - Regression models for count data",
    "text": "Problem 6 - Regression models for count data\nOk, now we make this more interesting by modeling the expected number of bugs as function of covariates/features in the X matrix loaded above:\n\nintercept - this is columns of ones to model the intercept\nnCommit - the number of commits since the last release of the software\npropC - proportion of C/C++ code in the project\npropJava - proportion of Java code in the project\ncomplexity - a measure of code complexity that takes into account the frequency of if statements etc.\n\n\nProblem 6a)\nUse numerical optimization to fit the Poisson regression model\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Poisson}\\Big(\\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nNote how each observation has its ‚Äúown‚Äù \\(\\lambda_i\\) parameter, which is modeled as the exponential of \\(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\). We use the exponential function to make sure that \\(\\lambda_i\\) is always positive, as it has to be in the Poisson model. Note that a Poisson regression is a model for the conditional expectation\n\\[\n\\mathbb{E}(Y_i \\vert \\boldsymbol{x}_i) = \\lambda_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big) = \\exp\\big( \\beta_0 + \\beta_1 x_{1,i} + \\ldots + \\beta_p x_{p,i} \\big).\n\\]\nThe covariates/features for the \\(i\\)th observation \\(\\boldsymbol{x}_i=(x_{1,i},\\ldots,x_{p,i})^\\top\\) is given the \\(i\\)th row of the matrix X . For example, for the second observation we have y[2] = 6, so six bugs in the second release, and the covariate values for this second release are:\n\nX[2,]\n\n intercept   nCommits      propC   propJava complexity \n 1.0000000  4.0000000  0.4150649  0.3295860  0.7031914 \n\n\nThat is, this release (observation) has 4 commits, approximately 41.4% C code, 32.9 % Java code and a Code Complexity of 0.7. The regression coefficients in \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) measures how the mean of the Poisson distribution \\(\\lambda_i\\) changes when the features change. For example, \\(\\beta_4\\) determines how sensitive the mean number of a bugs in a release is to the code complexity. However, since \\(\\lambda_i\\) is not a linear function of the features (as a result of the exponential function), the \\(\\beta\\) coefficients cannot be interpreted as in ordinary linear regression; see the Lecture on Poisson regression.\nOk, I can sense that you are eager to fit this model, so let‚Äôs do it! Use the bugs dataset to compute the maximum likelihood estimate of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) with numerical optimization. Note that since we have used the exponential function in the model, the elements of \\(\\boldsymbol{\\beta}=(\\beta_0,\\beta_1,\\ldots,\\beta_4)^\\top\\) are free to take on any value, also negative values. So you can use method = c(\"BFGS\") and there is no need to use the lower argument in optim.\n\n\nProblem 6b)\nUse the optimization output from the previous problem to get approximate standard errors for each estimated \\(\\beta\\) coefficient. Compute 95% confidence intervals for each \\(\\beta\\) parameter. Which covariates seem to be statistically different from zero? [Hint: use the large-sample normal approximation of the sampling distribution, and use the confidence interval to check the statistical significance.]\n\n\nProblem 6c)\nThe upcoming release, which we here number as observation number 92, has the following covariate vector:\n\nx92 = c(1, 10, 0.45, 0.5, 0.89)\n\nSo, the release is based on 10 commits, good proportions of C and Java code and a high code complexity of 0.89. What is the predicted number of bugs for this release?"
  },
  {
    "objectID": "assignment/AssignmentPart2.html#bonus-problem",
    "href": "assignment/AssignmentPart2.html#bonus-problem",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Bonus problem",
    "text": "Bonus problem\n\n\n\n\n\n\nCaution\n\n\n\nThis is a bonus problem. It is not very hard, but do it only if you feel up for it and have energy left. You also need time to study for the exam. And to have a life. üòä\n\n\nConsider the negative binomial regression\n\\[\nY_i \\vert \\boldsymbol{x}_i \\overset{\\mathrm{ind}}{\\sim} \\mathrm{NegBin}\\Big(r, \\mu_i = \\exp\\big(\\boldsymbol{x}_i^\\top \\boldsymbol{\\beta}\\big)\\Big)\n\\]\nThe parameters in this model is therefore \\(r\\) and the regression coefficients in \\(\\boldsymbol{\\beta}\\). Find the maximum likelihood estimates of the parameters using numerical optimization for the bugs data. Note that you want to use lower = (0.0001, -Inf, -Inf, -Inf, -Inf, -Inf) here to make sure that \\(r\\) is positive and the regression coefficients are unrestricted."
  },
  {
    "objectID": "assignment/AssignmentPart2.html#problem-4---linear-regression-in-vector-form",
    "href": "assignment/AssignmentPart2.html#problem-4---linear-regression-in-vector-form",
    "title": "Statistical Theory and Modeling, 7.5 hp",
    "section": "Problem 4 - Linear regression in vector form",
    "text": "Problem 4 - Linear regression in vector form\n\nProblem 4a)\nLet \\(\\boldsymbol{a}=(1,0,2)^\\top\\) and \\(\\boldsymbol{b}=(1,0,-1)^\\top\\) be two vectors. The dot product is computes in R as\n\na = c(1,0,2)\nb = c(1,0,-1)\na%*%b\n\n     [,1]\n[1,]   -1\n\n\nNote that R returns the dot product as a matrix (with one row and one column) even though it is a scalar (a number). If you really want a number you can do return the first (and only) element like this:\n\n(a%*%b)[1]\n\n[1] -1\n\n\nAre \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) orthogonal?\n\n\nProblem 4b)\nSimulate a \\(10\\times 3\\) matrix \\(\\boldsymbol{X}\\) with standard normal N(0,1) random variables. Let \\(\\beta = (1,1,2)^\\top\\) be a vector. Compute the matrix-vector product \\(\\boldsymbol{\\mu} = \\boldsymbol{X}\\boldsymbol{\\beta}\\). When you code this, use the variable name mu for \\(\\boldsymbol{\\mu}\\) and b for \\(\\boldsymbol{\\beta}\\). Explain how the first element of \\(\\boldsymbol{\\mu}\\) relates to the elements in \\(\\boldsymbol{X}\\).\n\n\nProblem 4c)\nNow simulate a vector of errors \\(\\boldsymbol{\\varepsilon}\\) (use the variable name epsilon) from a normal distribution with mean zero and standard deviation \\(\\sigma = 0.1\\). Compute the vector of response observations \\(\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\). Compute the least squares estimate \\(\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\) based on the simulated \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\).\n\n\nProblem 4d)\nThe variance of the errors in \\(\\boldsymbol{\\varepsilon}\\) can be estimated from the vector of residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}\\) as follows\n\\[\ns^2 = \\frac{\\boldsymbol{e}^\\top \\boldsymbol{e}}{n-p}\n\\]\nThis is the same residual variance formula as you used in the previous statistics course, where it was written using sums instead of vectors\n\\[\ns^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-p}\n\\]\nThe covariance matrix of the least squares \\(\\hat\\beta\\) estimator can be estimated by the matrix formula\n\\[\ns^2(\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\n\\]\nand the standard errors for each \\(\\hat\\beta\\) is therefore the square root of the diagonal elements of this covariance matrix. Compute the standard errors of the regression coefficients based on the 10 observations that you simulated in Problems 4c)."
  }
]