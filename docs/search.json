[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "Aim\n\nThis is a course on the Masterâ€™s Program in Data Science, Statistics and Decision Analysis at Stockholm University.\nThe course is specifically designed to bridge between the basic course Statistics and Data Analysis for Computer and Systems Sciences, 15 hp and the masterâ€™s level course Bayesian Learning, 7.5 hp. The objective is to provide the probability, statistical theory and modeling needed to follow the Bayesian Learning course.\n\n\n\nContents\n\nMathematical methods: derivatives, integrals, optimization, numerical optimization, vectors and matrices.\nProbability theory: discrete and continuous stochastic variables, density and probability functions, distribution functions, multivariate distributions, multivariate normal distribution, marginal distributions, conditional distributions, independence, expected value, variance, and covariance, functions of stochastic variables, sampling distributions, law of large numbers, central limit theorem.\nModelling and prediction: linear and non-linear regression, dummy variables and interactions, model selection, cross-validation, overfitting, regularization, classification, logistic regression, multinomial logistic regression, Poisson regression.\nInference: point estimation, bias-variance trade-off, maximum likelihood (ML), likelihood theory, numerical optimization for ML estimation, bootstrap.\nTime series: trend and seasonality, autocorrelation, autoregressive models.\n\n\n\nLiterature\n\nAuthors (2021). Book Name\nAdditional material and handouts distributed during the course.\n\n\n\nStructure\nThe course consists of lectures, mathematical exercises and computer labs.\n\n\nExamination\nThe course is examined by a\n\nwritten exam (grades A-F)\nhome assignment (grade pass/fail).\n\n\n\nSchedule\nThe course schedule can be found on TimeEdit. A tip is to select Subscribe in the upper right corner of TimeEdit and then paste the link into your phoneâ€™s calendar program.\n\n\nFormula cheet sheets\n\n\nInteractive material\nThe course makes heavy use of interactive Observable notebooks in javascript that runs in your browser. The widgets will be linked below each relevant lecture. All widgets used in the course are available here.\n\n\nTeachers\n\n\n\n\nMattias VillaniCourse responsible and lecturerProfessor\n\n\n\nTBDExercises\n\n\n\n\n\nTBDComputer labs"
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html",
    "href": "tutorial/numericalML/numericalML.html",
    "title": "Maximum likelihood by numerical optimization",
    "section": "",
    "text": "In this tutorial you will learn how maximum likelihood estimates and standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is typically surprisingly accurate also for smaller sample sizes.\n\nIt will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner giving you the super-power ðŸ’ª to use the computer to estimate the parameters and their uncertainty in quite complex models.\nWe will start with simple models with a single parameter to cover all the concepts, and then move on to the practically more important multi-parameter case.\nLetâ€™s first load some useful libraries (install them using install.packages() if you havenâ€™t already).\n\nlibrary(latex2exp) # for plotting mathematical symbols (LaTeX)\nlibrary(remotes)   # for loading packages from GitHub\nlibrary(ggplot2)   # for fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/numericalML/numericalML.html#footnotes",
    "href": "tutorial/numericalML/numericalML.html#footnotes",
    "title": "Maximum likelihood by numerical optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdfâ†©ï¸Ž\nIf we want to actually interpret these joint probabilities, we can consider looking at the average probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual arithmetic mean\n\\[\\frac{1}{n}\\sum_ {i=1}^n P(y_i \\vert \\lambda)\\]\nis not so great for averaging probabilities, however. The geometric mean\n\\[\\Big(\\prod_ {i=1}^n P(y_i \\vert \\lambda)\\Big)^{\\frac{1}{n}}\\]\nhas nicer properties, so we would use that.â†©ï¸Ž"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html",
    "href": "tutorial/bootstrap/bootstrap.html",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "",
    "text": "In this tutorial you will learn about the bootstrap method for approximating the sampling distribution of any estimator, for example the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\nLetâ€™s first load some libraries that we will use (install them using install.packages() if you havenâ€™t already).\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution"
  },
  {
    "objectID": "tutorial/bootstrap/bootstrap.html#footnotes",
    "href": "tutorial/bootstrap/bootstrap.html#footnotes",
    "title": "The bootstrap for approximating sampling distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, Journal of Business and Economic Statistics pdfâ†©ï¸Ž"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nTutorials\n\nMaximum likelihood by numerical optimization html\nBootstrap html\nBonus: State-space models html\nTemporary: html"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nLecture 0 - (really) Basic maths\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 1 - Introduction. Discrete random variables.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 2 - Continuous random variables. Probability density functions. Integration.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 3 - Joint and conditional distributions. Covariance and correlation. Bayes theorem.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 4 - Functions. Derivatives. Mathematical optimization.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 5 - Transformation of random variables. Monte Carlo simulation. Law of large numbers. Central limit theorem.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 6 - Point estimation. Maximum likelihood.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 7 - Vectors and matrices. Multivariate normal distribution.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 8 - Linear regression in vector form.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 9 - Sampling distributions. Observed and Fisher information. Numerical optimization.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 10 - Logistic regression.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 11 - Nonlinear regression. Interactions. Overfitting. Regularization. Cross-validation. Bias-variance trade-off\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 12 - Time series. Autocorrelation function. Autoregressive models.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:\n\nLecture 13 - Course summary and example exam.\nRead: X | Slides | tutorial on numerical ML\nCode:\nData:"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Programming for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future."
  },
  {
    "objectID": "computerlabs.html",
    "href": "computerlabs.html",
    "title": "Computer labs for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nComputer lab 1 \nMaterial: html\nComputer lab 2 \nMaterial: html"
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises for Statistical Theory and Modelling, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nExercise 1 - Basic probability.\nProblems: Book 2.1, 2.2, â€¦\nExercise 2 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 3 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 4 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 5 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 6 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 7 - Whatever.\nProblems: Book 3.1, 3.2, â€¦\nExercise 8 - Whatever.\nProblems: Book 3.1, 3.2, â€¦"
  },
  {
    "objectID": "tutorial/statespace/statespace.html",
    "href": "tutorial/statespace/statespace.html",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "",
    "text": "This tutorial gives a very brief introduction to state-space models, along with inference methods like Kalman filtering, smoothing and forecasting. The methods are illustrated using the R package dlm , exemplified with the local level model fitted to the well-known Nile river data. The tutorial is also sprinkled with some cool interactivity in Javascript."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#footnotes",
    "href": "tutorial/statespace/statespace.html#footnotes",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClicking on the  below the widget will take you to the Observable notebook of the widget where you can also change the locations of the thresholds, \\(t_1,t_2,\\ldots,t_{K-1}\\), if you are really into that sort of thing.â†©ï¸Ž\nIts all about that Bayes\nThe Kalman filter is often presented from a frequentist point of view in statistics, where the Kalman filtered estimates are the optimal estimates in the mean square error sense.\n\nThe Kalman filter can also be derived as simple Bayesian updating, using Bayesâ€™ theorem to update the information about the state as a new measurement comes in. The \\(\\boldsymbol{\\mu_{0|0}}\\) and \\(\\boldsymbol{\\Omega_{0|0}}\\) can be seen as the prior mean and prior covariance matrix summarizing your prior information about the state before collecting any measurements.\n\nThe Kalman filter is great. When something is great, Bayes usually lurks in the background! ðŸ˜œâ†©ï¸Ž"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#state-space-model",
    "href": "tutorial/statespace/statespace.html#state-space-model",
    "title": "State-Space models and the Kalman Filter",
    "section": "State-space model",
    "text": "State-space model\nA state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) is in the dlm package written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{C} &= 1 \\\\\n\\boldsymbol{A} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nHence the state vector is a single scalar, \\(\\mu_t\\), the unobserved local level of time series. We learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) .\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models: filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#filtering",
    "href": "tutorial/statespace/statespace.html#filtering",
    "title": "State-Space models and the Kalman Filter",
    "section": "Filtering",
    "text": "Filtering\nLoad the package\n\n#install.packages(\"kalmanfilter\")\nlibrary(kalmanfilter)\n\nSet up the state-space model"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#dlm-package",
    "href": "tutorial/statespace/statespace.html#dlm-package",
    "title": "State-Space models and the Kalman Filter",
    "section": "dlm package",
    "text": "dlm package\nIn the package dlm package with univariate observation vector \\(y_t\\) and state vector \\(\\boldsymbol{\\theta}_t\\) the model is written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is obtained with\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{C} &= 1 \\\\\n\\boldsymbol{A} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\n\nFiltering\nLoad the dlm package\n\n#install.packages(\"dlm\")\nlibrary(dlm)\n\nSet up the local level model as a state-space model with \\(\\sigma_\\varepsilon^2 = 10000\\) and \\(\\sigma_\\nu^2 = 10000\\) (see below on how I decided these values).\n\nmodel = dlm(FF = 1, V = 10000, GG = 1, W = 10000, m0 = 0, C0 = 100^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\nu^2\\) were just set to some values above. Letâ€™s instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 0, C0 = 100^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(0,0), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1]  9120.485 15956.088\n\n\nSo we see that the values used initially are not too far of the maximum likelihood estimates:\n\\(\\hat \\sigma_\\varepsilon^2 \\approx 9120\\) and \\(\\hat\\sigma_\\nu^2 \\approx 15956\\). We can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 0, C0 = 100^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nSmoothing\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\"), lty = 1, \n    col = c(\"steelblue\", \"red\"))\n\n\n\n\n\n\nForecasting\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), \n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, \n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#piecewise-constant-model",
    "href": "tutorial/statespace/statespace.html#piecewise-constant-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Piecewise constant model",
    "text": "Piecewise constant model\nAn extremely simple model for a time series is to treat the observations as independent normally distributed with the same mean \\(\\mu\\) and variance \\(\\sigma_\\varepsilon\\)\n\\[\ny_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\n\nShow the code\n#install.packages(\"latex2exp\")\nlibrary(latex2exp)\nn = 200\nmu = 2\nsigma_eps = 1\ny = rnorm(n, mean = mu, sd = sigma_eps)\nplot(seq(1,n), y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y_t\", lwd = 1.5,\n    main = \"Simulated data from the naive iid model\")\nlines(seq(1,n), rep(mu,n), type = \"l\", col = \"orange\")\nlegend(\"topright\", legend = c(TeX(\"$y_t$\"), TeX(\"$\\\\mu$\")), lty = 1, lwd = 1.5, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nThis model is of course not something to write home about, it basically ignores the time series nature of the data. Let us start to make it a little more interesting by allowing the mean to vary of time. This means that we will have a time-varying parameter model where the mean \\(\\mu_t\\) changes (abruptly) at certain time points \\(t_1, t_2, \\dots, t_K\\):\n\\[\ny_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\\[\n\\begin{align}   \n\\mu_t &=\n\\begin{cases}            \n  \\mu_1 & \\text{if $1 \\leq t \\leq t_1$} \\\\\n  \\mu_2 & \\text{if $t_1 &lt; t \\leq t_2$} \\\\            \n  \\vdots & \\vdots \\\\\n  \\mu_K & \\text{if $t_{K-1} &lt; t \\leq T$}. \\\\          \n\\end{cases}\n\\end{align}\n\\]\nHere is a widget that lets you simulate data from the piecewise constant model1."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#local-level-model",
    "href": "tutorial/statespace/statespace.html#local-level-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Local level model",
    "text": "Local level model\nThe piecewise constant model has a few abrupt changes in the mean, but what if the mean changes more gradually? The local level model has a constantly changing mean following a random walk model:\n\\[y_t = \\mu_t + \\varepsilon_t,\\qquad \\varepsilon_t \\sim N(0,\\sigma_\\varepsilon^2)\\]\n\\[\\mu_t = \\mu_{t-1} + \\eta_t,\\qquad \\eta_t \\sim N(0,\\sigma_\\eta^2)\\]\nwhich models the observed time series \\(y_t\\) as a mean \\(\\mu_t\\) plus a random measurement error or disturbance \\(\\varepsilon_t\\). The mean \\(\\mu_t\\) evolves over time as a random walk driven by innovations \\(\\eta_t\\).\nHere is a widget that simulates data from the model. Go ahead, experiment with the measurement/noise \\(\\sigma_\\varepsilon\\) and the standard deviation of the innovations to the mean process, \\(\\sigma_\\eta\\). For example, drive \\(\\sigma_\\eta\\) toward zero and note how the mean becomes close to constant over time."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#regression-with-time-varying-parameters",
    "href": "tutorial/statespace/statespace.html#regression-with-time-varying-parameters",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Regression with time-varying parameters",
    "text": "Regression with time-varying parameters\nThe usual simple linear time series regression model is\n\\[\ny_t = \\alpha + \\beta x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\qquad t=1,\\ldots,T\n\\]\nwhere \\(y_t\\) is a time series response variable (for example electricity price) that is being explained by the explanatory variable \\(x_t\\) (for example temperature). This model assumes that the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma_\\varepsilon\\) are constant in time, that the relationship between electricity price and temperature has remained the same throughout the whole observed time period.\nIt sometimes makes sense to let the parameters vary with time. Here is one such model, the time-varying regression model:\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nwhere the intercept \\(\\alpha\\) now has a time \\(t\\) subscript and evolves in time following a random walk process\n\\[\\alpha_{t} = \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)\\]\nso that in every time period, the intercept changes by adding on an innovation \\(\\eta_t\\) drawn from a normal distribution with standard deviation \\(\\sigma_\\alpha\\). This standard deviation therefore controls how much the intercept changes over time. The slope \\(\\beta\\) changes over time in a similar fashion, with the speed of change determined by \\(\\sigma_\\beta\\).\nHere is a widget that simulates data from the time-varying regression above. By moving the slider (show regline at time) you can plot the regression line \\(\\alpha_t + \\beta_t x_t\\) at any time period \\(t\\). The single data point at that time period is the larger dark red point. The plot also highlights (darker blue) data points that are closer in time to the time chosen by the slider. To the left you can see the whole time path of the simulated \\(\\alpha\\) and \\(\\beta\\) with the current parameters highlighted by dots."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#the-dlm-package-in-r",
    "href": "tutorial/statespace/statespace.html#the-dlm-package-in-r",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "The dlm package in R",
    "text": "The dlm package in R\nThe dlm package is a user-friendly R package for analyzing some state-space models. The package has a nice vignette that is worth reading if you plan to use the package more seriously.\n\nFiltering\nLetâ€™s first do some filtering in the dlm package. Start by loading the dlm package:\n\n#install.packages(\"dlm\") # uncomment the first time to install.\nlibrary(dlm)\n\nWe now need to tell the dlm package what kind of state-space model we want to estimate. The means setting up the matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\). We will keep it simple and use the local level model as example, where all parameter matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\) are scalars (single numbers). As we have seen above, the local level model corresponds to a state-space model with parameters\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\eta^2\n\\end{align}\n\\]\nSo we only need to set \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) to start the fun. We will for now set \\(\\sigma_\\varepsilon^2 = 100^2\\) and \\(\\sigma_\\eta^2 = 100^2\\), and return to this when we learn how the dlm package can find maximum likelihood estimates for these parameters. Here is how you setup the local level model in the dlm package:\n\nmodel = dlm(FF = 1, V = 100^2, GG = 1, W = 100^2, m0 = 1000, C0 = 1000^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) were just set to some values above. Letâ€™s instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 1000, C0 = 1000^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(0,0), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1] 15101.339  1467.049\n\n\nor the square roots, to get the maximum likelihood estimates of the standard deviations\n\nsqrt(exp(fit$par))\n\n[1] 122.88750  38.30208\n\n\nWe can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 1000, C0 = 1000^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lwd = 1.5, lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nSmoothing\nWe can also use the dlm package to compute the smoothed retrospective estimates of the local level \\(\\mu_t\\) at time \\(t\\) using all the data from \\(t=1\\) until the end of the time series \\(T\\). We havenâ€™t showed the mathematical algorithm for smoothing, but you can look it up in many books. Anyway, here is the smoothing results for the Nile data, using the function dlmSmooth from the dlm package. The filtered estimates are also shown.\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\",\"Smoothed\"), lty = 1, lwd = 1.5, col = c(\"steelblue\", \"orange\", \"red\"))\n\n\n\n\n\n\nForecasting\nWe can also use state-space models for forecasting. Here is how it is done in the dlm package.\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), lwd = 1.5,\n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, lwd = 1.5,\n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#state-space-model---filtering-smoothing-and-forecasting",
    "href": "tutorial/statespace/statespace.html#state-space-model---filtering-smoothing-and-forecasting",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "State-space model - filtering, smoothing and forecasting",
    "text": "State-space model - filtering, smoothing and forecasting\n\nThe state space model\nAll of the models above, and many, many, many more can be written as a so called state-space model. A state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) can be written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\nFor example, the local level model is a state-space model with a single scalar state variable \\(\\boldsymbol{\\theta}_t = \\mu_t\\) and parameters\n\\[\n\\begin{align}\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nWe learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) . The first equation is often called the observation or measurement model since it gives the connection between the unobserved state and the observed measurements. The measurements can also be a vector, but we will use a single measurement in this tutorial. The second equation is called the state transition model since it determines how the state evolves over time.\nWe can even let the state-space parameters \\(\\boldsymbol{F}, \\boldsymbol{G}, \\boldsymbol{V}, \\boldsymbol{W}\\) be different i every time period. This is in fact needed if we want to write the time-varying regression model in state-space form. Recall the time varying regression model\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nWe can tuck the two time-varying parameters in a vector \\(\\boldsymbol{\\beta}_t=(\\alpha_t,\\beta_t)^\\top\\) and also write the models as\n\\[\n\\begin{align}  \ny_t &= \\boldsymbol{x}_t^\\top\\boldsymbol{\\beta}_{t}   + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\    \n\\boldsymbol{\\beta}_{t} &= \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t, \\quad \\quad \\nu_t \\sim N(0, \\boldsymbol{W})\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}  \n\\boldsymbol{x}_t &= (1,x_t)^\\top  \\\\    \n\\boldsymbol{w}_t &= (\\eta_t,\\nu_t)^\\top  \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nNote this is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{F}_t &= \\boldsymbol{x}_t\\\\\n\\boldsymbol{G} &=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nand note now that \\(\\boldsymbol{F}\\) changes in every time period, hence the subscript \\(t\\).\n\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models: filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data.\nFiltering means to compute the sequence of instantaneous estimates of the unobserved state at every time point \\(t=1,2,\\ldots,T\\)\n\\[\n\\hat{\\boldsymbol{\\theta}}_{1|1},\\hat{\\boldsymbol{\\theta}}_{2|2},\\ldots,\\hat{\\boldsymbol{\\theta}}_{T|T}\n\\]\nWe will take a time series and compute the filtered estimates for the whole time series, but it is important to understand that filtering is often done in real-time, which means it is a continously ongoing process that returns filtered estimates of the state \\(\\boldsymbol{\\theta}_t\\) as time progresses and new measurements \\(y_t\\) come in. Think about a self-driving car that is continously trying to understand the environment (people, other cars, the road conditions etc). The environment is the state and the car uses its sensors to collect measurements. The filtering estimates tells the car about the best guess for the environment at every point in time.\nFor state-space models of the type discussed here (linear measurement equation and linear evolution of the state, with independent Normal measurement errors and state innovations), the filtered estimates are computed with one of the most famous algorithms in statistics: the Kalman filter.\nThe Kalman filter is a little messy to write up, we will do it for completeness, but we will use a package for it so donâ€™t worry if the linear algebra is intidimating. We will use the notation $\\(\\boldsymbol{\\mu}_{t|t}\\) instead of \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\), but they really mean the same.\n\ntime \\(t = 0\\). The Kalman filter starts with mean \\(\\boldsymbol{\\mu}_{0|0}\\) and covariance matrix \\(\\boldsymbol{\\Omega}_{0|0}\\) for the state at time \\(t=0\\). Think about \\(\\boldsymbol{\\mu}_{0|0}\\) as the best guess \\(\\boldsymbol{\\theta}_0\\) of the state vector at time \\(t=0\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\) representing how sure we can be about this guess2.\ntime \\(t = 1\\). The Kalman filter then uses the first measurement \\(y_1\\) to update \\(\\boldsymbol{\\mu}_{0|0} \\rightarrow \\boldsymbol{\\mu}_{1|1}\\) and \\(\\boldsymbol{\\Omega}_{0|0} \\rightarrow \\boldsymbol{\\Omega}_{1|1}\\) to represent the estimate and the uncertainty for \\(\\boldsymbol{\\theta}_1\\), the state at time \\(t=1\\).\ntime \\(t = 2,...,T\\). It then continues in this fashion using the next measurement \\(y_2\\) to compute \\(\\boldsymbol{\\mu}_{2|2}\\) and \\(\\boldsymbol{\\Omega}_{2|2}\\) and so on all the way to the end of the time series to finally get \\(\\boldsymbol{\\mu}_{T|T}\\) and \\(\\boldsymbol{\\Omega}_{T|T}\\).\n\nHere is the Kalman filter algorithm:\n\n\nInitialization: set \\(\\boldsymbol{\\mu}_{0|0}\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\)\nfor \\(t=1,\\ldots,T\\) do\n\nPrediction update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t-1} &= \\boldsymbol{G} \\boldsymbol{\\mu}_{t-1|t-1} \\\\  \n\\boldsymbol{\\Omega}_{t|t-1} &= \\boldsymbol{G}\\boldsymbol{\\Omega}_{t-1|t-1}  \\boldsymbol{G}^\\top + \\boldsymbol{W}\n\\end{align}\n\\]\nMeasurement update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t} &= \\boldsymbol{\\mu}_{t|t-1} + \\boldsymbol{K}_t ( y_t - \\boldsymbol{F} \\boldsymbol{\\mu}_{t|t-1}  )  \\\\  \n\\boldsymbol{\\Omega}_{t|t} &= (\\boldsymbol{I} - \\boldsymbol{K}_t \\boldsymbol{F} )\\boldsymbol{\\Omega}_{t|t-1}\n\\end{align}\n\\]\n\n\nwhere \\[\\boldsymbol{K}_t = \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top ( \\boldsymbol{F} \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top + \\boldsymbol{V})^{-1}\\] is the Kalman Gain.\n\nThe widget below lets you experiment with the Kalman filter for the local level model fitted to the Nile river data. In the widget we infer (filter) the local levels \\(\\mu_1,\\mu_2,\\ldots,\\mu_T\\) and can experiment with the measurement standard deviation \\(\\sigma_\\varepsilon\\), the standard deviation of the innovations to the local mean \\(\\sigma_\\eta\\), and also the initial guess for \\(\\mu_0\\) and the standard deviation \\(\\sigma_0\\) of that guess.\nHere are few things to try out in the widget below:\n\nIncrease the measurement standard deviation \\(\\sigma_\\varepsilon\\) and note how the filtered mean pays less and less attention to changes in the data (because the model believes that the data is very poor quality (noisy) and tells us basically nothing about the level). Then move \\(\\sigma_\\varepsilon\\) to smaller values and note how the filtered mean starts chasing the data (because the model believes that the data are super informative about the level).\nMake the standard deviation for the initial level \\(\\sigma_0\\) very small and then change the initial mean \\(\\mu_0\\) to see how this affects the filtered mean at the first part of the time series.\nMove the standard deviation of the innovations to the level \\(\\sigma_\\eta\\) small and note how the filtered mean becomes smoother and smoother over time."
  },
  {
    "objectID": "tutorial/statespace/statespace.html#non-gaussian-state-space-models",
    "href": "tutorial/statespace/statespace.html#non-gaussian-state-space-models",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Non-Gaussian state-space models",
    "text": "Non-Gaussian state-space models\n\nPoisson time series model\nA useful model for time series of counts \\(Y \\in \\{0,1,2,\\ldots \\}\\) is a Poisson distribution with time-varying intensity \\(\\lambda_t = \\exp(z_t)\\), where \\(z_t\\) is some continuous stochastic process with autocorrelation, most commonly a random walk:\n\\[\n\\begin{align} y_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= z_{t-1} + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nNote that because of the exponential function \\(\\lambda_t = \\exp(z_t)\\) is guaranteed to be positive for all \\(t\\), as required for the Poisson distribution. It is easily to simulate data from the Poisson time series model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimPoisTimeSeries &lt;- function(T, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the Poisson variables with different intensities, lambda_t = exp(z_t) for each time\n  lambda = exp(z)\n  return (rpois(T, lambda = lambda[2:(T+1)]))\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1) \nT = 100\nsigma_eta = 0.1\ny = simPoisTimeSeries(T, sigma_eta)\nplot(y, type = \"o\", pch = 19, col = \"steelblue\", yaxt = \"n\", xlab = \"time, t\", ylab = \"counts, y\", \n      main = paste(\"A simulated Poisson time series with sigma_eta =\", sigma_eta))\naxis(side = 2, at = seq(0,max(y)))\n\n\n\n\n\nWhatâ€™s life without widgets? Here is one for a slightly more general Poisson time series model where the random walk is replaced by an autoregressive process of order 1:\n\\[\n\\begin{align}\ny_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= \\mu + \\phi(z_{t-1} -\\mu) + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\n\n\n\nStochastic volatility models\nMany time series, particularly in the finance, has a variance that is changing over time. Furthermore, it is common to find volatility clustering in the data, meaning that once the the variance is high (turbulent stock market) it tends to remain high for a while and vice versa. The basic stochastic volatility (SV) model tries to capture this:\n\\[\n\\begin{align}\ny_t &= \\mu + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= z_{t-1} + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere we have for simplicity assumed just a constant mean \\(\\mu\\), but we can extend this with and autoregressive process, or basically any model of your preference. The thing that set the SV model apart from the other model presented so far is that the variance of the measurement errors \\(Var(y_t)=Var(\\varepsilon_t) = \\exp(z_t)\\) is heteroscedastic, that is, it varies over time. The variance is driven by the \\(z_t\\) process, which here is modeled as a random walk, which will induce volatility clustering. Note again that we use the exponential function to ensure that the variance is positive for all \\(t\\). Here is code to simulate from this basic stochastic volatility model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVol &lt;- function(T, mu, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigmaÂ²_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\nLetâ€™s use that function to simulate a time series and plot it:\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(2) \nT = 100\nmu = 3\nsigma_eta = 1\nsimuldata = simStochVol(T, mu, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nWe can replace the random walk for the \\(z_t\\) with a more well-behaved AR(1) process:\n\\[\n\\begin{align}\ny_t &= \\mu_y + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= \\mu_z + \\phi(z_{t-1} - \\mu_z) + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere \\(\\mu_y\\) is the mean of the time series \\(y\\) and \\(\\mu_z\\) is the mean of the (log) variance process \\(z_t\\). The parameter \\(\\mu_z\\) therefore determines how much variance \\(y_t\\) has on average and \\(\\phi\\) determines how much volatility clustering there is. A \\(\\phi\\) close to 1 gives long periods of persistently large or small variance. Here is the code:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVolAR &lt;- function(T, mu_y, mu_z, phi, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = mu_z + phi*(z[t-1] - mu_z) + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigmaÂ²_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu_y, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1)\nT = 1000\nmu_y = 3\nmu_z = -1\nphi = 0.95\nsigma_eta = 1\nsimuldata = simStochVolAR(T, mu_y, mu_z, phi, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nWidget time!"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#implementing-the-kalman-filter-from-scratch",
    "href": "tutorial/statespace/statespace.html#implementing-the-kalman-filter-from-scratch",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Implementing the Kalman filter from scratch",
    "text": "Implementing the Kalman filter from scratch\nFor the curious, the code below implements the Kalman filter from scratch in R. Let us first implement a function kalmanfilter_update that does the update for a single time step:\n\nkalmanfilter_update &lt;- function(mu, Omega, y, G, C, V, W) {\n  \n  # Prediction step - moving state forward without new measurement\n  muPred &lt;- G %*% mu\n  omegaPred &lt;- G %*% Omega %*% t(G) + W\n  \n  # Measurement update - updating the N(muPred, omegaPred) prior with the new data point\n  K &lt;- omegaPred %*% t(F) / (F %*% omegaPred %*% t(F) + V) # Kalman Gain\n  mu &lt;- muPred + K %*% (y - F %*% muPred)\n  Omega &lt;- (diag(length(mu)) - K %*% F) %*% omegaPred\n  \n  return(list(mu, Omega))\n}\n\nThen a function does all the Kalman iterations, using the kalmanfilter_update function above:\n\nkalmanfilter &lt;- function(Y, G, F, V, W, mu0, Sigma0) {\n  T &lt;- dim(Y)[1]  # Number of time steps\n  n &lt;- length(mu0)  # Dimension of the state vector\n  \n  # Storage for the mean and covariance state vector trajectory over time\n  mu_filter &lt;- matrix(0, nrow = T, ncol = n)\n  Sigma_filter &lt;- array(0, dim = c(n, n, T))\n  \n  # The Kalman iterations\n  mu &lt;- mu0\n  Sigma &lt;- Sigma0\n  for (t in 1:T) {\n    result &lt;- kalmanfilter_update(mu, Sigma, t(Y[t, ]), G, F, V, W)\n    mu &lt;- result[[1]]\n    Sigma &lt;- result[[2]]\n    mu_filter[t, ] &lt;- mu\n    Sigma_filter[,,t] &lt;- Sigma\n  }\n  \n  return(list(mu_filter, Sigma_filter))\n}\n\nLetâ€™s try it out on the Nile river data:\n\n# Analyzing the Nile river data\nprettycolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\")\ny = as.vector(Nile)\nV = 100^2\nW = 100^2\nmu0 = 1000\nSigma0 = 1000^2\n\n# Set up state-space model for local level model\nT = length(y)\nG = 1\nF = 1\nY = matrix(0,T,1)\nY[,1] = y\nfilterRes = kalmanfilter(Y, G, F, V, W, mu0, Sigma0)\nmeanFilter = filterRes[[1]]\nstd_filter = sqrt(filterRes[[2]][,,, drop =TRUE])\n\nplot(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\npolygon(c(seq(1:T), rev(seq(1:T))), \n        c(meanFilter - 1.96*std_filter, rev(meanFilter + 1.96*std_filter)), \n        col = \"#F0F0F0\", border = NA)\nlines(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\nlines(seq(1:T), meanFilter, type = \"l\", col = prettycolors[3], lwd = 1.5)\nlegend(\"topright\", legend = c(\"time series\", \"filter mean\", \"95% intervals\"), lty = 1, lwd = 1.5,\n    col = c(prettycolors[1], prettycolors[3], \"#F0F0F0\"))"
  },
  {
    "objectID": "tutorial/statespace/statespace.html#bonus-implementing-the-kalman-filter-from-scratch",
    "href": "tutorial/statespace/statespace.html#bonus-implementing-the-kalman-filter-from-scratch",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Bonus: Implementing the Kalman filter from scratch",
    "text": "Bonus: Implementing the Kalman filter from scratch\nFor the curious, the code below implements the Kalman filter from scratch in R. Let us first implement a function kalmanfilter_update that does the update for a single time step:\n\nkalmanfilter_update &lt;- function(mu, Omega, y, G, C, V, W) {\n  \n  # Prediction step - moving state forward without new measurement\n  muPred &lt;- G %*% mu\n  omegaPred &lt;- G %*% Omega %*% t(G) + W\n  \n  # Measurement update - updating the N(muPred, omegaPred) prior with the new data point\n  K &lt;- omegaPred %*% t(F) / (F %*% omegaPred %*% t(F) + V) # Kalman Gain\n  mu &lt;- muPred + K %*% (y - F %*% muPred)\n  Omega &lt;- (diag(length(mu)) - K %*% F) %*% omegaPred\n  \n  return(list(mu, Omega))\n}\n\nThen we implement a function that does all the Kalman iterations, using the kalmanfilter_update function above:\n\nkalmanfilter &lt;- function(Y, G, F, V, W, mu0, Sigma0) {\n  T &lt;- dim(Y)[1]  # Number of time steps\n  n &lt;- length(mu0)  # Dimension of the state vector\n  \n  # Storage for the mean and covariance state vector trajectory over time\n  mu_filter &lt;- matrix(0, nrow = T, ncol = n)\n  Sigma_filter &lt;- array(0, dim = c(n, n, T))\n  \n  # The Kalman iterations\n  mu &lt;- mu0\n  Sigma &lt;- Sigma0\n  for (t in 1:T) {\n    result &lt;- kalmanfilter_update(mu, Sigma, t(Y[t, ]), G, F, V, W)\n    mu &lt;- result[[1]]\n    Sigma &lt;- result[[2]]\n    mu_filter[t, ] &lt;- mu\n    Sigma_filter[,,t] &lt;- Sigma\n  }\n  \n  return(list(mu_filter, Sigma_filter))\n}\n\nLetâ€™s try it out on the Nile river data:\n\n# Analyzing the Nile river data\nprettycolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\")\ny = as.vector(Nile)\nV = 100^2\nW = 100^2\nmu0 = 1000\nSigma0 = 1000^2\n\n# Set up state-space model for local level model\nT = length(y)\nG = 1\nF = 1\nY = matrix(0,T,1)\nY[,1] = y\nfilterRes = kalmanfilter(Y, G, F, V, W, mu0, Sigma0)\nmeanFilter = filterRes[[1]]\nstd_filter = sqrt(filterRes[[2]][,,, drop =TRUE])\n\nplot(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\npolygon(c(seq(1:T), rev(seq(1:T))), \n        c(meanFilter - 1.96*std_filter, rev(meanFilter + 1.96*std_filter)), \n        col = \"#F0F0F0\", border = NA)\nlines(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\nlines(seq(1:T), meanFilter, type = \"l\", col = prettycolors[3], lwd = 1.5)\nlegend(\"topright\", legend = c(\"time series\", \"filter mean\", \"95% intervals\"), lty = 1, lwd = 1.5,\n    col = c(prettycolors[1], prettycolors[3], \"#F0F0F0\"))"
  },
  {
    "objectID": "tutorial/rektorsmedel.html",
    "href": "tutorial/rektorsmedel.html",
    "title": "Rektorsmedel 9",
    "section": "",
    "text": "Widgets kan\n\nplaceras pÃ¥ en webbsida: Maximum likelihood fÃ¶r Poissonmodellen\nvara en del av s k notebooks som Ã¤ven innehÃ¥ller text: Central grÃ¤nsvÃ¤rdessatsen\nanvÃ¤ndas fÃ¶r att gÃ¶ra dokument eller hela bÃ¶cker interaktiva: State-space models\nklistras in presentationer och anvÃ¤ndas interaktivt direkt under fÃ¶relÃ¤sning\nklistras in pÃ¥ lÃ¤roplattform Athena (som egentligen ocksÃ¥ Ã¤r en webblÃ¤sare)"
  },
  {
    "objectID": "homeassignment.html",
    "href": "homeassignment.html",
    "title": "Home assignment for Statistical Theory and Methods, 7.5 hp",
    "section": "",
    "text": "This is just a skeleton template for a course that will be given in the future.\n\nHome assignment \nMaterial: html"
  }
]