<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mattias Villani">

<title>Maximum likelihood by numerical optimization â€“ STM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//misc/favicons/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-fc3cd45728d25fc1a494926d8a114a58.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-e566e7d53b7ab26a57d70dd27f9c6c6b.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../misc/SU_logo_small.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">STM</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Overview</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../lectures.html"> 
<span class="menu-text">Lectures</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tutorials.html"> 
<span class="menu-text">Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../exercises.html"> 
<span class="menu-text">Exercises</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../interactive_exercises.html"> 
<span class="menu-text">Interactive exercises</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../computerlabs.html"> 
<span class="menu-text">Computer Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../homeassignment.html"> 
<span class="menu-text">Home assignment</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../coding.html"> 
<span class="menu-text">Programming</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sections</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a></li>
  <li><a href="#the-likelihood-function-and-maximum-likelihood-for-the-poisson-model" id="toc-the-likelihood-function-and-maximum-likelihood-for-the-poisson-model" class="nav-link" data-scroll-target="#the-likelihood-function-and-maximum-likelihood-for-the-poisson-model">The likelihood function and maximum likelihood for the Poisson model</a>
  <ul>
  <li><a href="#probability-of-the-observed-data---only-the-first-auction" id="toc-probability-of-the-observed-data---only-the-first-auction" class="nav-link" data-scroll-target="#probability-of-the-observed-data---only-the-first-auction">Probability of the observed data - only the first auction</a></li>
  <li><a href="#probability-of-the-observed-data---first-two-auctions" id="toc-probability-of-the-observed-data---first-two-auctions" class="nav-link" data-scroll-target="#probability-of-the-observed-data---first-two-auctions">Probability of the observed data - first two auctions</a></li>
  <li><a href="#probability-of-the-observed-data---all-auctions" id="toc-probability-of-the-observed-data---all-auctions" class="nav-link" data-scroll-target="#probability-of-the-observed-data---all-auctions">Probability of the observed data - all auctions</a></li>
  </ul></li>
  <li><a href="#derivation-of-the-ml-estimator-and-its-standard-error" id="toc-derivation-of-the-ml-estimator-and-its-standard-error" class="nav-link" data-scroll-target="#derivation-of-the-ml-estimator-and-its-standard-error">Derivation of the ML estimator and its standard error</a></li>
  <li><a href="#numerical-optimization-to-find-the-ml-estimator-and-its-standard-error" id="toc-numerical-optimization-to-find-the-ml-estimator-and-its-standard-error" class="nav-link" data-scroll-target="#numerical-optimization-to-find-the-ml-estimator-and-its-standard-error">Numerical optimization to find the ML estimator and its standard error</a></li>
  <li><a href="#approximate-sampling-distribution-of-the-ml-estimator-in-large-samples" id="toc-approximate-sampling-distribution-of-the-ml-estimator-in-large-samples" class="nav-link" data-scroll-target="#approximate-sampling-distribution-of-the-ml-estimator-in-large-samples">Approximate sampling distribution of the ML estimator in large samples</a></li>
  <li><a href="#multi-parameter-models" id="toc-multi-parameter-models" class="nav-link" data-scroll-target="#multi-parameter-models">Multi-parameter models</a>
  <ul>
  <li><a href="#poisson-regression-with-one-covariate" id="toc-poisson-regression-with-one-covariate" class="nav-link" data-scroll-target="#poisson-regression-with-one-covariate">Poisson regression with one covariate</a></li>
  <li><a href="#poisson-regression-with-two-covariates" id="toc-poisson-regression-with-two-covariates" class="nav-link" data-scroll-target="#poisson-regression-with-two-covariates">Poisson regression with two covariates</a></li>
  <li><a href="#poisson-regression-with-arbitrary-number-of-covariates" id="toc-poisson-regression-with-arbitrary-number-of-covariates" class="nav-link" data-scroll-target="#poisson-regression-with-arbitrary-number-of-covariates">Poisson regression with arbitrary number of covariates</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Maximum likelihood by numerical optimization</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mattias Villani </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>In this tutorial you will learn how maximum likelihood estimates and standard errors can be computed by numerical optimization routines in R. We learn about a general way to compute a normal approximation of the sampling distribution of the maximum likelihood estimator, which can be proved to be accurate in large samples, but is typically surprisingly accurate also for smaller sample sizes.</p>
</blockquote>
<p>It will take some work to get to the end of the document, but by the end of it you will have learned invaluable tools for a statistician/data scientist/machine learner giving you the super-power ðŸ’ª to use the computer to estimate the parameters and their uncertainty in quite complex models.</p>
<p>We will start with simple models with a single parameter to cover all the concepts, and then move on to the practically more important multi-parameter case.</p>
<p>Letâ€™s first load some useful libraries (install them using <code>install.packages()</code> if you havenâ€™t already).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp) <span class="co"># for plotting mathematical symbols (LaTeX)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(remotes)   <span class="co"># for loading packages from GitHub</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)   <span class="co"># for fancy plotting</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)   <span class="co"># the multivariate normal distribution</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>We will use the dataset <code>ebaycoins</code> in the R package <code>SUdatasets</code> for illustration. The dataset contains data from 1000 eBay auctions of collectorâ€™s coins<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Letâ€™s load the dataset and have a look:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#install_github("StatisticsSU/SUdatasets") # uncomment if this is not installed</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"SUdatasets"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ebaycoins)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  BookVal MinorBlem MajorBlem PowerSeller IDSeller Sealed NegFeedback
1   18.95         0         0           0        0      0           0
2   43.50         0         0           1        0      0           0
3   24.50         0         0           1        0      0           0
4   34.50         1         0           0        0      0           0
5   99.50         0         0           0        0      0           1
6    9.50         0         0           0        0      0           0
  ReservePriceFrac NBidders FinalPrice
1        0.3688654        2      15.50
2        0.2298851        6      41.00
3        1.0200000        1      24.99
4        0.7217391        1      24.90
5        0.1672362        4      72.65
6        1.2094737        2      17.52</code></pre>
</div>
</div>
<p>Each auction (rows in the dataset) will be taken as an observation, and the dataset has the following variables:</p>
<ul>
<li><p>the number of bidders in each auction (<code>NBidders</code>)</p></li>
<li><p>the final price (<code>FinalPrice</code>)</p></li>
<li><p>the book value of the coin according to a coin collector catalogue (<code>BookVal</code>).</p></li>
<li><p>the sellerâ€™s reservation price (lowest price that the seller is willing to sell for) as a fraction the book value (<code>ReservePriceFrac</code>).</p></li>
<li><p>binary variables on whether or not the seller is a verified ebay seller (<code>IDSeller</code>), sells large quantites (<code>PowerSeller</code>) and if the seller has many reviews with negative feedback (<code>NegFeedback</code>)</p></li>
<li><p>binary variables with information about the condition of the object: if it has a minor blemish (<code>MinorBlem</code>), a major one (<code>MajorBlem</code>), or sold in its original unbroken packaging (<code>Sealed</code>).</p></li>
</ul>
<p>We will intially analyze only the variable <code>NBidders</code> , the number of bidders in each auction. A histogram of the data are plotted below.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">prop.table</span>(<span class="fu">table</span>(ebaycoins<span class="sc">$</span>NBidders)))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"NBidders"</span>, <span class="st">"Proportions"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> NBidders, <span class="at">y =</span> Proportions)) <span class="sc">+</span> </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">fill =</span> <span class="st">"steelblue"</span>) <span class="sc">+</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span>percent) <span class="sc">+</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"NBidders"</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Proportion"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>   <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="co"># Use a minimal theme</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.grid.major =</span> <span class="fu">element_blank</span>(), <span class="co"># Remove major gridlines</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel.grid.minor =</span> <span class="fu">element_blank</span>()  <span class="co"># Remove minor gridlines</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="numericalML_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-likelihood-function-and-maximum-likelihood-for-the-poisson-model" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-likelihood-function-and-maximum-likelihood-for-the-poisson-model">The likelihood function and maximum likelihood for the Poisson model</h3>
<p>Since <code>NBidders</code> is a count variable, a natural first model to consider is the <strong>Poisson model</strong>:</p>
<p><span class="math display">\[
Y_1,\ldots,Y_n \vert \lambda \overset{\mathrm{iid}}{\sim}\mathrm{Poisson}(\lambda)
\]</span></p>
<p>where we use the symbol <span class="math inline">\(Y\)</span> for the random variable <code>NBidders</code> and <span class="math inline">\(y\)</span> as the observed value. The Poisson has one particularly noteworthy property: the parameter <span class="math inline">\(\lambda\)</span> is both the mean <em>and</em> the variance, i.e.&nbsp;if <span class="math inline">\(X\sim\mathrm{Poisson}(\lambda)\)</span>, then <span class="math inline">\(E(X)=V(X)=\lambda\)</span>. This makes the Poisson model suitable for count data where the mean and variance are approximately equal, which is not true for all datasets.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can get to know the Poisson distribution <span class="math inline">\(P(X=x)\)</span> by changing its parameter <span class="math inline">\(\lambda\)</span> with the slider below. Hover over the bars in the plot if you want to see the exact probabilities. The darker shaded bars represent the distribution function, i.e.&nbsp;probabilities of the form <span class="math inline">\(P(X\leq x)\)</span>.</p>
</div>
</div>
<iframe width="100%" height="727" frameborder="0" src="https://observablehq.com/embed/@mattiasvillani/poisson-distribution@792?cells=viewof+params%2Cdist_quantile%2Cmoments%2Cplt_pdf"></iframe>
<p>We can estimate <span class="math inline">\(\lambda\)</span> by the <strong>maximum likelihood</strong> (<strong>ML</strong>) metod. The ML method finds the value for <span class="math inline">\(\lambda\)</span> in the Poisson distribution that maximizes the probability of the observed dataset <span class="math inline">\(y_1,\ldots,y_n\)</span>.</p>
<section id="probability-of-the-observed-data---only-the-first-auction" class="level5">
<h5 class="anchored" data-anchor-id="probability-of-the-observed-data---only-the-first-auction">Probability of the observed data - only the first auction</h5>
<p>To break this down, let us start with the probability of observing the number of bidders in the <em>first</em> auction <span class="math inline">\(y_1\)</span> (which was <code>Nbidders = 2</code>, see the dataset above). Assuming a Poisson distribution, this probability is given by the formula (where <span class="math inline">\(y_1\)</span> is the observed number of bidders in the first auction): <span class="math display">\[
P(y_1 \vert \lambda) = \frac{\lambda^{y_1}e^{-\lambda}}{y_1!}
\]</span></p>
<p>where <span class="math inline">\(n!=n(n-1)(n-2)\cdot 2 \cdot 1\)</span> is the usual factorial and <span class="math inline">\(P(y_1 \vert \lambda)=\mathrm{Pr}(Y_1=y_1 \vert \lambda)\)</span>. Now, assume that <span class="math inline">\(\lambda = 3\)</span>. The probability of observing the number of bidders in the first auction (<code>Nbidders = 2</code>) is then <span class="math display">\[
P(y_1 = 2 \vert \lambda = 3) = \frac{3^{2}e^{-3}}{2!} = 0.2240418
\]</span> We can let R do this calculation for us using the <code>dpois</code> function</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2240418</code></pre>
</div>
</div>
<p>If we instead had <span class="math inline">\(\lambda = 2\)</span>, this probability changes to</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2706706</code></pre>
</div>
</div>
<p>This is higher, so <span class="math inline">\(\lambda = 2\)</span> agrees better with observing <code>Nbidders = 2</code> in the first auction.</p>
</section>
<section id="probability-of-the-observed-data---first-two-auctions" class="level5">
<h5 class="anchored" data-anchor-id="probability-of-the-observed-data---first-two-auctions">Probability of the observed data - first two auctions</h5>
<p>What if we look at the first <em>two</em> auctions where we have observed <span class="math inline">\(y_1 = 2\)</span> and <span class="math inline">\(y_2 = 6\)</span>. What is the <em>joint</em> probability of observing <span class="math inline">\(y_1 = 2\)</span> <em>and</em> <span class="math inline">\(y_2 = 6\)</span> when <span class="math inline">\(\lambda = 2\)</span>? If we assume that the number of bidders in the different auctions are independent, then we known that this probability is the product of the individual probabilities: <span class="math display">\[
P(y_1 = 2,y_2 = 6 \vert \lambda = 2)= P(y_1 = 2 \vert \lambda = 2)P(y_2 = 6 \vert \lambda = 2) = \frac{2^{2}e^{-2}}{2!} \frac{2^{6}e^{-2}}{6!}
\]</span> Rather than pulling out our calculators, let R do the work for us</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">2</span>)<span class="sc">*</span><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">6</span>, <span class="at">lambda =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.003256114</code></pre>
</div>
</div>
<p>This probability is of course lower than the probability of only observing the first auction (the probability of an intersection <span class="math inline">\(A \cap B\)</span> is always smaller or equal to the probability of each event <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>; think Venn diagram), but it is the <em>relative</em> probabilities for different <span class="math inline">\(\lambda\)</span> that interests us<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. For example, the probability of observing <span class="math inline">\(y_1 = 2\)</span> and <span class="math inline">\(y_2 = 6\)</span> when <span class="math inline">\(\lambda = 3\)</span> is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">3</span>)<span class="sc">*</span><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">6</span>, <span class="at">lambda =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.01129381</code></pre>
</div>
</div>
<p>which is more than three times larger than when <span class="math inline">\(\lambda=2\)</span>. So when we added another observation <span class="math inline">\(y_2 = 6\)</span>, the data started to â€˜preferâ€™ a higher <span class="math inline">\(\lambda\)</span>. The reason is that <span class="math inline">\(y_2 = 6\)</span> is a rather high count that is more likely to be observed if <span class="math inline">\(\lambda = 3\)</span> than if <span class="math inline">\(\lambda = 2\)</span> (remember <span class="math inline">\(\lambda\)</span> is the mean of the Poisson distribution). Now is a good time to go back to the Poisson distribution widget above, change the slider from <span class="math inline">\(\lambda = 2\)</span> to <span class="math inline">\(\lambda = 3\)</span> and look at how the probabilities of <span class="math inline">\(Y=2\)</span> and <span class="math inline">\(Y=6\)</span> change.</p>
<p>But what is the <em>optimal</em> <span class="math inline">\(\lambda\)</span> for these two observations, i.e.&nbsp;which value of <span class="math inline">\(\lambda\)</span> gives the <em>highest</em> joint probability for the observed <span class="math inline">\(y_1 = 2\)</span> and <span class="math inline">\(y_2 = 6\)</span>? We can explore this by plotting the joint probability <span class="math inline">\(P(y_1 = 2,y_2 = 6 \vert \lambda)\)</span> <strong>as a function</strong> of <span class="math inline">\(\lambda\)</span>. We need to compute this probability for a range of <span class="math inline">\(\lambda\)</span> values in a <code>for</code> loop:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>nlambdas <span class="ot">=</span> <span class="dv">2000</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="at">length =</span> nlambdas)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>jointprob <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(nlambdas))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nlambdas){</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  jointprob[i] <span class="ot">=</span> <span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda=</span>lambdas[i])<span class="sc">*</span><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">6</span>,<span class="at">lambda=</span>lambdas[i])</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambdas, jointprob, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"orange"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">TeX</span>(r<span class="st">'($\lambda$)'</span>), </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Probability of two bidders in auction 1 and six bidders in auction 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="numericalML_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Very nice! We can see that <span class="math inline">\(\lambda = 4\)</span> gives the highest probability to the observed number of bidders in the first two auctions. The function plotted above is called the <strong>likelihood function</strong> (for the first two observations).</p>
</section>
<section id="probability-of-the-observed-data---all-auctions" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="probability-of-the-observed-data---all-auctions">Probability of the observed data - all auctions</h5>
<p>It is now easy to see that the likelihood function for all <span class="math inline">\(n=1000\)</span> auctions is obtained by multiplying the Poisson probabilities for all <span class="math inline">\(n=1000\)</span> auctions:</p>
<p><span class="math display">\[
P(y_1,y_2,\ldots,y_n \vert \lambda)=\prod_{i=1}^n P(y_i \vert \lambda)
\]</span></p>
<p>where <span class="math inline">\(P(y_i \vert \lambda)\)</span> is the Poisson probability for observing <span class="math inline">\(y_i\)</span> bidders in auction <span class="math inline">\(i\)</span> in the dataset, if the parameter has the value <span class="math inline">\(\lambda\)</span>. You should note that the likelihood function is the probability of the observed data <strong>viewed as a function of the parameter</strong> (like we did with the orange line above). The data observations are observed and fixed (hence the lower case letters <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span>) and the likelihood function explores how the joint probability of the observed data varies as we change <span class="math inline">\(\lambda\)</span>.</p>
<p>The <strong>maximum likelihood</strong> (<strong>ML</strong>) method for parameter estimation sets the parameter <span class="math inline">\(\lambda\)</span> to the value that maximizes the probability of observed data, i.e.&nbsp;the <span class="math inline">\(\lambda\)</span> that <strong>maximizes the likelihood function</strong>. This method can be used in in <strong>any</strong> probability model, not just the Poisson model. Knowing the ML method gives you superpowers! ðŸ’ª</p>
<blockquote class="blockquote">
<p>- This model is too complex, it is impossible to estimate its parameters!<br>
- ML: hold my beer.</p>
</blockquote>
<p>When we start to multiply the probabilities for all data points, the joint probability becomes tiny, and we run into numerical problems. We there often choose to plot and maximize the logarithm of the likelihood, the so called <strong>log-likelihood function</strong></p>
<p><span class="math display">\[
l(\lambda)=\log P(y_1,y_2,\ldots,y_n \vert \lambda)=\sum_{i=1}^n \log P(y_i \vert \lambda)
\]</span></p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div id="fig-logarithm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logarithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="numericalML_files/figure-html/fig-logarithm-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logarithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The logarithm is a monotonically increasing function.
</figcaption>
</figure>
</div>
</div></div></div>
<p>I have given the log-likelihood a symbol <span class="math inline">\(l(\lambda)\)</span> to really make it clear that we want to view this as a function of the parameter <span class="math inline">\(\lambda\)</span>. This function does also depend on the observed data <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span>, but this is not written out explicitly in the symbol <span class="math inline">\(l(\lambda)\)</span>, to reduce the clutter. Since the logarithm is a <em>monotonically increasing function</em> (see plot in margin), it does not matter if we maximize the likelihood or the log-likelihood. We get exactly the same estimate of <span class="math inline">\(\lambda\)</span>. And logarithms are also often more easy to deal with mathematically. Letâ€™s verify that we find the same maximum if we plot the log-likelihood function for the first two auctions (note how I use the <code>log=TRUE</code> argument in <code>dpois</code> to get the log of the Poisson probabilities, which I now add):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>nlambdas <span class="ot">=</span> <span class="dv">2000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="at">length =</span> nlambdas)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>loglik <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(nlambdas))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nlambdas){</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  loglik[i] <span class="ot">=</span> <span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> lambdas[i], <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">6</span>, <span class="at">lambda =</span> lambdas[i], <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambdas, loglik, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"orange"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylab =</span> <span class="st">"log-likelihood"</span>,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">TeX</span>(r<span class="st">'($\lambda$)'</span>), </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"log-likelihood two bidders in auction 1 and six bidders in auction 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="numericalML_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We are now ready to plot the log-likelihood function based on all <span class="math inline">\(n=1000\)</span> observations (auctions). It seems like a real mess to add terms like <code>dpois(x = 6, lambda = lambdas[i], log = TRUE)</code> for each of the 1000 observations. One solution is to use a <code>for</code> loop, but we can do even better (loops are slow in R). The <code>dpois</code> function is <em>vectorized</em>, meaning that it can compute the probabilties for <em>all</em> observations in one go. Letâ€™s try it for the first two auctions and <span class="math inline">\(\lambda = 2\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first one observation at the time. first obs:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">2</span>, <span class="at">lambda =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2706706</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># seconds obs</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="dv">6</span>, <span class="at">lambda =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0120298</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and now using that dpois accepts a vector c(2,6) as first argument</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">6</span>), <span class="at">lambda =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2706706 0.0120298</code></pre>
</div>
</div>
<p>Note how the call <code>dpois(x = c(2,6), lambda = 2)</code> returns a vector where each element is a probability for the corresponding observation. This also works with the <code>log = TRUE</code> argument</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">6</span>), <span class="at">lambda =</span> <span class="dv">2</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -1.306853 -4.420368</code></pre>
</div>
</div>
<p>so we can obtain the log-likelihood for all <span class="math inline">\(n=1000\)</span> observations by just summing this vector:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>loglik_vect <span class="ot">=</span> <span class="fu">dpois</span>(<span class="at">x =</span> ebaycoins<span class="sc">$</span>NBidders, <span class="at">lambda =</span> <span class="dv">2</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(loglik_vect)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2974.806</code></pre>
</div>
</div>
<p>Sweet! So now we can compute the log-likelihood for all the data over a grid of <code>lambda</code>-values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>nlambdas <span class="ot">=</span> <span class="dv">2000</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="at">length =</span> nlambdas)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>loglik <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(nlambdas))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nlambdas){</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  loglik[i] <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(<span class="at">x =</span> ebaycoins<span class="sc">$</span>NBidders, <span class="at">lambda =</span> lambdas[i], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An alternative (faster) way of computing the log-likelihood function that avoids the <code>for</code>-loop is by using the <code>sapply</code> function in R. We first define a function that compute the log-likelihood for any given <code>lambda</code> and input and then let <code>sapply</code> evaluate that function over a vector of <code>lambda</code>-values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>loglikfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda){</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fu">dpois</span>(<span class="at">x =</span> ebaycoins<span class="sc">$</span>NBidders, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>nlambdas <span class="ot">=</span> <span class="dv">2000</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="at">length =</span> nlambdas)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>loglik <span class="ot">=</span> <span class="fu">sapply</span>(lambdas, loglikfunc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ok, letâ€™s now plot the log-likelihood function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambdas, loglik, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"orange"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylab =</span> <span class="st">"log-likelihood"</span>,</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">TeX</span>(r<span class="st">'($\lambda$)'</span>), <span class="at">main =</span> <span class="st">"log-likelihood for all 1000 auctions"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="numericalML_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It is a little tricky to read of which <span class="math inline">\(\lambda\)</span> value gives the maximum log-likelihood, but we can look for the maximal log-likelihood among the 2000 <span class="math inline">\(\lambda\)</span> values that we used to compute the function values in the graph:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>lambdas[<span class="fu">which.max</span>(loglik)] <span class="co"># which.max finds the position of loglik with the max</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.633187</code></pre>
</div>
</div>
<p>which should be really close to the true maximum likelihood estimate.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>The interactive graph below lets you explore how different <span class="math inline">\(\lambda\)</span> values give a different fit to the data (the model fit is the orange line in the left graph, which ideally should match the histogram of the data). The graph to the right shows the log-likelihood function. The orange dot marks out the log-likelihood value for the <span class="math inline">\(\lambda\)</span> value chosen with the slider. By clicking the checkbox you will see the fit based on the ML estimate. Note that the Poisson model is a bad fit to this data, even with the ML estimate. We will improve on this later on when we turn to Poisson regression models.</p>
</div>
</div>
<iframe width="100%" height="887" frameborder="0" src="https://observablehq.com/embed/@mattiasvillani/multivariate-normal-distribution@267?cells=moments%2Cviewof+param%2Cplttrue"></iframe>
<p>Ok, but do we really need to compute the log-likelihood function for a bunch of <span class="math inline">\(\lambda\)</span> values to determine where the maximum is? Fortunately no, and there are two smarter ways of getting the ML estimate:</p>
<ol type="1">
<li><p>Use methods from mathematical analysis to obtain the ML estimator as a nice little formula.</p></li>
<li><p>Use numerical maximization in R to let the computer find the ML estimate for us.</p></li>
</ol>
<p>Option 1 is nice (once the maths is done ðŸ¥µ) but is usually only possible in simpler models.<br>
Option 2 unlocks your superpowers to easily find the ML estimate in essentially any model you like. ðŸ˜</p>
</section>
</section>
<section id="derivation-of-the-ml-estimator-and-its-standard-error" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="derivation-of-the-ml-estimator-and-its-standard-error">Derivation of the ML estimator and its standard error</h3>
<p>But letâ€™s torture ourselves a bit and derive the ML estimator for the Poisson model mathematically. Here we go.</p>
<p>We need to start with writing out the log-likelihood function</p>
<p><span class="math display">\[l(\lambda) = \log P(y_1,y_2,\ldots,y_n \vert \lambda) = \sum_{i=1}^n \log P(y_i \vert \lambda)\]</span></p>
<p>for the Poisson model. In the case where the data comes from a Poisson distribution, the probability function for one observation is given by <span class="math display">\[P(y) = \frac{e^{-\lambda}\lambda^y}{y!}\]</span> Therefore,</p>
<p><span class="math display">\[\log P(y) = -\lambda + y\log\lambda - \log y!,\]</span> so the log-likelihood is a sum of these terms, one for each observation: <br><br></p>
<p><span class="math display">\[l(\lambda) = \sum_{i=1}^n\left(-\lambda + y_i\log\lambda - \log(y_i !)\right) = -n\lambda + \log\lambda \sum_{i=1}^n y_i - \sum_{i=1}^n \log(y_i !)\]</span></p>
<p>We know from calculus that the maximum of a function <span class="math inline">\(f(x)\)</span> is found by setting its first derivative <span class="math inline">\(f'(x)\)</span> to zero and solving for <span class="math inline">\(x\)</span>. The first derivative is simply:</p>
<p><span class="math display">\[l'(\lambda) = \frac{d}{d\lambda}l(\lambda) = -n + \frac{\sum_{i=1}^n y_i}{\lambda} = 0\]</span></p>
<p>which gives the solution <span class="math display">\[\lambda = \frac{\sum_{i=1}^n y_i}{n} = \bar y.\]</span></p>
<p>To verify that this is indeed a maximum, we can check that the second derivative is negative at <span class="math inline">\(\lambda = \bar y\)</span>. The second derivative is</p>
<p><span class="math display">\[l''(\lambda)=\frac{d^2}{d\lambda^2}l(\lambda) = - \frac{\sum_{i=1}^n y_i}{\lambda^2},\]</span></p>
<p>which is negative for all <span class="math inline">\(\lambda\)</span> because both the data and <span class="math inline">\(\lambda\)</span> must be positive.</p>
<p>Thus, the maximum likelihood estimate for the parameter <span class="math inline">\(\lambda\)</span> in the Poisson model is simply the mean, given by <span class="math inline">\(\hat\lambda = \bar y\)</span>. ðŸ¥³</p>
<p>Letâ€™s check if the formula gives the same result as we obtained by plotting:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(ebaycoins<span class="sc">$</span>NBidders)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.635</code></pre>
</div>
</div>
<p>Yup, checks out.</p>
<p>A nice thing with having a formula for the estimator is that we can then try to compute how much the estimator can vary from sample to sample, i.e.&nbsp;computing the standard deviation of the sampling distribution of <span class="math inline">\(\hat\lambda = \bar Y\)</span>. Note here that we use a capital letter <span class="math inline">\(\bar Y\)</span> since we are now considering the data as random variables <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span>, before even observing a given sample <span class="math inline">\(y_1,y_2,\ldots,y_n\)</span>. Let us first derive the mean of the ML estimator</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Remember that for a random variable <span class="math inline">\(X\)</span> we have</p>
<p><span class="math display">\[
\mathrm{E}(aX)=a\mathrm{E}(X)\\
\]</span> and for any random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> (independent or dependent) we have</p>
<p><span class="math display">\[
\mathrm{E}(\sum_{i=1}^n X_i) = \sum_{i=1}^n\mathrm{E}(X_i)
\]</span></p>
</div></div><p><span class="math display">\[
\mathrm{E}(\hat\lambda)=\mathrm{E}(\bar Y)=\mathrm{E}\Bigg(\frac{\sum_{i=1}^n Y_i}{n}\Bigg)=\frac{1}{n}\Big(\sum_{i=1}^n \mathrm{E}( Y_i)\Big)=\frac{1}{n}\Big(\sum_{i=1}^n \lambda\Big)=\frac{1}{n}n\lambda=\lambda
\]</span></p>
<p>where we have used that <span class="math inline">\(\mathrm{E}(\mathrm{Y})=\lambda\)</span> for a Poisson variable. This shows that the ML estimator is <strong>unbiased</strong> (which is not always true for ML estimators, except in large samples). The variance of the sampling distribution is: <span class="math display">\[
\mathrm{Var}(\hat\lambda) = \mathrm{Var}(\bar Y) = \mathrm{Var}\Bigg(\frac{\sum_{i=1}^n Y_i}{n}\Bigg) = \frac{1}{n^2}\mathrm{Var}\big(\sum_{i=1}^nY_i\big) = \frac{1}{n^2}n\mathrm{Var}\big(Y_i\big) =  \frac{\lambda}{n}
\]</span> where the last equality uses that the variance of a <span class="math inline">\(\mathrm{Pois}(\lambda)\)</span> variable is <span class="math inline">\(\lambda\)</span> (remember for Poisson both the mean and variance are <span class="math inline">\(\lambda\)</span>).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Remember that for a random variable <span class="math inline">\(X\)</span> we have</p>
<p><span class="math display">\[
\mathrm{Var}(aX)=a^2\mathrm{Var}(X)\\
\]</span> and for independent random variables <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> we have <span class="math display">\[
\mathrm{Var}(\sum_{i=1}^nX_i) = \sum_{i=1}^n\mathrm{Var}(X_i)
\]</span></p>
</div></div><p>The standard deviation is of course then <span class="math display">\[
S(\hat \lambda) = \sqrt{\frac{\lambda}{n}}
\]</span> The standard deviation in the sampling distribution of an estimator has its own name: the <strong>standard error</strong>. This is so that we should remember that a sampling distribution is not just any old distribution. The sampling distribution of an estimator (or some other function of the dataset) describes the distribution of the estimator as we take a large number (infinite!) repeated samples from the population. For a given observed dataset we get an estimate <span class="math inline">\(\hat\lambda = \bar y\)</span> of <span class="math inline">\(\lambda\)</span>, a single number. But another sample from the population would have given another estimate. The sampling distribution shows the distribution of all possible estimates that we could have obtained from the different samples.</p>
<p>Ok, the standard error formula above is nice, but how can we use it? It depends on <span class="math inline">\(\lambda\)</span>, which we donâ€™t know. The easy fix is to replace <span class="math inline">\(\lambda\)</span> with the best guess we have: the ML estimate <span class="math inline">\(\hat\lambda = \bar y\)</span>. This gives us an estimate of the standard error of the ML estimator <span class="math display">\[
SE(\hat \lambda) = \sqrt{\frac{\bar y}{n}}
\]</span></p>
<p>We can now compute the standard error for the ebaycoins data</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(ebaycoins<span class="sc">$</span>NBidders)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>lambda_hat <span class="ot">=</span> <span class="fu">mean</span>(ebaycoins<span class="sc">$</span>NBidders)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(lambda_hat<span class="sc">/</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06029096</code></pre>
</div>
</div>
<p>which represents an estimate of the â€˜typicalâ€™ variation of the ML estimator from sample to sample.</p>
<p>We can use this standard error to compute an approximate 95% confidence interval (assuming an approximate normal sampling distribution; since <span class="math inline">\(n=1000&gt;30\)</span> we can use the central limit theorem to motivate this). The interval is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(lambda_hat <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(lambda_hat<span class="sc">/</span>n), lambda_hat <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(lambda_hat<span class="sc">/</span>n))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.51683 3.75317</code></pre>
</div>
</div>
</section>
<section id="numerical-optimization-to-find-the-ml-estimator-and-its-standard-error" class="level3">
<h3 class="anchored" data-anchor-id="numerical-optimization-to-find-the-ml-estimator-and-its-standard-error">Numerical optimization to find the ML estimator and its standard error</h3>
<p>We will now learn how R (or any other programming language for data analysis) can find the ML estimate by numerically maximizing the log-likelihood function. For this we donâ€™t have to do the mathematical derivation that we did before, which is very important for more complex models where the maths is hard or even impossible. All R needs to know about is the log-likelihood function itself, so we need to code up such a function. This is quite easy for most models.</p>
<p>Let us define an R function that takes the data <code>y</code> and the parameter <code>lambda</code> as inputs and returns the log-likelihood value <code>loglik</code> as output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>loglik_pois <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda, y){</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>  loglik <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(y, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loglik)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can try it out with some values for <span class="math inline">\(\lambda\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_pois</span>(<span class="at">lambda =</span> <span class="dv">2</span>, <span class="at">y =</span> ebaycoins<span class="sc">$</span>NBidders)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2974.806</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_pois</span>(<span class="at">lambda =</span> <span class="dv">3</span>, <span class="at">y =</span> ebaycoins<span class="sc">$</span>NBidders)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2500.94</code></pre>
</div>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_pois</span>(<span class="at">lambda =</span> <span class="fu">mean</span>(ebaycoins<span class="sc">$</span>NBidders), <span class="at">y =</span> ebaycoins<span class="sc">$</span>NBidders)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2438.032</code></pre>
</div>
</div>
<p>where the last value is for the ML estimate <span class="math inline">\(\hat\lambda = \bar y\)</span>.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
First argument must be the parameter that you optimize
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is crucial that the very first argument in your log-likelihood function is the parameter that you want to find ML estimate for. So you have to write <code>function(lambda, y)</code> and <strong>not</strong> <code>function(y, lambda)</code> in the function definition. As usual in programming, the exact name <code>lambda</code> is not required, but whatever you call the model parameter, it has to go first in the function definition.</p>
</div>
</div>
<p>Now that we have told R about the function that we want to maximize, we can use the <code>optim</code> function to do the maximization (optimization is the term used for both finding the minimum or the maximum of a function). Here is the code and output, which I will now explain.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_pois, <span class="at">gr=</span><span class="cn">NULL</span>, ebaycoins<span class="sc">$</span>NBidders, </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>optres<span class="sc">$</span>par</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.635</code></pre>
</div>
</div>
<p>First, note that the result from the optimization is return to a list, which I have named <code>optres</code>. The slot <code>optres$par</code> contains the value for <code>lambda</code> that maximizes the log-likelihood function. It agrees with our earlier estimate. Good, but what on earth are all those arguments to the <code>optim</code> function? Let me tell you:</p>
<ul>
<li>the first argument, which I have named <code>initVal</code>, is the initial value that R starts out with. The optim function then finds the maximum by iteratively moving this initial value to a better and better value (higher log-likelihood). For simple problems, this choice is not so important (try to change it in the Quarto notebook), but for models with a large number of parameters it can matter. The better the initial guess, the quicker the algorithm will reach the maximum.</li>
<li>the second argument, <code>loglik_pois</code>, is the name of the function that you want to maximize.</li>
<li>the third argument just says that we donâ€™t know the derivative of the log-likelihood function. If we would supply this, the algorithm would be even quicker, but most of the time we just say it is <code>NULL</code>.</li>
<li>the arguments following the third argument, in this case only <code>ebaycoins$NBidders</code>, are all the things that R needs to evaluate the log-likelihood function <code>loglik_pois</code>. We only have one such argument, the data variable <code>y</code> which in our case here is the vector with the number of bidders in each auction, <code>ebaycoins$NBidders</code>. It is common to have more than one such variable, and then you just keep adding them with a comma (,) between each argument. We will see an example of this later when we look at Poission regression where we also need to supply the covariate data.</li>
<li>the very cryptic argument <code>control=list(fnscale=-1)</code> tells R that we want to maximize rather than minimize. The reason is that the optim function does minimization by default, and <code>control=list(fnscale=-1)</code> tells R to multiply the log-likelihood function by <span class="math inline">\(-1\)</span> which flips the function upside down, turning the minimization into a maximization. It is a little hacky, so just get used to always have this argument when you want to <em>maximize</em>, rather than minimize.</li>
<li>the final two arguments <code>method=c("BFGS")</code> and <code>hessian=TRUE</code> tell R to use a specific optimization method that also allows us to obtain things we need to approximate the standard error of the ML estimator. More on this below.</li>
</ul>
</section>
<section id="approximate-sampling-distribution-of-the-ml-estimator-in-large-samples" class="level3">
<h3 class="anchored" data-anchor-id="approximate-sampling-distribution-of-the-ml-estimator-in-large-samples">Approximate sampling distribution of the ML estimator in large samples</h3>
<p>We now know how to find the ML estimate for any model for which we can code up the log-likelihood function. Pretty cool! But we can do more! In large samples we can approximate the sampling distribution of the ML estimator <span class="math inline">\(\hat\lambda\)</span> in essentially any model with the following normal distribution <span class="math display">\[
\hat\lambda \overset{\mathrm{approx}}{\sim} N\Bigg(\lambda_0, \frac{1}{-l''(\lambda_0)}\Bigg)
\]</span> where <span class="math inline">\(\lambda_0\)</span> is the true parameter in the population model and <span class="math display">\[
l''(\lambda) = \frac{d^2}{d\lambda^2}l(\lambda)
\]</span> is the second derivate of the log-likelihood function <span class="math inline">\(l(\lambda)\)</span>. So <span class="math inline">\(l''(\lambda_0)\)</span> is the second derivative of the log-likelihood function evaluated in the point <span class="math inline">\(\lambda = \lambda_0\)</span>. This says that the sampling distribution of the ML estimator will be approximately normal in large samples (<span class="math inline">\(n\)</span> large) with a standard deviation of <span class="math display">\[
\frac{1}{\sqrt{-l''(\lambda_0)}}
\]</span> Since we donâ€™t know the <span class="math inline">\(\lambda_0\)</span> that generated the data we have to estimate it and the standard error is therefore <span class="math display">\[
\frac{1}{\sqrt{-l''(\hat \lambda)}}
\]</span> where <span class="math inline">\(\hat\lambda\)</span> is the ML estimate, which in our Poisson case is <span class="math inline">\(\hat\lambda = \bar y\)</span>. We were able to compute <span class="math inline">\(l''(\hat \lambda)\)</span> mathematically for the Poisson model above when we verified that <span class="math inline">\(\hat\lambda = \bar y\)</span> was indeed the maximal point. By the way, donâ€™t worry about the negative sign that makes it look like we are taking the square root of a negative number. The number under the square root sign is always positive, because <span class="math inline">\(l''(\hat \lambda)\)</span> is always negative (remember that we do check if a maximum was found by checking if the second derivative is negative at the maximum).</p>
<p>In summary, the following approximate sampling distribution of the ML estimator is very accurate in large samples (<span class="math inline">\(n&gt;30\)</span> is a rule of thumb) <span class="math display">\[
\hat\lambda \overset{\mathrm{approx}}{\sim} N\Bigg(\lambda_0, \frac{1}{-l''(\hat\lambda)}\Bigg)
\]</span></p>
<p>Why does the second derivative appear here at all? Is there any intuition for it? There is. Maybe you know that the first derivative <span class="math inline">\(f'(x)\)</span> of a function <span class="math inline">\(f(x)\)</span> measures the slope (tangent) of the function at the point <span class="math inline">\(x\)</span>. The second derivative measures <em>how fast the first derivative changes</em>. So when computing the second derivative at the maximum (mode) we get a measure of the <em>curvature</em> around the maximum of the log-likelihood, that is, how peaky the log-likelihood is there. So for a log-likelihood which is very peaky there is clearly a lot of information in the data to pin down a good estimate. A peaky function has a large second derivative (the first derivative changes rapidly around the mode) and <span class="math inline">\(\frac{1}{-l''(\hat\lambda)}\)</span> become very small, i.e.&nbsp;a small standard error.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>The widget below shows how the second derivative (which measures the speed of change in the first derivative) measures how peaky a function is. Two functions are shown: a peaky one (on top) and a non-peaky (bottom) one. The orange line represent the first derivative, which you can see changes rapidly in the peaky function when you change the point where the derivative is evaluated.</p>
</div>
</div>
<iframe width="100%" height="965" frameborder="0" src="https://observablehq.com/embed/4394f409c6a21a5e@181?cells=viewof+x0%2Cplt1%2Cplt2"></iframe>
<p>The beauty of it all is that <code>optim</code> can actually also compute the second derivative at the ML estimate, so <code>optim</code> gives all we need for computing the (approximate) standard error! This is where the arguments <code>method=c("BFGS")</code> and <code>hessian=TRUE</code> come into play. By using these two arguments, <code>optim</code> will also return the second derivative evaluated at the ML estimate: <span class="math inline">\(l''(\hat \lambda)\)</span> (hessian is basically another word for second derivative). Here is the complete code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_pois, <span class="at">gr=</span><span class="cn">NULL</span>, ebaycoins<span class="sc">$</span>NBidders, </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>mle <span class="ot">=</span> optres<span class="sc">$</span>par</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>mle_se <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="sc">-</span>optres<span class="sc">$</span>hessian[<span class="dv">1</span>])</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of lambda is"</span>, <span class="fu">round</span>(mle, <span class="dv">4</span>), </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">" with an approximate standard error of"</span>, <span class="fu">round</span>(mle_se,<span class="dv">4</span>) ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of lambda is 3.635  with an approximate standard error of 0.0603</code></pre>
</div>
</div>
<p>Note how the slot <code>optres$hessian</code> contains the second derivative at the ML estimate (the maximal point) and how well the approximate standard error agrees with the exact standard error derived above <span class="math display">\[
\sqrt{\frac{\bar y}{n}} = 0.0602909
\]</span> We can plot the approximate normal sampling distribution</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">=</span> <span class="fu">seq</span>(mle <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>mle_se, mle <span class="sc">+</span> <span class="dv">4</span><span class="sc">*</span>mle_se, <span class="at">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambdas, <span class="fu">dnorm</span>(lambdas, mle, mle_se), <span class="at">type =</span><span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"sampling distribution"</span>,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"orange"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="fu">TeX</span>(r<span class="st">'($ \hat{\lambda}$)'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="numericalML_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="multi-parameter-models" class="level3">
<h3 class="anchored" data-anchor-id="multi-parameter-models">Multi-parameter models</h3>
<section id="poisson-regression-with-one-covariate" class="level5">
<h5 class="anchored" data-anchor-id="poisson-regression-with-one-covariate">Poisson regression with one covariate</h5>
<p>We will now consider the case with models that have more than one parameter, which is the usual case in applied statistical work. As an example, consider the Poisson regression model where the response count <span class="math inline">\(y\)</span> is regressed on a single covariate <span class="math inline">\(x\)</span>: <span class="math display">\[
y_i \vert x_i \overset{\mathrm{indep}}{\sim} \mathrm{Pois}(\lambda_i) \text{, where }
\lambda_i = \exp(\beta_0 + \beta_1 x_i).
\]</span>Note that each observation <span class="math inline">\(y_i\)</span> now has its own Poisson mean <span class="math inline">\(\lambda_i\)</span> which is in turn modelled as a function of the covariate <span class="math inline">\(x_i\)</span>. We use the exponential function to make sure that <span class="math inline">\(\lambda_i&gt;0\)</span> for any <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and covariate values <span class="math inline">\(x_i\)</span>.</p>
<p>If you think that the above is a weird way of writing a regression, note that the usual linear regression with normal error terms <span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \text{, where } \varepsilon_i \overset{\mathrm{iid}}{\sim} N(0,\sigma^2)
\]</span><br>
can equally well be written <span class="math display">\[
y_i \vert x_i  \overset{\mathrm{indep}}{\sim} \mathrm{N}(\mu_i, \sigma^2) \text{, where }
\mu_i = \beta_0 + \beta_1 x_i.
\]</span> Here we donâ€™t need to mess around with exponential function since the mean in the normal distribution is allowed to also be negative. If you are wondering how one can write the Poisson regression with some sort of error term <span class="math inline">\(\varepsilon\)</span>, then stop wondering now. You canâ€™t!</p>
<p>Suppose we want to explain the number of bidders (<code>NBidders</code>) in an auction with the covariate <code>ReservePriceFrac</code> (the sellerâ€™s lowest acceptable price divided by the coinâ€™s book value). This seems like a sensible covariate since if you set a high reservation price you will most likely attract fewer bidders. We can still use the maximum likelihood idea: choose the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to maximize the probability of the observed number of bidders. As is usual in regression, the covariate <span class="math inline">\(x\)</span> is taken to be fixed, so we donâ€™t worry about the probability for this variable. Here we run into trouble: while we can compute the (partial) derivative of the log-likelihood for each of the two parameters, we canâ€™t solve the resulting two equations to find formulas for the ML estimators <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span>.</p>
<p>Any suggestions on how to proceed? Optim! Letâ€™s code up the log likelihood for the Poisson regression</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>loglik_poisreg <span class="ot">&lt;-</span> <span class="cf">function</span>(betavect, y, x){</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  lambda <span class="ot">=</span> <span class="fu">exp</span>(betavect[<span class="dv">1</span>] <span class="sc">+</span> betavect[<span class="dv">2</span>]<span class="sc">*</span>x)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  loglik <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(y, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loglik)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note in particular that we used a <em>vector</em> <code>betavect</code> as first argument, a vector containing both parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. As mentioned before, <code>optim</code> wants the first argument of the optimized function to be the parameter, and when the model contains more than one parameter, you need to pack them in a vector like I did here. You should also note that <code>lambda</code> is a vector with one <span class="math inline">\(\lambda_i\)</span> for each observation (auction) computed with the formula <span class="math inline">\(\lambda_i = \exp(\beta_0 + \beta_1 x_i)\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
name calling can hurt
</div>
</div>
<div class="callout-body-container callout-body">
<p>Donâ€™t use the name <code>beta</code> as variable name, since R already uses the name beta for the <code>Beta</code> function.</p>
</div>
</div>
<p>Alright, letâ€™s try out our function for a couple of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> values, just to see if it works:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_poisreg</span>(<span class="at">betavect =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">y =</span> ebaycoins<span class="sc">$</span>NBidders, </span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">x =</span> ebaycoins<span class="sc">$</span>ReservePriceFrac)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -3709.59</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_poisreg</span>(<span class="at">betavect =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>), <span class="at">y =</span> ebaycoins<span class="sc">$</span>NBidders, </span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">x =</span> ebaycoins<span class="sc">$</span>ReservePriceFrac)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -2857.133</code></pre>
</div>
</div>
<p>from which we see that the observed <span class="math inline">\(y\)</span> data in the <span class="math inline">\(1000\)</span> auctions is much more probable with the parameters <span class="math inline">\(\beta_0=1\)</span> and <span class="math inline">\(\beta_1=-1\)</span> than with the parameters <span class="math inline">\(\beta_0=1\)</span> and <span class="math inline">\(\beta_1=1\)</span>; this makes sense since we expect the number of bidders to decrease with larger <code>ReservePriceFrac</code>.</p>
<p>Letâ€™s throw this at the <code>optim</code> function to get the ML estimate:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> ebaycoins<span class="sc">$</span>ReservePriceFrac</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_poisreg, <span class="at">gr=</span><span class="cn">NULL</span>, y, x, <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>mle <span class="ot">=</span> optres<span class="sc">$</span>par</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta0 is"</span>, <span class="fu">round</span>(mle[<span class="dv">1</span>], <span class="dv">4</span>) ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta0 is 2.1077</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta1 is"</span>, <span class="fu">round</span>(mle[<span class="dv">2</span>], <span class="dv">4</span>) ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta1 is -1.7434</code></pre>
</div>
</div>
<p>Holy smoke, that is cool! Note that after the argument <code>gr=NULL</code> I have both <code>y</code> and <code>x</code> as the additional arguments needed to evaluate the <code>loglik_poisreg</code> function. The return value <code>optres$par</code> is a vector containing the two <span class="math inline">\(\beta\)</span> parameters.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>Explore how changing values for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> changes the fit of the Poisson regression by dragging the sliders below. The graph at the bottom shows the two-dimensional log-likelihood function in the form of a contour plot of <span class="math inline">\(l(\beta_0,\beta_1)\)</span>. You should think of the log-likelihood function like a mountain and each of the contours (the ellipses) as levels around the mountain of a specific fixed altitude (height). The smaller contours are higher up the mountain. The orange dot shows the log-likelihood of the Poisson regression fit chosen with the sliders.</p>
<p>By clicking on the checkbox â€œshow ML fitâ€ you get to see the optimal maximum likelihood fit (note that the log-likelihood for the ML fit is at the top of the mountain, as it should since this is literally the definition of ML).</p>
</div>
</div>
<iframe width="100%" height="963" frameborder="0" src="https://observablehq.com/embed/7b7b2cc340d0eed7@2215?cells=viewof+beta%2Cviewof+view_ml_reg%2Ctextslh_reg%2Cbothplots_reg"></iframe>
<p>What about the standard errors of <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span>? Can we approximate those similarly to the case with one model parameter? Yes! But we need the following more complicated result for the large-sample approximate sampling distribution, which will take a little of explaining. We need to know what a bivariate normal distribution is.</p>
<p>A bivariate normal distribution for two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is essentially a univariate normal distribution for each of the two random variables <span class="math inline">\(X\sim N(\mu_x,\sigma_x^2)\)</span> and <span class="math inline">\(Y\sim N(\mu_y,\sigma_y^2)\)</span> and a correlation coefficient <span class="math inline">\(\rho\)</span></p>
<p><span class="math display">\[
\rho = \frac{\mathrm{Cov}(X,Y)}{SD(X) \cdot SD(Y)}
\]</span>that determines the degree of dependence between the two variables. We write</p>
<p><span class="math display">\[
(X,Y) \sim N(\mu_x,\mu_y,\sigma_x,\sigma_y,\rho)
\]</span></p>
<p>which shows that the distribution has five parameters. The joint density for the bivariate normal distribution is given by the fairly complicated expression (that you shouldnâ€™t learn to memorize)</p>
<p><span class="math display">\[
\small
p(x,y)=\frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}\exp\Bigg(
-\frac{1}{2(1-\rho^2)} \bigg[
\Big( \frac{x-\mu_x}{\sigma_x} \Big)^2 + \Big( \frac{y-\mu_y}{\sigma_y} \Big)^2 -2\rho\Big( \frac{x-\mu_x}{\sigma_x} \Big) \Big( \frac{y-\mu_y}{\sigma_y} \Big)
\bigg]
\Bigg)
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interactive exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can experiment with the five parameters in the bivariate normal distribution at this interactive widget:<br>
<a href="https://observablehq.com/embed/@mattiasvillani/multivariate-normal-distribution@270?cells=moments%2Cviewof+param%2Cplttrue">Interactive bivariate normal distribution</a><br>
Note in particular what happens when you change the correlation coefficient parameter <span class="math inline">\(\rho\)</span> to positive and negative values.</p>
</div>
</div>
<p>An alternative way to express that two variables follow a bivariate normal distribution is</p>
<p><span class="math display">\[
(X,Y) \sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean <em>vector</em> that contains the two means</p>
<p><span class="math display">\[
\boldsymbol{\mu} = \pmatrix{\mu_y \\ \mu_y}
\]</span></p>
<p>and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the <strong>covariance matrix</strong></p>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \pmatrix{\sigma_x^2 \hspace{1cm} \rho\sigma_x\sigma_y \\
\rho\sigma_x\sigma_y \hspace{1cm}\sigma_y^2}
\]</span></p>
<p>where the elements on the diagonal (<span class="math inline">\(\sigma_x^2\)</span> and <span class="math inline">\(\sigma_y^2\)</span>) are the variances and the off-diagonal elements (which are the same) are the covariances <span class="math inline">\(\mathrm{Cov}(X,Y) = \rho\sigma_x\sigma_y\)</span>.</p>
<p>We can now express the approximate sampling distribution of the ML estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> :</p>
<blockquote class="blockquote">
<p>The sampling distribution of the ML estimator of the vector with Poisson regression coefficients can be approximated in large samples (<em>n</em> large) by a bivariate normal distribution:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \pmatrix{\hat\beta_0 \\ \hat \beta_1} \overset{\mathrm{approx}}{\sim} N(\boldsymbol{\beta}, \Sigma)\, \text{ for large }\,n
\]</span></p>
</blockquote>
<p>where the standard deviation of <span class="math inline">\(\hat\beta_0\)</span> is the square root of the element in first row and first column of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and the standard deviation of <span class="math inline">\(\hat\beta_1\)</span> is the square root of the element in second row and second column of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>We already know how to obtain the ML estimate of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> with <code>optim</code>, but how can we use optim to compute an estimate of the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>? It turns out that we can use almost the same code, the arguments <code>method=c("BFGS")</code> and <code>hessian=TRUE</code> will cause <code>optim</code> to return a <em>matrix</em> of second derivatives, as <strong>Hessian matrix</strong>. If you remember how we used the second derivative to obtain the standard error when we only had one parameter: <code>mle_se = 1/sqrt(-optres$hessian[1])</code> we can do nearly the same to obtain the standard errors in the bivariate case:</p>
<p><code>mle_cov = solve(-optres$hessian)</code></p>
<p><code>mle_se = sqrt(diag(mle_cov))</code></p>
<p>where the first line of code computes an estimate of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and the second line of code extracts the diagonal elements (this is <code>diag(mle_cov)</code>) and then directly computes the square root of each element to obtain the standard error. The only weird part is that the division is now done with the <code>solve</code> function. This function computes the <strong>matrix inverse</strong>, which is a special type of division for matrices, like covariance matrices. You donâ€™t need to worry to much about it now, just know that the following code gives us the ML estimates and their respective standard errors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> ebaycoins<span class="sc">$</span>ReservePriceFrac</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_poisreg, <span class="at">gr=</span><span class="cn">NULL</span>, y, x, <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>mle <span class="ot">=</span> optres<span class="sc">$</span>par</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>mle_cov <span class="ot">=</span> <span class="fu">solve</span>(<span class="sc">-</span>optres<span class="sc">$</span>hessian)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>mle_se <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(mle_cov))</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta0 is"</span>, <span class="fu">round</span>(mle[<span class="dv">1</span>], <span class="dv">4</span>), </span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">"with approx standard error"</span>, mle_se[<span class="dv">1</span>] ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta0 is 2.1077 with approx standard error 0.0268950137116756</code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta1 is"</span>, <span class="fu">round</span>(mle[<span class="dv">2</span>], <span class="dv">4</span>), </span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">"with approx standard error"</span>, mle_se[<span class="dv">2</span>] ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta1 is -1.7434 with approx standard error 0.0562816780005428</code></pre>
</div>
</div>
</section>
<section id="poisson-regression-with-two-covariates" class="level5">
<h5 class="anchored" data-anchor-id="poisson-regression-with-two-covariates">Poisson regression with two covariates</h5>
<p>All of this is great, but what if I want to add another covariate to the model, perhaps the binary covariate <code>PowerSeller</code>? Now I have a Poisson regression model with three parameters</p>
<p><span class="math display">\[
y_i \vert x_i \overset{\mathrm{indep}}{\sim} \mathrm{Pois}(\lambda_i) \text{, where }
\lambda_i = \exp(\beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2})
\]</span></p>
<p>where <span class="math inline">\(x_1\)</span> is the old <code>ReservePriceFrac</code> covariate and <span class="math inline">\(x_2\)</span> is the new <code>PowerSeller</code> covariate. So <span class="math inline">\(x_{5,2}\)</span> is for example the <span class="math inline">\(5\)</span>th observation on the second covariate, <code>PowerSeller</code> .</p>
<p>The log-likelihood <span class="math inline">\(l(\beta_0 ,\beta_1 , \beta_2)\)</span> is now a function of three variables, which makes it hard to visualize in a plot. But we can obtain the ML estimates and standard errors by <code>optim</code>, in exact the same way. We would have to define the log-likelihood function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>loglik_poisreg <span class="ot">&lt;-</span> <span class="cf">function</span>(betavect, y, x1, x2){</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>  lambda <span class="ot">=</span> <span class="fu">exp</span>(betavect[<span class="dv">1</span>] <span class="sc">+</span> betavect[<span class="dv">2</span>]<span class="sc">*</span>x1 <span class="sc">+</span> betavect[<span class="dv">3</span>]<span class="sc">*</span>x2)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>  loglik <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(y, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loglik)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and then run <code>optim</code> on it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> ebaycoins<span class="sc">$</span>ReservePriceFrac</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> ebaycoins<span class="sc">$</span>PowerSeller</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_poisreg, <span class="at">gr=</span><span class="cn">NULL</span>, y, x1, x2, <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), </span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>mle <span class="ot">=</span> optres<span class="sc">$</span>par</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>mle_cov <span class="ot">=</span> <span class="fu">solve</span>(<span class="sc">-</span>optres<span class="sc">$</span>hessian)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>mle_se <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(mle_cov))</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta0 is"</span>, <span class="fu">round</span>(mle[<span class="dv">1</span>], <span class="dv">4</span>), </span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">"with approx standard error"</span>, mle_se[<span class="dv">1</span>] ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta0 is 2.1019 with approx standard error 0.0280451103011723</code></pre>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta1 is"</span>, <span class="fu">round</span>(mle[<span class="dv">2</span>], <span class="dv">4</span>), </span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">"with approx standard error"</span>, mle_se[<span class="dv">2</span>] ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta1 is -1.7579 with approx standard error 0.0596746840000152</code></pre>
</div>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste</span>(<span class="st">"The ML estimate of beta2 is"</span>, <span class="fu">round</span>(mle[<span class="dv">3</span>], <span class="dv">4</span>), </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">"with approx standard error"</span>, mle_se[<span class="dv">3</span>] ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta2 is 0.0262 with approx standard error 0.0354787795410904</code></pre>
</div>
</div>
<p>Sorry for repeating myself, but note how I supplied <code>y</code>, <code>x1</code> and <code>x2</code> to optim, and that the initial value is now a vector with three elements.</p>
</section>
<section id="poisson-regression-with-arbitrary-number-of-covariates" class="level5">
<h5 class="anchored" data-anchor-id="poisson-regression-with-arbitrary-number-of-covariates">Poisson regression with arbitrary number of covariates</h5>
<p>It is however a little annoying to have to rewrite the log-likelihood function every time we decide to add a covariate to the regression. There is a beautiful way around this, but it requires a final step to explain. Still have the energy to learn something new? Letâ€™s go!</p>
<p>First, we place all the covariate data as columns in a <em>matrix</em>. For the case with two covariates, it looks like this:</p>
<p><span class="math display">\[
\mathbf{X} = \pmatrix{
1 \; x_{ 11} \; x_{12} \\
1 \; x_{ 21} \; x_{22} \\
\vdots \;\;\;\; \vdots \;\;\;\; \vdots \\
1 \; x_{ n1} \; x_{n2}
}
\]</span></p>
<p>The triple dots <span class="math inline">\(\vdots\)</span> are just saying that there a bunch of rows that I am not writing out here. There is nothing really special here, it is just the mathematical way of saying a table with data. Well, ok, something is a little special: what is the deal with the first column with only 1â€™s? Those are there to represent the intercept <span class="math inline">\(\beta_0\)</span>. If you think of it, the intercept in the regression effect <span class="math inline">\(\exp(\beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2})\)</span> is sort of like having a covariate <span class="math inline">\(x_{i,0}\)</span> that always takes the value one, since trivially <span class="math inline">\(\beta_0 \cdot 1 = \beta_0\)</span>. Finally, <span class="math inline">\(\mathbf{X}\)</span> is in <strong>bold</strong> UPPERCASE font, to show that it is a matrix. Letâ€™s set up this matrix in R code using the <code>cbind</code> function in R (<strong>c</strong>olumn <strong>bind</strong>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span>,ebaycoins<span class="sc">$</span>ReservePriceFrac, ebaycoins<span class="sc">$</span>PowerSeller)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]      [,2] [,3]
[1,]    1 0.3688654    0
[2,]    1 0.2298851    1
[3,]    1 1.0200000    1
[4,]    1 0.7217391    0
[5,]    1 0.1672362    0
[6,]    1 1.2094737    0</code></pre>
</div>
</div>
<p>Here used the <code>cbind</code> function to construct the matrix. There is also the <code>matrix</code> command in R to construct any type of matrix from its elements. Here is an example with a <span class="math inline">\(3\times2\)</span> matrix (three rows and two columns):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="fl">0.4</span>,<span class="dv">3</span>,<span class="dv">10</span>), <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>A</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    1  0.4
[2,]    2  3.0
[3,]    5 10.0</code></pre>
</div>
</div>
<p>We can similarly put all the observations on the response variable in a long vector:</p>
<p><span class="math inline">\(\mathbf{y}=\pmatrix{y_1 \\y_2 \\ \vdots \\ y_n}\)</span> which is in bold lowercase font to show that it is a vector. We have already set up this before, but lets do it again:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The often used <code>c</code> function in R constructs general vectors:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 4</code></pre>
</div>
</div>
<p>Finally, we can also (as we already did above) pack all the regression coefficient in a vector <span class="math display">\[\boldsymbol{\beta} = \pmatrix{\beta_0  \\ \beta_1 \\ \beta_2
}
\]</span> Here comes the beautiful, but little tricky part, we can <em>multiply</em> the matrix <span class="math inline">\(\mathbf{X}\)</span> with the vector <span class="math inline">\(\boldsymbol{\beta}\)</span>, in a very specific way that gives the end result <span class="math display">\[
\mathbf{X}*\boldsymbol{\beta} = \pmatrix{\beta_0 + \beta_1 x_{1,1} + \beta_2 x_{1,2} \\ \beta_0 + \beta_1 x_{2,1} + \beta_2 x_{2,2} \\ \vdots \\ \beta_0 + \beta_1 x_{n,1} + \beta_2 x_{n,2} }
\]</span></p>
<p>What I have denote by the *-symbol is the matrix multiplication operator, which we applied to a matrix and and vector gives the end result above. We will not define the matrix multiplication here, but you can read more about it if you want on <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">wikipedia</a>. This matrix multiplication is convenient, since we can then compute the <span class="math inline">\(\lambda_i\)</span> for <em>all</em> observations (auctions) in the Poisson regression at once:</p>
<p><span class="math display">\[
\pmatrix{\lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n} = \exp(\mathbf{X}*\boldsymbol{\beta}) = \pmatrix{\exp(\beta_0 + \beta_1 x_{1,1} + \beta_2 x_{1,2}) \\ \exp(\beta_0 + \beta_1 x_{2,1} + \beta_2 x_{2,2}) \\ \vdots \\ \exp(\beta_0 + \beta_1 x_{n,1} + \beta_2 x_{n,2} )}
\]</span></p>
<p>because in R the <code>exp</code> function is vectorized, so <code>exp()</code> on a vector will return a vector where the exponential function is applied to each element separetely. R uses the symbol <code>%*%</code> for matrix multiplication. Letâ€™s try to use it on some made-up matrix and vector:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="fl">0.4</span>,<span class="dv">3</span>,<span class="dv">10</span>), <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>A<span class="sc">%*%</span>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]  2.6
[2,] 14.0
[3,] 45.0</code></pre>
</div>
</div>
<p>Ok, thatâ€™s it, with this new matrix terminology and the matrix product, we can define the log-likelihood function generally for <em>any</em> number of covariates placed in the columns of <span class="math inline">\(\mathbf{X}\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>loglik_poisreg <span class="ot">&lt;-</span> <span class="cf">function</span>(betavect, y, X){</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>  lambda <span class="ot">=</span> <span class="fu">exp</span>(X <span class="sc">%*%</span> betavect)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>  loglik <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(y, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(loglik)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that this works for any <span class="math inline">\(\mathbf{X}\)</span>, by just adding more and more covariates as columns. But letâ€™s try to use it now for the regression with three covariates:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span>, ebaycoins<span class="sc">$</span>ReservePriceFrac, ebaycoins<span class="sc">$</span>PowerSeller, ebaycoins<span class="sc">$</span>Sealed)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>betavect <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="fu">loglik_poisreg</span>(betavect, y, X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -4999.953</code></pre>
</div>
</div>
<p>We can now finally find the ML estimates and standard errors from <code>optim</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> ebaycoins<span class="sc">$</span>NBidders</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span>, ebaycoins<span class="sc">$</span>ReservePriceFrac, ebaycoins<span class="sc">$</span>PowerSeller, ebaycoins<span class="sc">$</span>Sealed)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>initVal <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>optres <span class="ot">&lt;-</span> <span class="fu">optim</span>(initVal, loglik_poisreg, <span class="at">gr=</span><span class="cn">NULL</span>, y, X, </span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), <span class="at">method=</span><span class="fu">c</span>(<span class="st">"BFGS"</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>mle <span class="ot">=</span> optres<span class="sc">$</span>par</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>mle_cov <span class="ot">=</span> <span class="fu">solve</span>(<span class="sc">-</span>optres<span class="sc">$</span>hessian)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>mle_se <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(mle_cov))</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="fu">message</span>(<span class="fu">paste0</span>(</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>  <span class="st">"The ML estimate of beta0 is "</span>, <span class="fu">round</span>(mle[<span class="dv">1</span>], <span class="dv">4</span>), </span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">" with approx standard error "</span>, <span class="fu">round</span>(mle_se[<span class="dv">1</span>], <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">"The ML estimate of beta1 is "</span>, <span class="fu">round</span>(mle[<span class="dv">2</span>], <span class="dv">4</span>), </span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>              <span class="st">" with approx standard error "</span>, <span class="fu">round</span>(mle_se[<span class="dv">2</span>], <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"The ML estimate of beta2 is "</span>, <span class="fu">round</span>(mle[<span class="dv">3</span>], <span class="dv">4</span>), </span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>              <span class="st">" with approx standard error "</span>, <span class="fu">round</span>(mle_se[<span class="dv">3</span>], <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">"The ML estimate of beta3 is "</span>, <span class="fu">round</span>(mle[<span class="dv">4</span>], <span class="dv">4</span>), </span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>              <span class="st">" with approx standard error "</span>, <span class="fu">round</span>(mle_se[<span class="dv">4</span>], <span class="dv">4</span>)</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>The ML estimate of beta0 is 2.0668 with approx standard error 0.0286
The ML estimate of beta1 is -1.7437 with approx standard error 0.0599
The ML estimate of beta2 is -0.0113 with approx standard error 0.0358
The ML estimate of beta3 is 0.3882 with approx standard error 0.0485</code></pre>
</div>
</div>
<p>ðŸ˜ðŸ˜ðŸ˜</p>
<p>So now we know how to find the ML estimates and standard errors using numerical optimization. Before we also said that the sampling distribution was approximately normal (Poisson model with one parameter) or bivariate normal (Poisson regression with one covariate and two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). But what is the sampling distribution in the case with two or more covariates? ðŸ¤” It is what we call a <strong>multivariate normal distribution</strong>, and we write it exactly like we did for the bivariate case, but this time with <span class="math inline">\(p\)</span> variables</p>
<p><span class="math display">\[
(X_1,X_2,\ldots,X_p) \sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean <em>vector</em> that now contains <span class="math inline">\(p\)</span> means</p>
<p><span class="math display">\[
\boldsymbol{\mu} = \pmatrix{\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p}
\]</span></p>
<p>and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the <strong>covariance matrix</strong> which now has <span class="math inline">\(p\)</span> rows and <span class="math inline">\(p\)</span> columns:</p>
<p><span class="math display">\[
\boldsymbol{\Sigma} = \pmatrix{
\sigma_1^2 &amp; \rho_{12}\sigma_1\sigma_2 &amp; \cdots &amp; \rho_{1p}\sigma_1\sigma_p \\
\rho_{12}\sigma_1\sigma_2 &amp; \sigma_2^2 &amp; \cdots &amp;\rho_{2p}\sigma_2\sigma_p \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
\rho_{1p}\sigma_1\sigma_p &amp; \rho_{2p}\sigma_1\sigma_p &amp; \cdots &amp; \sigma_p^2 }
\]</span></p>
<p>where the diagonal elements are the variance for each variable, and the <span class="math inline">\(\rho_{ij}\)</span> are the correlation coefficients between all pairs of variables.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is the end of the line, here is a summary of all the amazing things that we have gone through:</p>
<ul>
<li>The likelihood function</li>
<li>The maximum likelihood estimator</li>
<li>How to derive the maximum likelihood estimator and its standard error for iid Poisson data</li>
<li>How to use numerical optimization in R to obtain:
<ul>
<li>the maximum likelihood estimator</li>
<li>the approximate standard error of the maximum likelihood estimator</li>
<li>a large sample normal approximation of the sampling distribution of the maximum likelihood estimator</li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reflection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Take a minute or two to reflect on how general this is. Say that you instead want to model some continuous variable <code>y</code> in a regression setting where there are some covariates <code>x1</code>, <code>x2</code> etc available. The Poisson distribution is clearly not the right model now. But maybe an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>? Since <span class="math inline">\(\lambda\)</span> has to be positive in an exponential distribution we can use the model <span class="math display">\[y \mid x_1,\ldots,x_p \sim \mathrm{Expon}(\lambda = \exp(\beta_0+\beta_1 x_1 + \cdots + \beta_p x_p))\]</span></p>
<p>Now we can do <strong>exactly</strong> the same as we did above, using <code>dexp</code> instead of <code>dpois</code> to compute the exponential density function. Since the have continuous <code>y</code> here we use the <em>density</em> of the observed data to build up the likelihood function, but otherwise it is all the same.</p>
<p>This is the superpower that I mentioned in the beginning. When you are given some data to analyze, think about a suitable model and code up the likelihood function. Then let <code>optim</code> do the rest. ðŸ˜Ž</p>
</div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, <em>Journal of Business and Economic Statistics</em> <a href="https://www.tandfonline.com/doi/abs/10.1198/jbes.2011.08289">pdf</a><a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2"><p>If we want to actually interpret these joint probabilities, we can consider looking at the <em>average</em> probability per observation. This average probability will not become tiny when we look at more and more observations, it remains interpretable. The usual <strong>arithmetic mean</strong></p>
<p><span class="math display">\[\frac{1}{n}\sum_ {i=1}^n P(y_i \vert \lambda)\]</span></p>
<p>is not so great for averaging probabilities, however. The <strong>geometric mean</strong></p>
<p><span class="math display">\[\Big(\prod_ {i=1}^n P(y_i \vert \lambda)\Big)^{\frac{1}{n}}\]</span></p>
<p>has nicer properties, so we would use that.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://www.su.se/statistiska-institutionen/">
<p>Department of Statistics - Stockholm University</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Use our course!
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/StatisticsSU/STM">
      <i class="bi bi-github" role="img" aria-label="Quarto GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>