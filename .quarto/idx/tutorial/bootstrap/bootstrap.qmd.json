{"title":"The bootstrap","markdown":{"yaml":{"title":"The bootstrap","author":"Mattias Villani","editor":"visual"},"headingText":"| output: false","containsRefs":false,"markdown":"\n\nIn this tutorial you will learn about the **bootstrap** method for approximating the sampling distribution of estimator, like that for the maximum likelihood (ML) estimator. It is a purely simulation-based method that is quite useful in many situations.\n\nLet's first load some libraries that we will use (install them using `install.packages()` if you haven't already).\n\n```{r}\nlibrary(latex2exp) # to be able to plot mathematical symbols (LaTeX)\nlibrary(remotes)   # to be able to load packages from GitHub\nlibrary(ggplot2)   # fancy plotting\nlibrary(mvtnorm)   # the multivariate normal distribution\n```\n\n### Data\n\nWe will use the dataset `ebaycoins` in the R package `SUdatasets` for illustration. The dataset contains data from 1000 eBay auctions of collector's coins[^1]. Let's load the dataset and have a look:\n\n[^1]: The data comes from the paper: Wegmann, B. och Villani, M. (2011). Bayesian Inference in Structural Second-Price Common Value Auctions, *Journal of Business and Economic Statistics* [pdf](https://www.tandfonline.com/doi/abs/10.1198/jbes.2011.08289)\n\n```{r}\n#install_github(\"StatisticsSU/SUdatasets\") # uncomment if this is not installed\nlibrary(\"SUdatasets\")\nhead(ebaycoins)\n```\n\nEach auction (rows in the dataset) will be taken as an observation, and the dataset has the following variables:\n\n-   the number of bidders in each auction (`NBidders`)\n\n-   the final price (`FinalPrice`)\n\n-   the book value of the coin according a coin collectors catalogue (`BookVal`).\n\n-   the seller's reservations price (lowest price that the seller is willing to sell for) as a fraction the book value (`ReservePriceFrac`).\n\n-   binary variables on whether or not the seller is a verified ebay seller (`IDSeller`), sells large quantites (`PowerSeller`) and if the seller has many reviews with negative feedback (`NegFeedback`)\n\n-   information about the condition of the object: if it has a minor blemish (`MinorBlem`), a major one (`MajorBlem`), or sold in its original unbroken packaging (`Sealed`).\n\n### Maximum likelihood for the Poisson model\n\nWe will first analyze only the variable `NBidders` and later move over to a regression modeling situation. Since `NBidders` is a count variable, a natural first model to consider is the **Poisson model**:\n\n$$\nY_1,\\ldots,Y_n \\vert \\lambda \\overset{\\mathrm{iid}}{\\sim}\\mathrm{Poisson}(\\lambda)\n$$\n\nwhere we use the symbol $Y$ for the random variable `NBidders` and $y$ as the observed value.\n\nWe can estimate $\\lambda$ by the **maximum likelihood** (**ML**) metod. The ML method finds the value for $\\lambda$ in the Poisson distribution that maximizes the probability of the observed dataset $y_1,\\ldots,y_n$. As we have seen in [tutorial on numerical ML](../../MLnumerical.qmd), the ML estimate for this model is just the sample mean $\\hat \\lambda = \\bar y$ and the standard error is $\\mathrm{SE}(\\hat\\lambda)=\\sqrt{\\bar y/n}$. For `NBidders` in the `ebayscoins` data we have\n\n```{r}\nn = length(ebaycoins$NBidders)\nmessage(paste(\"ML estimate:\", mean(ebaycoins$NBidders)))\nmessage(paste(\"SE of ML estimate:\", sqrt(mean(ebaycoins$NBidders)/n)))\n```\n\n### Sampling distribution of MLE by bootstrap\n\nThe **bootstrap** is an simulation-based technique for obtaining an approximation to the sampling distribution. Recall first that the sampling distribution of an estimator answers the question \"what is the distribution of the estimator when we repetedly draw samples of size $n$ from the population?\". The underlying assumption of the bootstrap method is that the sample is good representation of the underlying population distribution. We can therefore approximate the sampling distribution by sampling a lot of new datasets from the sample *with replacement*. This means the each new so called *bootstrap sample* contains only observations from the sample, but since the sampling is with replacement each such bootstrap sample will typically contain multiple copies of some observations while some observations in the original sample will not appear at all.\n\nTo fix ideas, assume that we have av sample of only $n=5$ observations: $y=(3,5,1,7,2)$. Here are five bootstrap samples:\n\n```{r}\ny = c(3,5,1,7,2)\nsample(y, replace = TRUE)\nsample(y, replace = TRUE)\nsample(y, replace = TRUE)\nsample(y, replace = TRUE)\nsample(y, replace = TRUE)\n```\n\nNow, for each bootstrap sample we compute the estimator, in the Poisson case the mean of the bootstrap sample. The bootstrap approximation of the sampling distribution is then approximated by a histogram of the estimates from the bootstrap samples.\n\nLet's try this out for the Poisson model with a single parameter $\\lambda$, where the ML estimate was earlier shown to be $\\hat\\lambda=\\bar y$ and the estimator standard error $SE(\\hat\\lambda)=\\sqrt{\\bar y/n}$. The code below computes the bootstrap approximation of the sampling distribution of $\\hat\\lambda = \\bar y$ for the number of bidders in the `ebaycoins` data (the large sample approximation from `optim` is overlayed as an orange curve for comparison). The bootstrap approximation is fairly close to the normal approximation from `optim`.\n\n```{r}\ny = ebaycoins$NBidders\nn = length(y)\nmle = mean(y)\nmle_se = sqrt(mean(y)/n)\nnboot = 10000\nmlboot = rep(NA, nboot)\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  mlboot[j] = mean(yboot)\n}\nhist(mlboot, 100, freq = FALSE, ylim = c(0,7), c = \"steelblue\", \n     main = \"sampling distribution via the bootstrap\")\nlambdas = seq(mle - 4*mle_se, mle + 4*mle_se, by = 0.001)\nlines(lambdas, dnorm(lambdas, mle, mle_se), type =\"l\", col = \"orange\", lwd = 2, \n      xlab = TeX(r'($\\lambda$)'))\n```\n\nIn this simple case we had a formula for the ML estimator $\\bar y$. But what if we didn't have such a formula (Iike in the Poisson regression case), what can we do then? Well, we can just use `optim` on each bootstrap sample.\n\nTo use optim, we need to code up the Poisson log-likelihood. Let us define this as an R function that takes the data `y` and the parameter `lambda` as inputs and returns the log-likelihood value `loglik` as output.\n\n```{r}\nloglik_pois <- function(lambda, y){\n  loglik = sum(dpois(y, lambda, log = TRUE))\n  return(loglik)\n}\n```\n\nWe will only draw 1000 bootstrap samples so that the computations run relatively fast:\n\n```{r}\ny = ebaycoins$NBidders\nn = length(y)\nmle = mean(y)\nmle_se = sqrt(mean(y)/n)\nnboot = 1000\nmlboot = rep(NA, nboot)\ninitVal = 2\nfor (j in 1:nboot){\n  yboot = sample(y, replace = TRUE) # sample with replace of the original y\n  optres <- optim(initVal, loglik_pois, gr=NULL, yboot, control=list(fnscale=-1), \n                  method=c(\"BFGS\"), hessian=TRUE)\n  mlboot[j] = optres$par\n}\nhist(mlboot, 100, freq = FALSE, ylim = c(0,7), c = \"steelblue\", \n     main = \"sampling distribution via the bootstrap and optim\")\nlambdas = seq(mle - 4*mle_se, mle + 4*mle_se, by = 0.001)\nlines(lambdas, dnorm(lambdas, mle, mle_se), type =\"l\", col = \"orange\", lwd = 2, \n      xlab = TeX(r'($\\lambda$)'))\n```\n\nIt is important to note that I use the generated bootstrap sample `yboot` as the argument to `loglik_pois` at every iteration of the loop, i.e. `optim` gets a fresh bootstrap sample at every iteration of the loop.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"bootstrap.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","knitr":{"opts_knit":{"verbose":false}},"callout-appearance":"default","theme":{"dark":"superhero","light":"lumen"},"toc-title":"Sections","title":"The bootstrap","author":"Mattias Villani","editor":"visual"},"extensions":{"book":{"multiFile":true}}},"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","toc":false,"output-file":"bootstrap.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.313","auto-stretch":true,"knitr":{"opts_knit":{"verbose":false}},"callout-appearance":"default","center":true,"slideNumber":true,"height":1200,"width":2000,"transition":"none","transitionSpeed":"slow","previewLinks":"auto","progress":true,"citations-hover":false,"logo":"../misc/SU_logotyp_Liggande_1000px.png","footnotes-hover":true,"footer":"Statistical Theory and Methods","title":"The bootstrap","author":"Mattias Villani","editor":"visual"}}}}