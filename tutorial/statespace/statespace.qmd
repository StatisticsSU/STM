---
title: "State-space models - filtering, smoothing and forecasting"
author: "Mattias Villani"
editor: visual
format: 
  html:
    toc: true
    toc-depth: 5
---

> This tutorial gives a very brief introduction to state-space models, along with inference methods like Kalman filtering, smoothing and forecasting. The methods are illustrated using the R package `dlm` , exemplified with the local level model fitted to the well-known Nile river data. The tutorial is also sprinkled with some cool interactivity in Javascript.

## Piecewise constant model

Let us start with a simple time-varying parameter model where the mean $\mu_t$ changes (abruptly) at certain time points $t_1, t_2, \dots, t_K$:

$$
y_t = \mu_t + \varepsilon_t, \quad \varepsilon_t \sim N(0, \sigma_\varepsilon^2)
$$

$$
\begin{align}   
 \mu_t &= 
 \begin{cases}            
  \mu_1 & \text{if $1 \leq t \leq t_1$} \\
  \mu_2 & \text{if $t_1 < t \leq t_2$} \\            
  \vdots & \vdots \\ 
  \mu_K & \text{if $t_{K-1} < t \leq T$}. \\          
 \end{cases}
\end{align}
$$

Here is a widget that lets you simulate data from the piecewise constant model.

```{=html}
<iframe width="100%" height="772" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/piecewise-constant-model?cells=viewof+input%2Cviewof+mu_vect%2Cviewof+sigmaeps%2Cviewof+simulatebutton%2Ctimeplot"></iframe>
```
## Local level model

The piecewise constant model has a few abrupt changes in the mean, but what if the mean changes more gradually? The **local level** model has a constantly changing mean following a random walk model:

$$y_t = \mu_t + \varepsilon_t,\qquad \varepsilon_t \sim N(0,\sigma_\varepsilon^2)$$

$$\mu_t = \mu_{t-1} + \nu_t,\qquad \nu_t \sim N(0,\sigma_\nu^2)$$

which models the observed time series $y_t$ as a mean $\mu_t$ plus a random **measurement error** or disturbance $\varepsilon_t$. The mean $\mu_t$ evolves over time as a random walk driven by **innovations** $\nu_t$.

Here is a widget that simulates data from the model. Go ahead, experiment with the two standard deviations in the model: the measurement/noise $\sigma_\varepsilon$ and the standard deviation of the innovations to the mean process, $\sigma_\nu$. For example, drive $\sigma_\nu$ toward zero and note how the mean becomes close to constant over time.

```{=html}
<iframe width="100%" height="696" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/local-level-model@1184?cells=viewof+input%2Cviewof+simulatebutton%2Ctimeplot"></iframe>
```
## Regression with time-varying parameters

The usual simple linear time series regression model is

$$
y_t = \alpha + \beta x_t  + \varepsilon_t, \quad \varepsilon_t \sim N(0, \sigma_\varepsilon^2) \qquad t=1,\ldots,T
$$

where $y_t$ is a time series response variable (for example electricity price) that is being explained by the explanatory variable $x_t$ (for example temperature). This model assumes that the parameters $\alpha$, $\beta$ and $\sigma_\varepsilon$ are constant in time, that the relationship between electricity price and temperature has remained the same throughout the whole observed time period.

It sometimes makes sense to let the parameters vary with time. Here is one such model, the **time-varying regression** model:

$$
\begin{align}  
y_t &= \alpha_{t} + \beta_{t} x_t  + \varepsilon_t, \quad \varepsilon_t \sim N(0, \sigma_\varepsilon^2)  \\  
\alpha_{t} &= \alpha_{t-1} + \eta_t, \qquad \quad \eta_t \sim N(0, \sigma_\alpha^2)   \\  
\beta_{t} &= \beta_{t-1} + \nu_t, \qquad \quad \nu_t \sim N(0, \sigma_\beta^2) 
\end{align}
$$

where the intercept $\alpha$ now has a time $t$ subscript and evolves in time following a random walk process

$$\alpha_{t} = \alpha_{t-1} + \eta_t, \qquad \quad \eta_t \sim N(0, \sigma_\alpha^2)$$

so that in every time period, the intercept changes by adding on an innovation $\eta_t$ drawn from a normal distribution with standard deviation $\sigma_\alpha$. This standard deviation therefore controls how much the intercept changes over time. The slope $\beta$ changes over time in a similar fashion, with the speed of change determined by $\sigma_\beta$.

Here is a widget that simulates data from the time-varying regression above. By moving the slider (*show regline at time*) you can plot the regression line $\alpha_t + \beta_t x_t$ at any time period $t$. The plot tries to highlight (darker blue) data points that are closer in time to the time chosen by the slider. To the left you can see the whole time path of the simulated $\alpha$ and $\beta$ with the current parameters highlighted by dots.

```{=html}
<iframe width="100%" height="586" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/time-varying-regression-model@1516?cells=viewof+input%2Cviewof+obsnumber%2Cviewof+simulatebutton%2Cplots"></iframe>
```
## State-space model - filtering, smoothing and forecasting

#### The state space model

A state-space model for a univariate time series $y_t$ with a state vector $\boldsymbol{\theta}_t$ is in the `dlm` package written as

$$
\begin{align}
y_t &= \boldsymbol{F} \boldsymbol{\theta}_t + v_t,\hspace{1.5cm} v_t \sim N(\boldsymbol{0},\boldsymbol{V})  \\
\boldsymbol{\theta}_t &= \boldsymbol{G} \boldsymbol{\theta}_{t-1} + \boldsymbol{w}_t, \qquad \boldsymbol{w}_t \sim N(\boldsymbol{0},\boldsymbol{W})
\end{align}
$$

For example, the local level model is a state-space model with

$$
\begin{align}
\boldsymbol{\theta}_t &= \mu_t \\
 \boldsymbol{C} &= 1 \\
 \boldsymbol{A} &= 1  \\
 \boldsymbol{V} &= \sigma_\varepsilon^2 \\
\boldsymbol{W} &= \sigma_\nu^2
\end{align}
$$

Hence the state vector is a single scalar, $\mu_t$, the unobserved local level of time series. We learn about the state $\mu_t$ from the observed time series $y_t$ .

#### Filtering and smoothing

There are two different types of relevant inferences in state-space models: filtering and smoothing:

-   The **filtered** estimate $\hat{\boldsymbol{\theta}}_{t|t}$ of the state $\boldsymbol{\theta}_t$ uses **data up to time** $t$.

-   The **smoothed** estimate $\hat{\boldsymbol{\theta}}_{t|T}$ of the state $\boldsymbol{\theta}_t$ uses **data up to time** $T$, the end of the time series.

The filtered estimate is therefore the **instantaneous** estimate, giving the best estimate of the current state. The smoothed estimate is the **retrospective** estimate that looks back in time and gives us the best estimate using all the data.

Filtering means to compute the sequence of instantaneous estimates of the unobserved state at every time point $t=1,2,\ldots,T$

$$
\hat{\boldsymbol{\theta}}_{1|1},\hat{\boldsymbol{\theta}}_{2|2},\ldots,\hat{\boldsymbol{\theta}}_{T|T}
$$

We will take a time series and compute the filtered estimates for the whole time series, but it is important to understand that filtering is often done in **real-time**, which means it is a continously ongoing process that returns filtered estimates of the state $\boldsymbol{\theta}_t$ as time progresses and new measurements $y_t$ come in. Think about a self-driving car that is continously trying to understand the environment (people, other cars, the road conditions etc). The environment is the state and the car uses its sensors to collect measurements. The filtering estimates tells the car about the best guess for the environment at every point in time.

For state-space models of the type discussed here (linear measurement equation and linear evolution of the state, with independent Normal measurement errors and state innovations), the filtered estimates are computed with one of the most famous algorithms in statistics: the **Kalman filter**.

The Kalman filter is a little messy to write up, we will do it for completeness, but we will use a package for it so don't worry if the linear algebra is intidimating. The Kalman filter starts with mean $\boldsymbol{\mu}_{0|0}$ (this is the same as $\hat{\boldsymbol{\theta}}_{0|0}$, just different notation) and covariance matrix $\boldsymbol{\Omega}_{0|0}$ for the state at time $t=0$. Think about $\boldsymbol{\mu}_{0|0}$ as the best guess $\boldsymbol{\theta}_0$ of the state vector at time $t=0$ and $\boldsymbol{\Omega}_{0|0}$ representing how sure we can be about this guess.

::: callout-note
## Its all about that Bayes

The Kalman filter is often presented from a frequentist point of view in statistics, where the Kalman filtered estimates are the optimal estimates in the mean square error sense.\
\
The Kalman filter can also be derived as simple Bayesian updating, using Bayes' theorem to update the information about the state as a new measurement comes in. The $\boldsymbol{\mu_{0|0}}$ and $\boldsymbol{\Omega_{0|0}}$ can be seen as the prior mean and prior covariance matrix summarizing your prior information about the state before collecting any measurements.\
\
*Personal opinion* of this author (but also many other): The Kalman filter is great. When something is great, Bayes usually lurks in the background! ðŸ˜œ
:::

The Kalman filter then uses the first measurement $y_1$ to update $\boldsymbol{\mu}_{0|0}$ and $\boldsymbol{\Omega}_{0|0}$ to $\boldsymbol{\mu}_{1|1}$ and $\boldsymbol{\Omega}_{1|1}$ to represent the estimate and the uncertainty for $\boldsymbol{\theta}_1$, the state at time $t=1$. It then continues in the this fashion using the next measurement $y_2$ to compute $\boldsymbol{\mu}_{2|2}$ and $\boldsymbol{\Omega}_{2|2}$ and so on all the way to the end of the time series to finally get $\boldsymbol{\mu}_{T|T}$ and $\boldsymbol{\Omega}_{T|T}$. Again, $\boldsymbol{\mu}_{t|t}= \hat{\boldsymbol{\theta}}_{t|t}$ it's just different notation. Here is the **Kalman filter algorithm**:

-   Initialization: set $\boldsymbol{\mu}_{0|0}$ and $\boldsymbol{\Omega}_{0|0}$

-   for $t=1,\ldots,T$ do

    -   **Prediction update**$$
        \begin{align}
        \boldsymbol{\mu}_{t|t-1} &= \boldsymbol{G} \boldsymbol{\mu}_{t-1|t-1} \\  
        \boldsymbol{\Omega}_{t|t-1} &= \boldsymbol{G}\boldsymbol{\Omega}_{t-1|t-1}  \boldsymbol{G}^\top + \boldsymbol{W}
        \end{align}
        $$

    -   **Measurement update**$$
        \begin{align}
        \boldsymbol{\mu}_{t|t} &= \boldsymbol{\mu}_{t|t-1} + \boldsymbol{K}_t ( y_t - \boldsymbol{F} \boldsymbol{\mu}_{t|t-1}  )  \\  
        \boldsymbol{\Omega}_{t|t} &= (\boldsymbol{I} - \boldsymbol{K}_t \boldsymbol{F} )\boldsymbol{\Omega}_{t|t-1} 
        \end{align}
        $$

where $$\boldsymbol{K}_t = \boldsymbol{\Omega}_{t|t-1}\boldsymbol{F}^\top ( \boldsymbol{F} \boldsymbol{\Omega}_{t|t-1}\boldsymbol{F}^\top + \boldsymbol{V})^{-1}$$ is the **Kalman Gain**.

The widget below lets you experiment with the Kalman filter for the local level model. In the widget, we simulate data from the local level model for given measurement standard deviation $\sigma_\varepsilon$ and innovation standard deviation $\sigma_\eta$. We will then see how well we can infer (filter) the true underlying time series of local levels $\mu_1,\mu_2,\ldots,\mu_T$ when we use a local level model but with potentially different parameters than those used to generate the data.

Try first to change the measurement standard deviation $\sigma_\varepsilon$ in the model used for filtering (*filtering* *model* $\sigma_\varepsilon$). Note how:

\- the filtered mean pays less and less attention to changes in the data when you make $\sigma_\varepsilon$ larger (because the model believes that the data is super crappy and tells us basically nothing about the level)

\- the filtered mean starts chasing the data when $\sigma_\varepsilon$ is made very small (because the model believes that the data are super informative about the level).

```{=html}
<iframe width="100%" height="810" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/kalman-filter@1573?cells=viewof+input%2Cviewof+input_filt%2Cviewof+plotselector%2Cviewof+simulatebutton%2Cplotfilter"></iframe>
```
## The `dlm` package in R

The dlm package is a user-friendly R package for analyzing some state-space models. The package has a nice [vignette](https://cran.r-project.org/web/packages/dlm/vignettes/dlm.pdf) that is worth reading if you plan to use the package more seriously.

##### Filtering

Let's first do some filtering in the `dlm` package. Start by loading the `dlm` package:

```{r}
#install.packages("dlm") # uncomment the first time to install.
library(dlm)
```

We now need to tell the `dlm` package what kind of state-space model we want to estimate. The means setting up the matrices $\boldsymbol{F}$, $\boldsymbol{G}$, $\boldsymbol{V}$ and $\boldsymbol{W}$. We will keep it simple and use the local level model as example, where all parameter matrices $\boldsymbol{F}$, $\boldsymbol{G}$, $\boldsymbol{V}$ and $\boldsymbol{W}$ are scalars (single numbers). As we have seen above, the local level model corresponds to a state-space model with parameters

$$
\begin{align}
\boldsymbol{\theta}_t &= \mu_t \\
 \boldsymbol{C} &= 1 \\
 \boldsymbol{A} &= 1  \\
 \boldsymbol{V} &= \sigma_\varepsilon^2 \\
\boldsymbol{W} &= \sigma_\nu^2
\end{align}
$$

So we only need to set $\sigma_\varepsilon^2$ and $\sigma_\nu^2$ to start the fun. We will for now set $\sigma_\varepsilon^2 = 10000$ and $\sigma_\nu^2 = 10000$, and return to this when we learn how the dlm package can find maximum likelihood estimates for these parameters. Here is how you setup the local level model in the `dlm` package:

```{r}
model = dlm(FF = 1, V = 10000, GG = 1, W = 10000, m0 = 0, C0 = 100^2)
```

Compute the filtering estimate using the Kalman filter and plot the result

```{r}
nileFilter <- dlmFilter(Nile, model)
plot(Nile, type = 'l', col = "steelblue")
lines(dropFirst(nileFilter$m), type = 'l', col = "orange")
legend("bottomleft", legend = c("Observed", "Filtered"), lty = 1, 
    col = c("steelblue", "orange"))
```

### Parameter estimation by maximum likelihood

The parameters $\sigma_\varepsilon^2$ and $\sigma_\nu^2$ were just set to some values above. Let's instead estimate them by maximum likelihood. The function `dlmMLE` does this for us, but we need to set up a model build object so the the `dlm` package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.

```{r}
 modelBuild <- function(param) {
   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 0, C0 = 100^2)
 }
 fit <- dlmMLE(Nile, parm = c(0,0), build = modelBuild)
```

We need to take the exponential of the estimates to get the estimated variance parameters.

```{r}
 exp(fit$par)
```

So we see that the values used initially are not too far of the maximum likelihood estimates:

$\hat \sigma_\varepsilon^2 \approx 9120$ and $\hat\sigma_\nu^2 \approx 15956$. We can redo the filter, this time using the maximum likelihood estimates of the parameters:

```{r}
model_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 0, C0 = 100^2)
nileFilter <- dlmFilter(Nile, model_mle)
plot(Nile, type = 'l', col = "steelblue")
lines(dropFirst(nileFilter$m), type = 'l', col = "orange")
legend("bottomleft", legend = c("Observed", "Filtered"), lty = 1, 
    col = c("steelblue", "orange"))
```

### Smoothing

```{r}
nileSmooth <- dlmSmooth(Nile, model_mle)
plot(Nile, type = 'l', col = "steelblue")
lines(dropFirst(nileSmooth$s), type = 'l', col = "red")
legend("bottomleft", legend = c("Observed", "Smoothed"), lty = 1, 
    col = c("steelblue", "red"))

```

### Forecasting

```{r}
nileFore <- dlmForecast(nileFilter, nAhead = 5)
sqrtR <- sapply(nileFore$R, function(x) sqrt(x))
pl <- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)
pu <- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)
x <- ts.union(window(Nile, start = c(1900, 1)),
              window(nileSmooth$s, start = c(1900, 1)), 
              nileFore$a, pl, pu)

plot(x, plot.type = "single", type = 'o', pch = c(NA, NA, NA, NA, NA), 
     col = c("steelblue", "red", "brown", "gray", "gray"),
     ylab = "River flow")
legend("bottomleft", legend = c("Observed", "Smoothed", "Forecast", 
    "90% probability limit"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, 
    col = c("steelblue", "red", "brown", "gray", "gray"))

```

## Non-Gaussian state-space models

##### Poisson time series model

A useful model for time series of counts $Y \in \{0,1,2,\ldots \}$ is a Poisson distribution with time-varying intensity $\lambda_t = \exp(z_t)$, where $z_t$ is some continuous stochastic process with autocorrelation, most commonly a random walk:

$$
\begin{align} y_t \vert z_t &\overset{\mathrm{indep}}{\sim} \mathrm{Pois}(\exp(z_t)) \\  
z_t &= z_{t-1} + \eta_t, \qquad \eta_t \sim N(0, \sigma^2_\eta)
\end{align}
$$

Note that because of the exponential function $\lambda_t = \exp(z_t)$ is guaranteed to be positive for all $t$, as required for the Poisson distribution. It is easily to simulate data from the Poisson time series model:

```{r}

# Set up the simulation function, starting the z_t process at zero.
simPoisTimeSeries <- function(T, sigma_eta){
  
  # Simulate the z_t process
  z = rep(0,T+1)
  for (t in 2:(T+1)){
    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)
  }
  
  # Simulate the Poisson variables with different intensities, lambda_t = exp(z_t) for each time
  lambda = exp(z)
  return (rpois(T, lambda = lambda[2:(T+1)]))
}

# Simulate and plot the time series
T = 100
sigma_eta = 0.1
y = simPoisTimeSeries(T, sigma_eta)
plot(y, type = "o", pch = 19, col = "steelblue", yaxt = "n", xlab = "time, t", ylab = "counts, y", 
      main = paste("A simulated Poisson time series with sigma_eta =", sigma_eta))
axis(side = 2, at = seq(0,max(y)))
```

What's life without widgets? Here is one for a slightly more general Poisson time series model where the random walk is replaced by a autoregressive process of order 1:

$$
\begin{align} 
y_t \vert z_t &\overset{\mathrm{indep}}{\sim} \mathrm{Pois}(\exp(z_t)) \\  
z_t &= \mu + \phi(z_{t-1} -\mu) + \eta_t, \qquad \eta_t \sim N(0, \sigma^2_\eta)
\end{align}
$$

```{=html}
<iframe width="100%" height="724" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/poisson-time-series-model?cells=viewof+input%2Cviewof+plotintensity%2Cviewof+simulatebutton%2Ctimeplot"></iframe>
```
##### Stochastic volatility models

Many time series, particularly in the finance, has a variance that is changing over time. Furthermore, it is common to find **volatility clustering** in the data, meaning that once the the variance is high (turbulent stock market) it tends to remain high for a while and vice versa. The basic stochastic volatility (SV) model tries to capture this:

$$
\begin{align}
y_t &= \mu + \varepsilon_t, \hspace{0.8cm} \varepsilon_t \overset{\mathrm{indep}}{\sim}N(0,\exp(z_t)), \\ 
z_t &= z_{t-1} + \eta_t, \quad \eta_t \overset{\mathrm{iid}}{\sim}N(0, \sigma^2_\eta)
\end{align}
$$

where we have for simplicity assumed just a constant mean $mu$, but we can extend this with and autoregressive process, or basically any model of your preference. The thing that set the SV model apart from the other model presented so far is that the variance of the measurement errors $Var(y_t)=Var(\varepsilon_t) = \exp(z_t)$ is heteroscedastic, that is, it varies over time. The variance is driven by the $z_t$ process, which here is modeled as a random walk, which will induce volatility clustering. Note again that we use the exponential function to ensure that the variance is positive for all $t$. Here is code to simulate from this basic stochastic volatility model:

```{r}
# Set up the simulation function, starting the z_t process at zero.
simStochVol <- function(T, mu, sigma_eta){
  
  # Simulate the z_t process
  z = rep(0,T+1)
  for (t in 2:(T+1)){
    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)
  }
  
  # Simulate the y_T with a different variance in for each sigmaÂ²_t = exp(z_t) for each t
  sigma2eps = exp(z)
  y = rnorm(T+1, mean = mu, sd = sqrt(sigma2eps))
  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )
}
```

Let's use that function to simulate a time series and plot it:

```{r}
# Simulate and plot the time series
T = 100
mu = 3
sigma_eta = 1
simuldata = simStochVol(T, mu, sigma_eta)
plot(simuldata$y, type = "l", col = "steelblue", xlab = "time, t", ylab = "y", 
     main = paste("A simulated stochastic volatility process with sigma_eta =", sigma_eta),
     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)
lines(simuldata$sigmaeps, col = "orange", lwd = 2)
legend("topleft", legend = c("time series", "standard deviation"), lty = 1, lwd = c(2,2),
    col = c("steelblue", "orange"))
```

We can replace the random walk for the $z_t$ with a more well-behaved AR(1) process:

$$
\begin{align}
y_t &= \mu_y + \varepsilon_t, \hspace{0.8cm} \varepsilon_t \overset{\mathrm{indep}}{\sim}N(0,\exp(z_t)), \\ 
z_t &= \mu_z + \phi(z_{t-1} - \mu_z) + \eta_t, \quad \eta_t \overset{\mathrm{iid}}{\sim}N(0, \sigma^2_\eta)
\end{align}
$$

where $\mu_y$ is the mean of the time series $y$ and $\mu_z$ is the mean of the (log) variance process $z_t$. The parameter $\mu_z$ therefore determines how much variance $y_t$ has on average and $\phi$ determines how much volatility clustering there is. A $\phi$ close to 1 gives long periods of persistently large or small variance. Here is the code:

```{r}
# Set up the simulation function, starting the z_t process at zero.
simStochVol <- function(T, mu_y, mu_z, phi, sigma_eta){
  
  # Simulate the z_t process
  z = rep(0,T+1)
  for (t in 2:(T+1)){
    z[t] = mu_z + phi*(z[t-1] - mu_z) + rnorm(1, mean = 0, sd = sigma_eta)
  }
  
  # Simulate the y_T with a different variance in for each sigmaÂ²_t = exp(z_t) for each t
  sigma2eps = exp(z)
  y = rnorm(T+1, mean = mu_y, sd = sqrt(sigma2eps))
  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )
}
```

```{r}
# Simulate and plot the time series
T = 1000
mu_y = 3
mu_z = -1
phi = 0.95
sigma_eta = 1
simuldata = simStochVol(T, mu_y, mu_z, phi, sigma_eta)
plot(simuldata$y, type = "l", col = "steelblue", xlab = "time, t", ylab = "y", 
     main = paste("A simulated stochastic volatility process with sigma_eta =", sigma_eta),
     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)
lines(simuldata$sigmaeps, col = "orange", lwd = 2)
legend("topleft", legend = c("time series", "standard deviation"), lty = 1, lwd = c(2,2),
    col = c("steelblue", "orange"))
```

Widget time!

```{=html}
<iframe width="100%" height="707" frameborder="0"
  src="https://observablehq.com/embed/@mattiasvillani/stochastic-volatility-time-series-model@1252?cells=viewof+input%2Cviewof+simulatebutton%2Ctimeplot"></iframe>
```
